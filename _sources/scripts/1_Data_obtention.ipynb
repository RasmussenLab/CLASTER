{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> 1. DATA OBTENTION & PREPROCESSING <center>\n",
    "\n",
    "Hi!\n",
    "This notebook is a guide to obtain and preprocess the data used to train and test CLASTER. It is meant to be run sequentially, i.e. one cell after the other, and it mainly contains:\n",
    "- Markdown cells with descriptions of the data or the code\n",
    "- Code cells:\n",
    "    - that can be run from this notebook, i.e. fast.\n",
    "    - that can be run as a separate python file. These cells, staring with the magic function %%writefile, create self-sufficient pieces of code were designed to speed up computations that can be parallelized, e.g. repeating a process per each gene / genomic region.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before we start ...\n",
    "\n",
    "A) It would be highly recommendable to create an environment for this project. The python package EIR, which is the core framework used to build, train and test CLASTER, will need python >= 3.11. If you have anaconda, it can be done as follows from the terminal:\n",
    "\n",
    "```bash\n",
    "conda create -n claster_env python=3.11 -y #Create environment conda create -p ./claster_env python=3.11 -y\n",
    "conda activate claster_env #Activate it\n",
    "pip install ipykernel #Install ipykernel to run notebook\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ⚠️ Warning ⚠️\n",
    ">\n",
    "> If you created a specific environment, make sure to have it activated.\n",
    "> In VS code we can simply choose the environment we want in the top right icon.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B) We will first install a number of dependencies required to follow the pipeline:\n",
    "- pyBigWig: python package to handle BigWig files, i.e. process genomic tracks as numpy arrays.\n",
    "- wget: python version of wget\n",
    "- pandas: python package to handle data tables as DataFrames\n",
    "- cooler: package to handle Micro-C data in mcool format.\n",
    "- matplotlib: python plotting package\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pyBigWig\n",
    "! pip install pyBigWig wget pandas cooler matplotlib imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data obtention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyBigWig\n",
    "import matplotlib.pyplot as plt\n",
    "import wget\n",
    "from scipy.ndimage import gaussian_filter1d, rotate\n",
    "import cooler\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_files(links, destination_folder):\n",
    "    \"\"\"\n",
    "    This function downloads the files in links and stores them in destination_folder.\n",
    "    Args:\n",
    "        links: dictionary where keys are file names and values are links.\n",
    "        destination_folder: path where we'll store the files.\n",
    "    \"\"\"\n",
    "    for name,link in links.items():\n",
    "        try:\n",
    "            # Ensure the destination folder exists\n",
    "            if not os.path.exists(destination_folder):\n",
    "                os.makedirs(destination_folder)\n",
    "\n",
    "            # Specify the output path for the downloaded file\n",
    "            output_path = os.path.join(destination_folder, name)\n",
    "\n",
    "            # Download the file to the specified destination folder\n",
    "            wget.download(link, out=output_path)\n",
    "            print(f\"\\nSuccessfully downloaded {name} to {destination_folder}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nFailed to download {link} because : {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Gene annotations** \n",
    "\n",
    "We obtained both the mouse reference genome and protein coding gene annotations from the release M25 (GRCm38.p6) from GENCODE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "links = {\"Gene_annotations.gtf.gz\":\"https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M25/gencode.vM25.annotation.gtf.gz\"}\n",
    "savepath = \"../annotations/\"\n",
    "download_files(links, savepath)\n",
    "\n",
    "! gunzip ../annotations/Gene_annotations.gtf.gz\n",
    "\n",
    "# Create a file with annotations that correspond only to genes\n",
    "! awk '$3 == \"gene\"' ../annotations/Gene_annotations.gtf > ../annotations/Filtered_gene_annotations.gtf\n",
    "\n",
    "# Further filter the number of fields/columns required\n",
    "\n",
    "file = Path(\"../annotations/Filtered_gene_annotations.gtf\")\n",
    "outfile = Path(\"../annotations/Final_gene_annotations.tsv\")\n",
    "gene_annotations = pd.read_csv(file, sep=\"\\t\", header=None)\n",
    "\n",
    "ids = []\n",
    "names = []\n",
    "\n",
    "final_df = pd.DataFrame(columns=[\"ID\",\"chr\",\"Start\",\"End\",\"Strand\",\"Name\",\"type\"]).set_index(\"ID\")\n",
    "for (chrom, _, _, start, end, _, strand, _, features) in gene_annotations.values:\n",
    "    splitline =  features.split(\";\")\n",
    "    id = splitline[0].split(\" \")[1]\n",
    "    gene_type = splitline[1].split(\" \")[2]\n",
    "    name = splitline[2].split(\" \")[2]\n",
    "\n",
    "    if gene_type == '\"protein_coding\"' and chrom !='chrM' : # or gene_type == '\"lincRNA\"':\n",
    "        final_df.loc[id[1:-1]] = [chrom,start,end,strand,name[1:-1], gene_type[1:-1]]\n",
    "\n",
    "final_df.to_csv(outfile, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Enhancer annotations**\n",
    "\n",
    "Coordinates for proximal Enhancer Like Signatures (pELS) and distal Enhancer Like Signatures (dELS) were obtained from Supplementary Table 11 in the paper: \n",
    "- https://doi.org/10.1038/s41586-020-2493-4\n",
    "\n",
    "Download link:\n",
    "- https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-020-2493-4/MediaObjects/41586_2020_2493_MOESM13_ESM.txt\n",
    "\n",
    "Once downloaded, you should move the file to the annotations folder to proceed. E.g.:\n",
    "```bash\n",
    "! mv ~/Downloads/41586_2020_2493_MOESM13_ESM.txt ../annotations/ # Edit if necessary\n",
    "```\n",
    "\n",
    "> _Note: mm10 and GRCm38 are synonyms to refer to a reference genome_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRE_df = pd.read_csv(Path(\"../annotations/41586_2020_2493_MOESM13_ESM.txt\"), sep=\"\\t\").set_index(\"cCRE_accession\")\n",
    "CRE_df = CRE_df[(CRE_df[\"group\"].str.contains(\"ELS\"))]\n",
    "CRE_df.to_csv(\"../annotations/Final_Enhancer_annotation.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Micro-C Data acquisition\n",
    "\n",
    "Micro-C matrices for mESCs were obtained from:\n",
    "\n",
    "Tsung-Han S. Hsieh et al., Resolving the 3D Landscape of Transcription-Linked Mammalian Chromatin Folding, Molecular Cell,\n",
    "2020 https://doi.org/10.1016/j.molcel.2020.03.002.\n",
    "\n",
    "> ⏰: The mcool file occupies quite some memory and it took us 33 minutes to download:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = {\"GSE130275_mESC_WT_combined_2.6B.mcool\":\"https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE130275&format=file&file=GSE130275%5FmESC%5FWT%5Fcombined%5F2%2E6B%2Emcool\"}\n",
    "savepath = \"../GEO_files/\"\n",
    "download_files(links, savepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells will create two python files that generate the microC arrays in different ways. The first one does not use parallelization but the second one does, but they do the same.\n",
    "You can run the file in the terminal as:\n",
    "\n",
    "```\n",
    "python create_microC_arrays.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile create_microC_arrays.py\n",
    "\n",
    "import os \n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyBigWig\n",
    "import matplotlib.pyplot as plt\n",
    "import wget\n",
    "from scipy.ndimage import rotate\n",
    "import cooler\n",
    "import logging\n",
    "\n",
    "########### Functions #####################################\n",
    "def find_dataset_min_max(input_path: str) -> float:\n",
    "    # Open contact file (mcool) as a cooler object\n",
    "    c = cooler.Cooler(input_path)\n",
    "    bin_weights = c.bins()['weight'][:]\n",
    "    return (np.min(bin_weights), np.max(bin_weights))\n",
    "\n",
    "def plot_histogram_dataset(input_path: str, bins = 1000):\n",
    "    # Open contact file (mcool) as a cooler object\n",
    "    c = cooler.Cooler(input_path)\n",
    "    bin_weights = c.bins()['weight'][:]\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.hist(bin_weights, bins=bins)\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "def mcool_summary(microc_path):\n",
    "    c = cooler.Cooler(microc_path)\n",
    "\n",
    "    # Inspecting what is inside the cooler object:\n",
    "    metadata = c.info\n",
    "    chromosomes = c.chromnames\n",
    "    extent = c.extent('chr2')\n",
    "\n",
    "    print(\"Metadata:\\n\", metadata)\n",
    "    print(\"Chromosomes:\\n\", chromosomes)\n",
    "    print(\"Extent:\\n\", extent)\n",
    "\n",
    "def create_Micro_C_arrays(gene_annotations_path, microc_path, savepath, shift: int = 500000, imputation_value: float = 1e-14) -> None:\n",
    "    \"\"\"\n",
    "    This function is aimed to retreive the contact maps for each sample.\n",
    "    \n",
    "    Args:\n",
    "    id_path: path to file with ids (including the file's name).\n",
    "    input_path: path to the input mcool (contact matrix) file.\n",
    "    output_path: path to the output folder where all the new cropped matrices \n",
    "                 will be stored as npy arrays named after the central gene.\n",
    "    shift: number of basepairs before and after the TSS of the central gene that we\n",
    "           want to keep in the sample\n",
    "    imputation_value: value that will be used to replace missing values. \n",
    "                      Default to 1e-5, minimum in Hsieh et al. is ca. 1,98e-5.\n",
    "\n",
    "    Returns:\n",
    "        The function creates .npy arrays for the contact matrices matching the dimensions\n",
    "        of the chromatin mark profiles.\n",
    "\n",
    "    Notes:\n",
    "        Signal ranges from 1e-5 to 1e0. Nans after balancing and 0 count bins are set to be 1e-10 \n",
    "        (same number of orders of magnitude below the first actual contact bin detected). \n",
    "        The signal is then upscaled 5 orders of magnitude and log10 transformed, leading to matrices with values \n",
    "        from 0.5\n",
    "    \"\"\"\n",
    "\n",
    "    gene_annotations_df = pd.read_csv(gene_annotations_path, sep=\"\\t\")\n",
    "    # Open contact file (mcool) as a cooler object\n",
    "    c = cooler.Cooler(microc_path)\n",
    "\n",
    "    split_list = [\"training\",\"test\"]\n",
    "    for split in split_list:\n",
    "        (savepath / \"microC\" / f\"{split}\").mkdir(parents=True, exist_ok = True)\n",
    "        (savepath/ \"microC_rotated\" / f\"{split}\").mkdir(parents=True, exist_ok = True)\n",
    "\n",
    "    # Read the file with gene coordinates\n",
    "    for (ID,chrom,Start,End,Strand,Name,CRE_type) in gene_annotations_df.values:\n",
    "\n",
    "        split = \"test\" if chrom == \"chr4\" else \"training\"\n",
    "        TSS = int(Start) if Strand == '+' else int(End)\n",
    "\n",
    "        window_start = TSS - shift\n",
    "        window_end = TSS + shift\n",
    "\n",
    "        try:\n",
    "            c_matrix = c.matrix(balance=True).fetch(f'{chrom}:{window_start}-{window_end}')\n",
    "            c_matrix = np.nan_to_num(c_matrix, nan=imputation_value) # Nans to imputation value\n",
    "            c_matrix[c_matrix == 0.] = imputation_value # Zeros to imputation value\n",
    "            c_matrix = np.log10(c_matrix) # Matrix is log10 transformed\n",
    "            \n",
    "            if c_matrix.shape == (626,626):\n",
    "                \n",
    "                anti_c_matrix = np.fliplr(np.flipud(c_matrix))\n",
    "\n",
    "                np.save(savepath / \"microC\" / f\"{split}\" / f\"{ID}_forward.npy\", c_matrix )\n",
    "                np.save(savepath / \"microC\" / f\"{split}\" / f\"{ID}_rev.npy\", anti_c_matrix)\n",
    "\n",
    "            else:\n",
    "                logging.info(f\"ID: {ID}, Original Shape: {c_matrix.shape}\")\n",
    "\n",
    "            # Rotate and crop\n",
    "            rotated_array = rotate(c_matrix, 45, reshape= False)\n",
    "            crop_pixel = np.where(rotated_array != 0)[1][0]\n",
    "            final_array = rotated_array[crop_pixel:-crop_pixel,:]\n",
    "            cut_pixel = int(len(final_array)/2)\n",
    "            final_array = final_array[:cut_pixel]\n",
    "\n",
    "            if final_array.shape == (129,626):\n",
    "                np.save(savepath / \"microC_rotated/\" / f\"{split}/\" / f\"{ID}_forward.npy\", final_array, allow_pickle=False)\n",
    "                np.save(savepath / \"microC_rotated/\" / f\"{split}/\" / f\"{ID}_rev.npy\", np.fliplr(final_array), allow_pickle=False) #flip over sequence axis\n",
    "            else:\n",
    "                logging.info(f\"ID: {ID}, Rotated Shape: {final_array.shape}\")\n",
    "\n",
    "        except:\n",
    "            logging.info(f\"{ID} raised an error\")\n",
    "\n",
    "\n",
    "############################### Script #################################################\n",
    "\n",
    "path = \"../\" # \"../data/raw_data/\"\n",
    "microc_path = path + \"/GEO_files/GSE130275_mESC_WT_combined_2.6B.mcool::/13\" # Until 16\n",
    "gene_annotations_path = Path(path) / \"annotations\" / \"Final_gene_annotations.tsv\" # Gene coordinates\n",
    "savepath = Path(path) / \"inputs\" \n",
    "savepath.mkdir(exist_ok = True, parents=True)\n",
    "\n",
    "# Start logger\n",
    "LOG_FILENAME = \"MicroC_data_obtention.log\"\n",
    "logging.basicConfig(filename=path + LOG_FILENAME, level=logging.INFO)  \n",
    "\n",
    "\n",
    "mcool_summary(microc_path)           \n",
    "min_dataset, max_dataset = find_dataset_min_max(microc_path)\n",
    "print(\"Log 10 (min)=\",np.log10(min_dataset), \"Max:\", max_dataset)\n",
    "\n",
    "create_Micro_C_arrays(gene_annotations_path, microc_path, savepath , imputation_value = min_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile create_microC_arrays.py\n",
    "\n",
    "import multiprocessing\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cooler\n",
    "import logging\n",
    "from scipy.ndimage import rotate\n",
    "\n",
    "########### Functions #####################################\n",
    "def find_dataset_min_max(input_path: str) -> float:\n",
    "    # Open contact file (mcool) as a cooler object\n",
    "    c = cooler.Cooler(input_path)\n",
    "    bin_weights = c.bins()['weight'][:]\n",
    "    return (np.min(bin_weights), np.max(bin_weights))\n",
    "\n",
    "def plot_histogram_dataset(input_path: str, bins = 1000):\n",
    "    # Open contact file (mcool) as a cooler object\n",
    "    c = cooler.Cooler(input_path)\n",
    "    bin_weights = c.bins()['weight'][:]\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.hist(bin_weights, bins=bins)\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "def mcool_summary(microc_path):\n",
    "    c = cooler.Cooler(microc_path)\n",
    "\n",
    "    # Inspecting what is inside the cooler object:\n",
    "    metadata = c.info\n",
    "    chromosomes = c.chromnames\n",
    "    extent = c.extent('chr2')\n",
    "\n",
    "    print(\"Metadata:\\n\", metadata)\n",
    "    print(\"Chromosomes:\\n\", chromosomes)\n",
    "    print(\"Extent:\\n\", extent)\n",
    "\n",
    "def process_microC_data(args):\n",
    "    (gene_annotations_path, microc_path, savepath, shift, imputation_value, row) = args\n",
    "    ID, chrom, Start, End, Strand, Name, CRE_type = row\n",
    "    split = \"test\" if chrom == \"chr4\" else \"training\"\n",
    "    TSS = int(Start) if Strand == '+' else int(End)\n",
    "\n",
    "    c = cooler.Cooler(microc_path)\n",
    "    window_start = TSS - shift\n",
    "    window_end = TSS + shift\n",
    "\n",
    "    try:\n",
    "        # Fetch and process the matrix\n",
    "        c_matrix = c.matrix(balance=True).fetch(f'{chrom}:{window_start}-{window_end}')\n",
    "        c_matrix = np.nan_to_num(c_matrix, nan=imputation_value)\n",
    "        c_matrix[c_matrix == 0.] = imputation_value\n",
    "        c_matrix = np.log10(c_matrix)\n",
    "\n",
    "        if c_matrix.shape == (626, 626):\n",
    "            # Save direct contact matrices\n",
    "            np.save(savepath / \"microC\" / split / f\"{ID}_forward.npy\", c_matrix)\n",
    "            anti_c_matrix = np.fliplr(np.flipud(c_matrix))\n",
    "            np.save(savepath / \"microC\" / split / f\"{ID}_rev.npy\", anti_c_matrix)\n",
    "\n",
    "            # Rotate, crop, and save rotated arrays\n",
    "            rotated_array = rotate(c_matrix, 45, reshape=False)\n",
    "            crop_pixel = np.where(rotated_array != 0)[1][0]\n",
    "            final_array = rotated_array[crop_pixel:-crop_pixel, :]\n",
    "            cut_pixel = int(len(final_array) / 2)\n",
    "            final_rotated_array = final_array[:cut_pixel]\n",
    "\n",
    "            # Ensuring rotated array has the expected shape before saving\n",
    "            if final_rotated_array.shape == (129, 626):  # Adjust the expected shape based on your specific requirements\n",
    "                np.save(savepath / \"microC_rotated\" / split / f\"{ID}_forward.npy\", final_rotated_array)\n",
    "                np.save(savepath / \"microC_rotated\" / split / f\"{ID}_rev.npy\", np.fliplr(final_rotated_array))\n",
    "            else:\n",
    "                logging.info(f\"ID: {ID}, Rotated Shape: {final_rotated_array.shape} does not meet expected criteria.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.info(f\"{ID} raised an error: {e}\")\n",
    "\n",
    "def create_Micro_C_arrays_parallel(gene_annotations_path, microc_path, savepath, shift=500000, imputation_value=1e-14):\n",
    "    \"\"\"\n",
    "    Parallel version for creating Micro C arrays.\n",
    "    \"\"\"\n",
    "    gene_annotations_df = pd.read_csv(gene_annotations_path, sep=\"\\t\")\n",
    "    \n",
    "    args = [(gene_annotations_path, microc_path, savepath, shift, imputation_value, row) for index, row in gene_annotations_df.iterrows()]\n",
    "\n",
    "    with multiprocessing.Pool(processes=multiprocessing.cpu_count() - 1) as pool:\n",
    "        pool.map(process_microC_data, args)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Define paths\n",
    "    path = Path(\"../\")\n",
    "    microc_path = path / \"GEO_files/GSE130275_mESC_WT_combined_2.6B.mcool::/13\" \n",
    "    gene_annotations_path = path / \"annotations/Final_gene_annotations.tsv\"\n",
    "    savepath = path / \"inputs\"\n",
    "    savepath.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Setup logging\n",
    "    LOG_FILENAME = \"MicroC_data_obtention.log\"\n",
    "    logging.basicConfig(filename=savepath / LOG_FILENAME, level=logging.INFO, format='%(asctime)s %(message)s')\n",
    "\n",
    "    # Create directories for output\n",
    "    for split in [\"training\", \"test\"]:\n",
    "        (savepath / \"microC\" / split).mkdir(parents=True, exist_ok=True)\n",
    "        (savepath / \"microC_rotated\" / split).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Run the processing in parallel\n",
    "    create_Micro_C_arrays_parallel(str(gene_annotations_path), str(microc_path), savepath, imputation_value=1e-14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python create_microC_arrays.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epigenomic Data acquisition:\n",
    "\n",
    "**Genomic tracks**\n",
    "\n",
    "The used genomic tracks can be obtained from NCBI GEO under accession numbers GSE146328 and GSE186349, which refer to papers:\n",
    "- Narita, T., Higashijima, Y., Kilic, S. et al. Acetylation of histone H2B marks active enhancers and predicts CBP/p300 target genes. Nat Genet 55, 679–692 (2023). https://doi.org/10.1038/s41588-023-01348-4\n",
    "- Takeo Narita, Shinsuke Ito, Yoshiki Higashijima, Wai Kit Chu, Katrin Neumann, Jonas Walter, Shankha Satpathy, Tim Liebner, William B. Hamilton, Elina Maskey, Gabriela Prus, Marika Shibata, Vytautas Iesmantavicius, Joshua M. Brickman, Konstantinos Anastassiadis, Haruhiko Koseki, Chunaram Choudhary,\n",
    "Enhancers are activated by p300/CBP activity-dependent PIC assembly, RNAPII recruitment, and pause release,\n",
    "Molecular Cell,\n",
    "Volume 81, Issue 10,\n",
    "2021,\n",
    "Pages 2166-2182.e6,\n",
    "ISSN 1097-2765,\n",
    "https://doi.org/10.1016/j.molcel.2021.03.008.\n",
    "(https://www.sciencedirect.com/science/article/pii/S1097276521001763)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = {\"EU_Seq_Ctrl.bw\":\"https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE146326&format=file&file=GSE146326%5FEUSeq%5FmESC%5FA485%5FTimecourse%5F0min%2En2%2Esmooth%2Ebw\", #p300 paper\n",
    "#\"EU_Seq_Treated.bw\":\"https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE146326&format=file&file=GSE146326%5FEUSeq%5FmESC%5FA485%5FTimecourse%5F60min%2En2%2Esmooth%2Ebw\", #p300\n",
    "\"ATAC_Seq.bw\": \"https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE146325&format=file&file=GSE146325%5FATACSeq%2EESC%5FCtrl%2Ebw\", #p300\n",
    "\"H3K4me3.bw\":\"https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE186349&format=file&file=GSE186349%5FESC%5FCtrl%5FTC0%5FH3K4me3%5FCC10%2En2%2Esmooth%2Ebw\",#H2B paper\n",
    "#\"H2BK20ac.bw\": \"https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE186349&format=file&file=GSE186349%5FESC%5FCtrl%5FTC0%5FH2BK20ac%2Eab177430%5FCC8%2ECC15%2En2%2Esmooth%2Ebw\", #H2B paper\n",
    "\"H3K27ac.bw\":\"https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE186349&format=file&file=GSE186349%5FESC%5FCtrl%5FTC0%5FH3K27ac%2Eab4729%5FCC1%2ECC11%2En2%2Esmooth%2Ebw\",\n",
    "\"H3K27me3.bw\": \"https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE146324&format=file&file=GSE146324%5FChIP%2EESC%5FCtrl%5FH3K27me3%5FTC0%2En2%2Esmooth%2Ebw\"} #p300 paper\n",
    "\n",
    "savepath = Path(\"../GEO_files/\")\n",
    "\n",
    "download_files(links, savepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "From the bigwig files from NCBI's GEO, we will obtain:\n",
    "- Input numpy arrays of shape ($n_{tracks}$,$n_{bins}$) = (4,10001) with enrichments of ATAC, H3K4me3, H2BK20ac and H3K27me3 (in rows) at 100bp resolution (where bins are in columns).\n",
    "- Target csv files containing target EU-seq profiles matching each ID.npy input array cropped to the desired distance. Targets are given in reads unless otherwise stated.\n",
    "\n",
    "We will create samples for forward strand and reverse strand by flipping the profiles and targets. They will ve called ID_forward.npy and ID_rev.npy.\n",
    "\n",
    "> ⏰ Note: This step takes quite some time. To speed up computations, we allowed multiprocessing to parallelize the creation of samples. We run it using a SLURM-based cluster, which allowed us to specify the number of CPUs:\n",
    ">\n",
    "> ```srun --cpus-per-task=128 -- python create_landscape_arrays_and_targets.py ```\n",
    ">\n",
    "> This took more or less a couple of hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting create_landscape_arrays_and_targets.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile create_landscape_arrays_and_targets.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyBigWig\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "from scipy.ndimage import gaussian_filter1d, rotate\n",
    "\n",
    "\n",
    "# Define your existing functions for creating input arrays, target arrays, and visualization here\n",
    "# create_input_array, create_target_array, visualize_input_array, visualize_target_array\n",
    "\n",
    "track_dict = {0:{\"name\":\"ATAC-seq\",\"function\":\"Chromatin accessibility\",\"color\":\"k\"},\n",
    "            1:{\"name\":\"H3K4me3\",\"function\":\"Promoter\",\"color\":\"r\"},\n",
    "            2:{\"name\":\"H3K27ac\",\"function\":\"Enhancer\",\"color\":\"b\"},\n",
    "            3:{\"name\":\"H3K27me3\",\"function\":\"Chromatin silencing\",\"color\":\"g\"}}\n",
    "\n",
    "def create_input_array(data_path: Path,\n",
    "                       input_shift: int,\n",
    "                       n_input_tracks: int, \n",
    "                       n_input_bins:int,\n",
    "                       bw_input_track_list:list,\n",
    "                       ID: str,\n",
    "                       TSS: int,\n",
    "                       chrom: str):\n",
    "    \"\"\"\n",
    "    Inputs will be centered at the TSS of the gene:\n",
    "      o _pos sample will be oriented left to right and neg will be oriented right to left.\n",
    "      o _neg sample \"\" right to left\n",
    "    \"\"\"\n",
    "    input_sample = np.empty((n_input_tracks,n_input_bins))\n",
    "    for i,bw_path in enumerate(bw_input_track_list):\n",
    "        try:\n",
    "            bw = pyBigWig.open(str(data_path / bw_path), \"r\")\n",
    "            stats = bw.stats(chrom,TSS-input_shift,TSS+input_shift,type=\"mean\",nBins=n_input_bins)\n",
    "            bw.close()\n",
    "            \n",
    "            stats = np.array([float(value) if value is not None else 0. for value in stats])\n",
    "            stats = np.clip(np.array(stats),0,None) # ReLU\n",
    "            input_sample[i] = stats\n",
    "        except:\n",
    "            logging.info(f\"{ID} input landscape coordinates are out of bounds.\")\n",
    "    \n",
    "    return input_sample\n",
    "\n",
    "\n",
    "def create_target_array(data_path: Path,\n",
    "                        output_shift: int,\n",
    "                        n_output_bins:int,\n",
    "                        bw_target_track_list:list,\n",
    "                        ID: str,\n",
    "                        TSS: int,\n",
    "                        chrom: str,\n",
    "                        sigma:int,\n",
    "                        binsize: int):\n",
    "\n",
    "    \"\"\"\n",
    "    Targets are centered at the TSS, and obtained at a 20 bp resolution, smoothed and downsized to 1kbp resolution.\n",
    "    \"\"\"\n",
    "    target_sample_pos = np.array([])\n",
    "    target_sample_neg = np.array([])\n",
    "    for i,bw_path in enumerate(bw_target_track_list):\n",
    "        try:\n",
    "            bw = pyBigWig.open(str(data_path / bw_path), \"r\")\n",
    "            stats = bw.stats(chrom,TSS-output_shift,TSS+output_shift,type=\"mean\",nBins=n_output_bins)\n",
    "            bw.close()\n",
    "            stats = np.array([float(value) if value is not None else 0. for value in stats])\n",
    "            stats = np.clip(np.array(stats),0,None)\n",
    "            stats = gaussian_filter1d(stats, sigma=sigma)\n",
    "            target_cond = np.zeros_like(stats)\n",
    "\n",
    "            # Averaging over a number of bins\n",
    "            for i in range(binsize):\n",
    "                target_cond += 1 / binsize * np.roll(stats, -i)\n",
    "            target_cond = target_cond[::binsize]\n",
    "            target_sample_pos = np.concatenate([target_sample_pos,target_cond])\n",
    "            target_sample_neg = np.concatenate([target_sample_neg,np.flip(target_cond)])\n",
    "        except:\n",
    "            logging.info(f\"{ID} target coordinates are out of bounds.\")\n",
    "\n",
    "    return target_sample_pos.flatten(), target_sample_neg.flatten()\n",
    "\n",
    "def visualize_input_array(a,\n",
    "                     cropped_bins : int = 4400,\n",
    "                     scaling_factor: float = 1.,\n",
    "                     track_dict: dict = track_dict):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to visualize an input numpy array.\n",
    "    \"\"\"\n",
    "\n",
    "    fig, axs = plt.subplots(len(track_dict), 2, figsize=(20,8), sharex = True)\n",
    "\n",
    "    # Plot the scaled arrays\n",
    "    for j,line in enumerate(a):\n",
    "        #axs[0][0].set_title(file.name[:-4])\n",
    "        axs[j][0].plot(np.arange(-len(line)//2,len(line)//2),line*scaling_factor, color=track_dict[j][\"color\"], label=track_dict[j][\"name\"], lw=.2)\n",
    "        axs[j][0].fill_between(np.arange(-len(line)//2,len(line)//2),line*scaling_factor, color=track_dict[j][\"color\"], alpha=1)\n",
    "        #axs[j][0].set_yticks([]) #([0,int(np.max(line))], [0,int(np.max(line))], fontsize=16)\n",
    "        #axs[j][0].set_ylim(0,350)\n",
    "        axs[j][0].set_ylabel(track_dict[j][\"name\"])\n",
    "        axs[-1][0].set_xlabel(\"Distance to TSS (kbp)\", fontsize= 16)\n",
    "        #axs[-1][0].set_xticks([])#([-5000,0,5000], [-500, 0, 500], fontsize= 16)\n",
    "        axs[-1][0].set_xlim((-5000,5000))\n",
    "\n",
    "        axs[j][1].plot(np.arange(-len(line)//2,len(line)//2)[cropped_bins:-cropped_bins],line[cropped_bins:-cropped_bins], color=track_dict[j][\"color\"], label=track_dict[j][\"name\"], lw=.2)\n",
    "        axs[j][1].fill_between(np.arange(-len(line)//2,len(line)//2)[cropped_bins:-cropped_bins],line[cropped_bins:-cropped_bins], color=track_dict[j][\"color\"], alpha=.6)\n",
    "\n",
    "    return fig\n",
    "        \n",
    "def visualize_target_array(a,\n",
    "                           scaling_factor: float = 1.):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to visualize target profiles.\n",
    "    \"\"\"\n",
    "    ctrl = a[:len(a)//2]\n",
    "    #treated = a[len(a)//2:]\n",
    "    fig = plt.figure(figsize=(20,8))\n",
    "    plt.plot(np.arange(-len(ctrl)//2,len(ctrl)//2),ctrl*scaling_factor,label=\"ctrl\",color=\"grey\",lw=.2)\n",
    "    plt.fill_between(np.arange(-len(ctrl)//2,len(ctrl)//2),ctrl*scaling_factor, color = \"silver\" , alpha=1)\n",
    "\n",
    "    #plt.plot(np.arange(-len(treated)//2,len(treated)//2),treated*scaling_factor,label=\"treated\", color=\"royalblue\", lw=.2)\n",
    "    #plt.fill_between(np.arange(-len(treated)//2,len(treated)//2),treated*scaling_factor, label=\"treated\", color=\"royalblue\",alpha=1)\n",
    "\n",
    "    return fig\n",
    "\n",
    "def process_gene_data(data_path, bw_input_track_list, bw_target_track_list, input_shift, output_shift, n_input_bins, n_input_tracks, n_output_bins, sigma, binsize, INPUT_SHAPE, OUTPUT_LENGTH, path, split_list, figure_path, PLOT_IDS, target_files, row):\n",
    "    ID, chrom, Start, End, Strand, Name, CRE_type = row\n",
    "    split = \"test\" if chrom == \"chr4\" else \"training\"\n",
    "    TSS = int(Start) if Strand == '+' else int(End)\n",
    "\n",
    "    input_sample = create_input_array(data_path, input_shift, n_input_tracks, n_input_bins, bw_input_track_list, ID, TSS, chrom)\n",
    "    target_sample_pos, target_sample_neg = create_target_array(data_path, output_shift, n_output_bins, bw_target_track_list, ID, TSS, chrom, sigma, binsize)\n",
    "\n",
    "    if (input_sample.shape == INPUT_SHAPE) and (len(target_sample_pos) == OUTPUT_LENGTH):\n",
    "        np.save(path / \"inputs\" / \"landscape_arrays\" / split / f\"{ID}_forward.npy\", input_sample)\n",
    "        np.save(path / \"inputs\" / \"landscape_arrays\" / split / f\"{ID}_rev.npy\", np.flip(input_sample, axis=1))\n",
    "        \n",
    "        target_file_path = target_files[split]\n",
    "        with open(target_file_path, 'a') as file:\n",
    "            file.write(f\"{ID}_forward,\" + \",\".join(map(str, target_sample_pos)) + \"\\n\")\n",
    "            file.write(f\"{ID}_rev,\" + \",\".join(map(str, target_sample_neg)) + \"\\n\")\n",
    "    else:\n",
    "        logging.info(f\"{ID} : Input or target did not match the expected shape.\")\n",
    "\n",
    "    if ID in PLOT_IDS:\n",
    "        fig = visualize_input_array(input_sample)\n",
    "        fig.savefig(figure_path / f\"{ID}_forward.png\")\n",
    "        fig = visualize_target_array(target_sample_pos)\n",
    "        fig.savefig(figure_path / f\"{ID}_forward_target.png\")\n",
    "        fig = visualize_input_array(np.flip(input_sample, axis=1))\n",
    "        fig.savefig(figure_path / f\"{ID}_rev.png\")\n",
    "        fig = visualize_target_array(target_sample_neg)\n",
    "        fig.savefig(figure_path / f\"{ID}_rev_target.png\")\n",
    "\n",
    "############################ Script ###################\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    path = Path(\"../\")\n",
    "    data_path = path / \"GEO_files\"\n",
    "    figure_path = path / \"figures\"\n",
    "    figure_path.mkdir(parents=True, exist_ok=True)\n",
    "    target_path = path / \"targets\"\n",
    "    target_path.mkdir(exist_ok=True, parents=True)\n",
    "    split_list = [\"training\",\"test\"]\n",
    "    output_dirs = [(path / \"inputs\" / \"landscape_arrays\" / f\"{split}\").mkdir(parents=True, exist_ok = True) for split in split_list]\n",
    "\n",
    "    PLOT_IDS = [\"ENSMUSG00000051951.5\", \"ENSMUSG00000025902\", \"ENSMUSG00000033845.13\", \"ENSMUSG00000025903.14\"]\n",
    "\n",
    "    # Start logger\n",
    "    LOG_FILENAME = \"/Landscape_data_obtention.log\"\n",
    "    logging.basicConfig(filename=str(path) + LOG_FILENAME, level=logging.INFO)  \n",
    "\n",
    "    gene_annotations_path = path / \"annotations\" / \"Final_gene_annotations.tsv\"\n",
    "    gene_annotations_df = pd.read_csv(gene_annotations_path, sep=\"\\t\")\n",
    "\n",
    "    input_shift = 500050 # In basepairs, from TSS to one side + central bin (100bp res)\n",
    "    output_shift = 200010 # Output in 20 bp resolution (to be smoothed and downsampled)\n",
    "    n_input_bins = 10001\n",
    "    n_input_tracks = 4\n",
    "    n_output_bins = 20001\n",
    "    sigma = 50\n",
    "    binsize = 50\n",
    "    INPUT_SHAPE = (4, 10001)\n",
    "    OUTPUT_LENGTH = 401 #802\n",
    "    N_BINS = 200\n",
    "    bw_input_track_list = [\"ATAC_Seq.bw\", \"H3K4me3.bw\", \"H3K27ac.bw\", \"H3K27me3.bw\"]\n",
    "    bw_target_track_list = [\"EU_Seq_Ctrl.bw\"] # \"EU_Seq_Treated.bw\"\n",
    "\n",
    "    target_files = {split: (path / \"targets\" / f\"{split}_targets.csv\") for split in [\"training\", \"validation\", \"test\"]}\n",
    "    for file_path in target_files.values():\n",
    "        with open(file_path, 'w') as file:\n",
    "            file.write(\"ID,\" + \",\".join([f\"{i}{cond}\" for cond in [\"_ctrl\"] for i in range(-N_BINS,N_BINS+1)]) + \"\\n\")\n",
    "\n",
    "    process_args = (data_path, bw_input_track_list, bw_target_track_list, input_shift, output_shift, n_input_bins, n_input_tracks, n_output_bins, sigma, binsize, INPUT_SHAPE, OUTPUT_LENGTH, path, [\"training\", \"validation\", \"test\"], figure_path, PLOT_IDS, target_files)\n",
    "\n",
    "    with multiprocessing.Pool(processes=multiprocessing.cpu_count() - 1) as pool:\n",
    "        pool.starmap(process_gene_data, [(process_args + (row,)) for row in gene_annotations_df.to_numpy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The created python file can be run from a jupyter notebook cell as:\n",
    "> ```! python create_landscape_arrays_and_targets.py ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create report of missing samples**\n",
    "\n",
    "Not all samples could be created in all data modalities given boundary issues (genes close to the end of the chromosome). Hence, we will create a report of missing samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile keep_only_sample_intersection.py\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Define base paths for inputs and targets\n",
    "base_input_path = Path(\"../inputs\")\n",
    "base_target_path = Path(\"../targets\")\n",
    "\n",
    "# Modalities and splits\n",
    "data_modalities = ['landscape_arrays', 'microC', 'microC_rotated']\n",
    "splits = ['training', 'test']\n",
    "\n",
    "# Initialize a list to collect rows for the new DataFrame\n",
    "rows_list = []\n",
    "\n",
    "# Read target samples and store available samples for each modality\n",
    "for split in splits:\n",
    "    targets_df = pd.read_csv(base_target_path / f\"{split}_targets.csv\", index_col=0)\n",
    "    target_samples = set(targets_df.index)\n",
    "\n",
    "    for modality in data_modalities:\n",
    "        modality_path = base_input_path / modality / split\n",
    "        modality_samples = {file.stem for file in modality_path.glob('*.npy')}\n",
    "\n",
    "        # Find missing samples\n",
    "        missing_samples = target_samples - modality_samples\n",
    "        extra_samples = modality_samples - target_samples\n",
    "\n",
    "        # Collect missing samples\n",
    "        rows_list.extend([{'Split': split, 'Modality': modality, 'Sample': sample} for sample in missing_samples])\n",
    "        # Collect extra samples (in modalities but not in targets)\n",
    "        rows_list.extend([{'Split': split, 'Modality': 'targets', 'Sample': sample} for sample in extra_samples])\n",
    "\n",
    "# Create a DataFrame from the collected rows\n",
    "missing_samples_df = pd.DataFrame(rows_list)\n",
    "\n",
    "# Save the DataFrame as a CSV\n",
    "missing_samples_df.to_csv(\"../annotations/missing_samples_report.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now remove all missing samples from the rest of the modalities, including targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile remove_missing_samples.py\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the report of missing samples\n",
    "missing_samples_df = pd.read_csv(\"../annotations/missing_samples_report.csv\")\n",
    "\n",
    "# Base paths for inputs and targets\n",
    "base_input_path = Path(\"../inputs\")\n",
    "base_target_path = Path(\"../targets\")\n",
    "\n",
    "# Modalities and splits\n",
    "data_modalities = ['landscape_arrays', 'microC', 'microC_rotated', 'targets']  # Include 'targets' as a modality for uniform processing\n",
    "splits = ['training', 'test']\n",
    "\n",
    "# Function to remove a sample from a specific modality and split\n",
    "def remove_sample_from_modality(split, modality, sample):\n",
    "    if modality == 'targets':\n",
    "        target_file_path = base_target_path / f\"{split}_targets.csv\"\n",
    "        targets_df = pd.read_csv(target_file_path, index_col=0)\n",
    "        if sample in targets_df.index:\n",
    "            targets_df = targets_df.drop(sample)\n",
    "            targets_df.to_csv(target_file_path)\n",
    "    else:\n",
    "        modality_path = base_input_path / modality / split\n",
    "        file_path = modality_path / f\"{sample}.npy\"  # Assuming .npy extension\n",
    "        if file_path.exists():\n",
    "            file_path.unlink()\n",
    "\n",
    "# Iterate over the missing samples DataFrame\n",
    "for _, row in missing_samples_df.iterrows():\n",
    "    split, missing_modality, sample = row['Split'], row['Modality'], row['Sample']\n",
    "    for modality in data_modalities:\n",
    "        if modality != missing_modality:  # Remove the sample from all modalities except where it's missing\n",
    "            remove_sample_from_modality(split, modality, sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Enhancer perturbations \n",
    "\n",
    "The goal of this section is to create slightly modified inputs, emulating the effect of an epigenetic perturbation at a given locus. We tried two perturbational scenarios:\n",
    "- P1: Only setting to zero the enhancer mark at enhancer loci (ELS) that fall within the boundaries of the predicted region.\n",
    "- P2: Setting all marks to zero except the silencing mark H3K27me3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create gene and enhancer coordinate table**\n",
    "\n",
    "We will create a table with all enhancers and genes that appear in a sample given the target boundaries from the TSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/maps/projects/rasmussen/data/enhancer_logic_project/claster_env/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "def calculate_tss(genes_df):\n",
    "    # Calculate TSS for genes based on strand information\n",
    "    genes_df['TSS'] = genes_df.apply(lambda row: row['Start'] if row['Strand'] == '+' else row['End'], axis=1)\n",
    "    return genes_df\n",
    "\n",
    "def process_genes_chunk(args):\n",
    "    genes_chunk, genes_df, enhancers_df, boundary = args\n",
    "    results_list = []  # Initialize an empty list to store intermediate results\n",
    "\n",
    "    for _, gene in genes_chunk.iterrows():\n",
    "        tss = gene['TSS']\n",
    "        chromosome = gene['chr']\n",
    "\n",
    "        # Adjust selection criteria for gene entities: overlap at some point with the prediction window\n",
    "        gene_entities = genes_df[(genes_df['chr'] == chromosome) & \n",
    "                         (genes_df['Start'] <= tss + boundary) & \n",
    "                         (genes_df['End'] >= tss - boundary)]\n",
    "\n",
    "        # Process gene entities\n",
    "        for _, entity in gene_entities.iterrows():\n",
    "            entity_start = entity['Start'] - tss\n",
    "            entity_end = entity['End'] - tss\n",
    "            results_list.append({\n",
    "                'Ref_gene': gene['ID'], \n",
    "                'Ref_gene_chromosome': chromosome, \n",
    "                'Ref_gene_strand': gene['Strand'],\n",
    "                'Entity_ID': entity['ID'], \n",
    "                'Entity_rel_Start': entity_start, \n",
    "                'Entity_rel_End': entity_end\n",
    "            })\n",
    "        \n",
    "        # Adjust selection criteria for enhancer entities: fully contained inside prediction window\n",
    "        enhancer_entities = enhancers_df[(enhancers_df['chr'] == chromosome) & \n",
    "                                         (enhancers_df['end'] <= tss + boundary) & \n",
    "                                         (enhancers_df['start'] >= tss - boundary)]\n",
    "        # Process enhancer entities\n",
    "        for _, entity in enhancer_entities.iterrows():\n",
    "            entity_start = entity['start'] - tss\n",
    "            entity_end = entity['end'] - tss\n",
    "            results_list.append({\n",
    "                'Ref_gene': gene['ID'], \n",
    "                'Ref_gene_chromosome': chromosome, \n",
    "                'Ref_gene_strand': gene['Strand'],\n",
    "                'Entity_ID': entity['cCRE_accession'], \n",
    "                'Entity_rel_Start': entity_start, \n",
    "                'Entity_rel_End': entity_end\n",
    "            })\n",
    "\n",
    "    # Convert the results list to a DataFrame\n",
    "    return pd.DataFrame(results_list)\n",
    "\n",
    "def main():\n",
    "    genes_df = pd.read_csv(\"../annotations/Final_gene_annotations.tsv\", sep=\"\\t\")\n",
    "    enhancers_df = pd.read_csv(\"../annotations/Final_Enhancer_annotation.tsv\", sep=\"\\t\")\n",
    "    boundary = 200010 \n",
    "\n",
    "    genes_df = calculate_tss(genes_df)\n",
    "\n",
    "\n",
    "    n_processes = cpu_count()\n",
    "    genes_chunks = np.array_split(genes_df, n_processes)\n",
    "    args = [(chunk, genes_df, enhancers_df, boundary) for chunk in genes_chunks]\n",
    "\n",
    "    with Pool(processes=n_processes) as pool:\n",
    "        results = pool.map(process_genes_chunk, args)\n",
    "\n",
    "    result_df = pd.concat(results, ignore_index=True)\n",
    "    result_df.to_csv('../annotations/gene_enhancer_relationships_corrected.tsv',sep=\"\\t\", index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create perturbed inputs and corresponding targets.**\n",
    "\n",
    "The goal of this part of the pipeline will be to assess what genes are affected when silencing single enhancers.\n",
    "\n",
    "- We will only perturb/silence enhancers that were originally active, i.e. for which the average read count in the enhancer region is above a certain threshold for the enhancer activity mark H3K27ac. The threshold was set to an average signal of 10 reads across the range.\n",
    "- Previous H3K27me3 levels will be kept.\n",
    "\n",
    "> _Note: EIR requires to have matching targets even when predicting new samples and therefore we will create dummy target tables filled with zeros._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting create_perturbed_arrays_and_targets.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile create_perturbed_arrays_and_targets.py \n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.integrate import simpson\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "def get_track_index(track_name):\n",
    "    \"\"\"Maps track names to their respective index in the numpy array.\"\"\"\n",
    "    track_order = {\"ATAC\": 0, \"H3K4me3\": 1, \"H3K27ac\": 2, \"H3K27me3\": 3}\n",
    "    return track_order.get(track_name, -1)  # Returns -1 if track name not found\n",
    "\n",
    "def modify_array(args):\n",
    "    \"\"\"\n",
    "    Modifies numpy arrays for a single reference gene by perturbing specified enhancers.\n",
    "    Returns a list of kept enhancers based on a threshold in the H3K27ac channel.\n",
    "    \"\"\"\n",
    "    ref_gene, enhancers, resolution, modification_dict, input_arrays_path, perturbed_arrays_path, threshold = args\n",
    "    kept_enhancers = []  # To keep track of enhancers that meet the threshold criteria\n",
    "\n",
    "    for _, enhancer in enhancers.iterrows():\n",
    "        try:\n",
    "            array_path = input_arrays_path / f\"{ref_gene}_forward.npy\"\n",
    "            gene_array = np.load(array_path)\n",
    "\n",
    "            central_idx = gene_array.shape[1] // 2\n",
    "            start_idx = central_idx + int(enhancer['Entity_rel_Start'] // resolution)\n",
    "            end_idx = central_idx + int(enhancer['Entity_rel_End'] // resolution)\n",
    "\n",
    "            h3k27ac_idx = get_track_index(\"H3K27ac\")\n",
    "            enhancer_mean = np.mean(gene_array[h3k27ac_idx, max(0, start_idx):min(end_idx + 1, gene_array.shape[1])])\n",
    "\n",
    "            if enhancer_mean > threshold:\n",
    "                for track_name, value in modification_dict.items():\n",
    "                    track_idx = get_track_index(track_name)\n",
    "                    gene_array[track_idx, max(0, start_idx):min(end_idx + 1, gene_array.shape[1])] = value\n",
    "\n",
    "                new_array_name = perturbed_arrays_path / f\"{ref_gene}_{enhancer['Entity_ID']}_forward.npy\"\n",
    "                np.save(new_array_name, gene_array)\n",
    "                kept_enhancers.append((ref_gene, enhancer['Entity_ID']))\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Array file for {ref_gene} not found.\")\n",
    "\n",
    "    return kept_enhancers\n",
    "\n",
    "def parallel_modify_arrays(input_table_path, input_arrays_path, perturbed_arrays_path, chromosomes, modification_dict, resolution=100, threshold=0.5):\n",
    "    \"\"\"Processes enhancer perturbations in parallel and generates a target file for kept enhancers.\"\"\"\n",
    "    df = pd.read_csv(input_table_path, sep=\"\\t\")\n",
    "    filtered_df = df[df['Ref_gene_chromosome'].isin(chromosomes)]\n",
    "    ref_genes = filtered_df['Ref_gene'].unique()\n",
    "\n",
    "    args = [(ref_gene, \n",
    "             filtered_df[(filtered_df['Ref_gene'] == ref_gene) & (~filtered_df['Entity_ID'].str.startswith('ENSMUSG'))],\n",
    "             resolution, modification_dict, input_arrays_path, perturbed_arrays_path, threshold) for ref_gene in ref_genes]\n",
    "\n",
    "    kept_enhancers = []\n",
    "    with Pool(cpu_count()) as pool:\n",
    "        for result in pool.imap(modify_array, args):\n",
    "            kept_enhancers.extend(result)\n",
    "\n",
    "    N_BINS = 200\n",
    "    condition_list = [\"_ctrl\"]\n",
    "    columns = [f\"{i}{cond}\" for cond in condition_list for i in range(-N_BINS, N_BINS + 1)]\n",
    "    index = [f\"{ref_gene}_{enhancer}_forward\" for ref_gene, enhancer in kept_enhancers]\n",
    "    perturbed_targets = pd.DataFrame(0, index=index, columns=columns)\n",
    "    perturbed_targets_path = Path('../targets/') / 'perturbed_targets.csv'\n",
    "    perturbed_targets.to_csv(perturbed_targets_path, index_label=\"ID\")\n",
    "\n",
    "# Example usage, adjust paths and parameters as needed\n",
    "input_table_path = Path(\"../annotations/gene_enhancer_relationships_corrected.tsv\")\n",
    "input_arrays_path = Path(\"../inputs/landscape_arrays/test/\")\n",
    "perturbed_arrays_path = Path(\"../inputs/perturbed_landscape_arrays/test_only_H3K27ac_silenced/\")\n",
    "perturbed_arrays_path.mkdir(parents=True, exist_ok=True)  # Ensure the directory exists\n",
    "\n",
    "chromosomes = ['chr4']\n",
    "modification_dict = {\"H3K27ac\": 0} # If all active marks silenced: {\"ATAC\": 0, \"H3K4me3\": 0, \"H3K27ac\": 0}\n",
    "resolution = 100\n",
    "threshold = 10  # Adjust the threshold as needed\n",
    "\n",
    "parallel_modify_arrays(input_table_path, input_arrays_path, perturbed_arrays_path, chromosomes, modification_dict, resolution, threshold)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the data required to train and test CLASTER!\n",
    "\n",
    "(We'll do that in the next python notebook.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Benchmark data obtention  \n",
    "**Comparing CLASTER with DNA-sequence based models: HyenaDNA and the Enformer**\n",
    "\n",
    "We will now obtain the genomic sequences corresponding to the regions constituting our samples. These will be used to:\n",
    "- Get HyenaDNA-160kb / Enformer pretrained embeddings or \n",
    "- Fine-tune HyenaDNA backbone with an addded head.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to install bedtools and samtools, two packages to handle genomic sequences:\n",
    "\n",
    "> Note: it is easier to do it directly on the terminal. Make sure to have the right environment activated.\n",
    "\n",
    "```bash\n",
    "conda install bioconda:bedtools\n",
    "conda install bioconda:samtools\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get the genome sequence**\n",
    "\n",
    "We will now obtain the mouse reference genome sequence. The genome sequence M25(GRCm38.p6) was obtained from GENCODE. \n",
    "\n",
    "https://www.gencodegenes.org/mouse/release_M25.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully downloaded GRCm38.p6.genome.fa.gz to ../GEO_files\n"
     ]
    }
   ],
   "source": [
    "links = {\"GRCm38.p6.genome.fa.gz\":\"https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M25/GRCm38.p6.genome.fa.gz\"} #p300 paper\n",
    "\n",
    "savepath = Path(\"../GEO_files/\")\n",
    "\n",
    "download_files(links, savepath)\n",
    "\n",
    "! gunzip ../GEO_files/GRCm38.p6.genome.fa.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create bed file with DNA chunk boundaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting create_sequence_boundaries.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile create_sequence_boundaries.sh\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "# Script to create BED files based on transcript IDs found in specific CSV files\n",
    "\n",
    "# Input file name\n",
    "input_file=\"../annotations/Final_gene_annotations.tsv\"\n",
    "\n",
    "# Additional CSV files\n",
    "test_targets_csv=\"../targets/test_targets.csv\"\n",
    "train_targets_csv=\"../targets/training_targets.csv\"\n",
    "\n",
    "# Output file names\n",
    "output_file1=\"../annotations/training_boundaries.bed\"\n",
    "output_file2=\"../annotations/test_boundaries.bed\"\n",
    "output_file3=\"../annotations/validation_boundaries.bed\"\n",
    "\n",
    "# Variable 'shift'\n",
    "shift=80000\n",
    "\n",
    "# Read IDs from CSV files into arrays\n",
    "IFS=$'\\n' read -d '' -r -a test_ids < <(tail -n +2 \"$test_targets_csv\" | cut -d ',' -f1)\n",
    "IFS=$'\\n' read -d '' -r -a train_ids < <(tail -n +2 \"$train_targets_csv\" | cut -d ',' -f1)\n",
    "\n",
    "# Function to check if an ID is in the arrays with _forward or _rev suffix\n",
    "id_in_targets() {\n",
    "    local id_forward=\"${1}_forward\"\n",
    "    local id_rev=\"${1}_rev\"\n",
    "    for target_id in \"${test_ids[@]}\" \"${train_ids[@]}\"; do\n",
    "        if [[ \"$target_id\" == \"$id_forward\" ]] || [[ \"$target_id\" == \"$id_rev\" ]]; then\n",
    "            return 0\n",
    "        fi\n",
    "    done\n",
    "    return 1\n",
    "}\n",
    "\n",
    "# Check if input file exists\n",
    "if [ ! -f \"$input_file\" ]; then\n",
    "    echo \"Error: Input file not found.\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Process the input file and create output files\n",
    "while IFS=$'\\t' read -r ID Chr Start End Strand Name type; do\n",
    "    if [ \"$ID\" != \"ID\" ]; then\n",
    "        # Define TSS based on the strand\n",
    "        if [[ \"$Strand\" == \"+\" ]]; then\n",
    "            TSS=$Start\n",
    "        elif [[ \"$Strand\" == \"-\" ]]; then\n",
    "            TSS=$End\n",
    "        fi\n",
    "\n",
    "        new_start=$((TSS - shift))\n",
    "        new_end=$((TSS + shift))\n",
    "        output_id=\"${ID}_forward\" # Use ID with _forward suffix for output\n",
    "\n",
    "        if id_in_targets \"$ID\"; then\n",
    "            if [ \"$Chr\" != \"chr4\" ] && [ \"$Chr\" != \"chr17\" ]; then\n",
    "                echo -e \"${Chr}\\t${new_start}\\t${new_end}\\t${output_id}\" >> \"$output_file1\"\n",
    "            fi\n",
    "\n",
    "            if [ \"$Chr\" == \"chr4\" ]; then\n",
    "                echo -e \"${Chr}\\t${new_start}\\t${new_end}\\t${output_id}\" >> \"$output_file2\"\n",
    "            fi\n",
    "\n",
    "            if [ \"$Chr\" == \"chr17\" ]; then\n",
    "                echo -e \"${Chr}\\t${new_start}\\t${new_end}\\t${output_id}\" >> \"$output_file3\"\n",
    "            fi\n",
    "        fi\n",
    "    fi\n",
    "done < \"$input_file\"\n",
    "\n",
    "echo \"Processing complete. Files generated: $output_file1, $output_file2, $output_file3\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Files generated: ../annotations/training_boundaries.bed, ../annotations/test_boundaries.bed, ../annotations/validation_boundaries.bed\n"
     ]
    }
   ],
   "source": [
    "! bash create_sequence_boundaries.sh "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create fasta files with the sequences:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "# Define paths\n",
    "ref_genome_path = \"../GEO_files/\"\n",
    "reference_genome = \"GRCm38.p6.genome.fa\"\n",
    "bed_path = \"../annotations/\"\n",
    "sequence_fasta_path = \"../inputs/DNA_sequences/\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "Path(sequence_fasta_path).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Loop through the splits and run bedtools getfasta\n",
    "splits = [\"training\", \"validation\", \"test\"]\n",
    "for split in splits:\n",
    "    # Construct file paths\n",
    "    input_fasta = f\"{ref_genome_path}{reference_genome}\"\n",
    "    input_bed = f\"{bed_path}{split}_boundaries.bed\"\n",
    "    output_fasta = f\"{sequence_fasta_path}{split}_boundaries.fasta\"\n",
    "    \n",
    "    # Construct the command\n",
    "    command = [\n",
    "        \"bedtools\", \"getfasta\",\n",
    "        \"-fi\", input_fasta,\n",
    "        \"-bed\", input_bed,\n",
    "        \"-fo\", output_fasta,\n",
    "        \"-name\"\n",
    "    ]\n",
    "    \n",
    "    # Run the command\n",
    "    subprocess.run(command, check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Enformer benchmark targets**\n",
    "\n",
    "Enformer targets are the same targets, just cropped to fit the new target window (-57 to 57 bins). Test targets are de-duplicated, i.e. we only want forward strand oriented predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File '../targets/training_targets_Enformer.csv' has been created with columns from -57 to 57.\n",
      "File '../targets/test_targets_Enformer.csv' has been created with columns from -57 to 57.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def filter_columns_by_bins(input_csv, output_csv, n_bins, TEST):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # Generate column names of interest based on the range from -N_BINS to N_BINS\n",
    "    cols_of_interest = [f\"{i}_ctrl\" for i in range(-n_bins, n_bins + 1)]\n",
    "\n",
    "    # Select the columns that exist in the DataFrame\n",
    "    selected_columns = ['ID'] + [col for col in cols_of_interest if col in df.columns]\n",
    "\n",
    "    # Filter the DataFrame to keep only the desired columns\n",
    "    filtered_df = df[selected_columns]\n",
    "\n",
    "    if TEST:\n",
    "        filtered_df = filtered_df[~filtered_df['ID'].str.contains('_rev')]\n",
    "\n",
    "    # Write the filtered DataFrame to a new CSV file\n",
    "    filtered_df.to_csv(output_csv, index=False)\n",
    "\n",
    "    print(f\"File '{output_csv}' has been created with columns from -{n_bins} to {n_bins}.\")\n",
    "\n",
    "\n",
    "for input_name,output_name in zip(['../targets/training_targets.csv','../targets/test_targets.csv'],['../targets/training_targets_Enformer.csv','../targets/test_targets_Enformer.csv']):\n",
    "    TEST = True if 'test' in input_name else False\n",
    "    filter_columns_by_bins(input_name, output_name, 57, TEST)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it!\n",
    "Now you have all files required to proceed to create, train and test CLASTER. This is done in the next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.8 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e44f9b6c2865d2d1964da278f2e72d55afaa9cfc23528d4e8089ad3422c3db6e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
