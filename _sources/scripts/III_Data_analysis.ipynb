{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA ANALYSIS NOTEBOOK\n",
    "\n",
    "This notebook was used to run the analyses and produce most of the images in the paper.\n",
    "\n",
    "> _Overview:_\n",
    "> - Get baseline predictions of the different models:\n",
    ">    - Bin by bin (pointwise)\n",
    ">    - Integrating the area of the central gene.\n",
    "> - Compare model performances\n",
    "> - **Input perturbations**: _How does nascent transcription change when we perturb the inputs?_\n",
    "> - **Input attributions**: _What is required in the input to predict a given output?_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# %%writefile claster_utils.py\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.patches as patches\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy.integrate import simpson\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from matplotlib import gridspec\n",
    "from PIL import Image\n",
    "\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "from scipy.ndimage import rotate\n",
    "\n",
    "plt.rcParams['font.family'] = 'Nimbus Roman'\n",
    "\n",
    "################################# Functions #####################################\n",
    "def read_gene_positions(csv_path, resolution=1):\n",
    "    \"\"\" \n",
    "    This function reads a csv containing genes and enhancers (entities), and creates a dictionary with the relative coordinates of\n",
    "    all entities to the TSS of the reference gene (central gene), which gives name to the sample.\n",
    "\n",
    "    Args:\n",
    "        csv_path: path to the csv with ref genes, entities and their relative coordinates.\n",
    "        resolution: distance unit we want our relative distances to be given with (1-> bp, 1000->kbp)\n",
    "\n",
    "    Returns:\n",
    "        pos_dict: dictionary given pos_dict[ref gene][entity_name] = (rel_start, rel_end)\n",
    "    \"\"\"\n",
    "    # Load the results from a CSV file\n",
    "    df = pd.read_csv(csv_path, sep=\"\\t\")\n",
    "    \n",
    "    # Initialize the dictionary to store positions\n",
    "    pos_dict = {}\n",
    "    \n",
    "    # Filter the DataFrame to get only gene entities \n",
    "    gene_entities = df[df['Entity_ID'].str.startswith('ENSMUSG')]  # Adjust the condition according to your data\n",
    "    \n",
    "    for _, row in gene_entities.iterrows():\n",
    "        ref_gene_id = row['Ref_gene']\n",
    "        entity_id = row['Entity_ID']\n",
    "        rel_start = int(round(row['Entity_rel_Start'] / resolution))\n",
    "        rel_end = int(round(row['Entity_rel_End'] / resolution))\n",
    "        \n",
    "        # Ensure the dictionary has the necessary structure\n",
    "        if ref_gene_id+'_forward' not in pos_dict:\n",
    "            pos_dict[ref_gene_id+'_forward'] = {}\n",
    "        \n",
    "        # Store the scaled and rounded start and end positions\n",
    "        pos_dict[ref_gene_id+'_forward'][entity_id+'_forward'] = [rel_start, rel_end]\n",
    "    \n",
    "    return pos_dict\n",
    "\n",
    "\n",
    "def _get_predictions(results_path: Path, N_BINS: int, condition_list: list):\n",
    "    \"\"\"\n",
    "    This function reads the prediction files that EIR yields for each target (bin_condition, e.g. -150_ctrl) in the \n",
    "    test (eirpredict) setting. Predictions are then converted into dataframes. \n",
    "    \n",
    "    Args:\n",
    "        results_path: path where the predictions are stored\n",
    "        N_BINS: number of the bins to one side of the central bin (200 normally, 57 for the Enformer predictions).\n",
    "        condition_list: list of conditions. As of now [\"_ctrl\"], but can be extended to other target conditions.\n",
    "    \n",
    "    Returns:\n",
    "        ids: names of the samples\n",
    "        predicted: predicted untransformed values\n",
    "        actual: actual values\n",
    "    \"\"\"\n",
    "    # Lists to store DataFrames for concatenation\n",
    "    predicted_dfs = []\n",
    "    actual_dfs = []\n",
    "\n",
    "    for condition in condition_list:\n",
    "        for i in range(-N_BINS, N_BINS + 1):\n",
    "            file_path = results_path / f\"expression_output/{i}{condition}/predictions.csv\"\n",
    "            predictions = pd.read_csv(file_path).set_index('ID')\n",
    "\n",
    "            # Apply ReLU to model predictions\n",
    "            predictions = predictions.clip(lower=0)\n",
    "\n",
    "            # Prepare data for concatenation.\n",
    "            predicted_column = predictions[f\"{i}{condition} Untransformed\"].rename(f\"{i}{condition}\")\n",
    "            actual_column = predictions[\"True Label Untransformed\"].rename(f\"{i}{condition}\")\n",
    "\n",
    "            predicted_dfs.append(predicted_column)\n",
    "            actual_dfs.append(actual_column)\n",
    "\n",
    "    # Concatenate all DataFrames horizontally\n",
    "    predicted = pd.concat(predicted_dfs, axis=1)\n",
    "    actual = pd.concat(actual_dfs, axis=1)\n",
    "    ids = list(predicted.index)\n",
    "\n",
    "\n",
    "    return ids, predicted, actual\n",
    "\n",
    "def plot_target_predictions(savepath, line_p, line_a, id, rel_start, rel_end, pred_area):\n",
    "    \"\"\" \n",
    "    Plot the predicted profiles. \n",
    "    \"\"\"\n",
    "    fig, axs =plt.subplots(2, figsize=(8,4), sharex = True)\n",
    "    axs[0].fill_between(np.arange(-200,201),line_p, lw=1, color='royalblue',alpha=.3, label=\"Predicted expression control\")\n",
    "    axs[1].fill_between(np.arange(-200,201),line_a, lw=1, color='silver', alpha=.6, label=\"Actual expression control\")\n",
    "    #plt.plot(np.arange(len(line_p)),line_p, lw=1, color='darkred', label=f\"{simpson(line_p[TSS:END])/WIDTH}\")\n",
    "    #plt.plot(np.arange(len(line_a)),line_a, lw=1, color='darkgreen', label=f\"{simpson(line_a[TSS:END])/WIDTH} {END}\")\n",
    "    #plt.plot(TSS,1,'go')\n",
    "    #plt.plot(END,1,'ro')\n",
    "    # for i in range(len(axs)):\n",
    "    #     axs[i].set_ylim(-10,50)\n",
    "    #     axs[i].set_xlim(-200,200)\n",
    "    #     axs[i].set_ylabel(\"Reads\", fontsize=12)\n",
    "    #     rect= patches.Rectangle((-75,-5),70,40, edgecolor='k', facecolor='none', linewidth=2)\n",
    "    #     axs[i].add_patch(rect) \n",
    "    #     if i > 1:\n",
    "    #         axs[i].annotate('', xy=(-50,10), xytext=(-50, 30), arrowprops=dict(arrowstyle='-|>,head_width=0.2,head_length=0.5', color='k'))             \n",
    "    axs[1].set_xlabel(\"Distance from TSS (kbp)\", fontsize=12)\n",
    "    axs[0].plot([rel_start-200, rel_end-200],[50,50], label=f\"Rel_area={pred_area}\")     \n",
    "    fig.legend(ncol=2, fancybox=True, shadow=True, fontsize=8)\n",
    "    fig.savefig(savepath / f\"{id}_prediction.png\", dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "def extend_df(df):\n",
    "    \"\"\" \n",
    "    Extend the gene-enhancer relationships df to add meaningful columns, e.g.\n",
    "    * Number of genes between the given enhancer and the promoter of the central gene.\n",
    "    * Whether the enhancer is adjacent to the measured gene or not.\n",
    "    * Area under the central gene (baseline and when the given enhancer is silenced in silico).\n",
    "    \"\"\"\n",
    "    \n",
    "    # EP_distance calculation\n",
    "    df['EP_distance'] = abs((df['Entity_rel_Start'] + df['Entity_rel_End']) / 2)\n",
    "    \n",
    "    # Initialize new columns\n",
    "    df['Genes_in_between'] = 0\n",
    "    df['Adjacent'] = 0\n",
    "    df['Area'] = 0\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        # Filter for entities related to the current reference gene and are genes (ENSMUSG)\n",
    "        related_genes = df[(df['Ref_gene'] == row['Ref_gene']) & (df['Entity_ID'].str.startswith('ENSMUSG'))]\n",
    "        \n",
    "        # Determine the side of TSS the current entity is on\n",
    "        entity_side = np.sign((row['Entity_rel_Start'] + row['Entity_rel_End']) / 2)\n",
    "        \n",
    "        # Count genes that are closer to the TSS than the current entity and on the same side\n",
    "        closer_genes = related_genes[(np.sign((related_genes['Entity_rel_Start'] + related_genes['Entity_rel_End']) / 2) == entity_side) &\n",
    "                                     (abs((related_genes['Entity_rel_Start'] + related_genes['Entity_rel_End']) / 2) < abs((row['Entity_rel_Start'] + row['Entity_rel_End']) / 2))]\n",
    "        \n",
    "        df.loc[index, 'Genes_in_between'] = len(closer_genes)\n",
    "        \n",
    "        # Set 'Adjacent' to 1 if there are no genes between this entity and the ref_gene\n",
    "        df.loc[index, 'Adjacent'] = 1 if len(closer_genes) == 0 else 0\n",
    "    \n",
    "    return df\n",
    "\n",
    "def filter_and_extend_gene_enhancer_table(samples_index_path: Path, samples_data_path: Path, perturbed_table_path: Path, chrom_to_keep: str = 'chr4'):\n",
    "    \"\"\" \n",
    "    This function filters the genes and gene-enhancer combinations to keep and adds the extra columns required for downstream tasks.\n",
    "    \"\"\"\n",
    "    # Load the first table and extract Ref_gene and Entity_ID pairs\n",
    "    samples_index_df = pd.read_csv(samples_index_path)\n",
    "    ref_entity_pairs = set(samples_index_df['ID'].apply(lambda x: tuple(x.rsplit('_', 2)[:2])))\n",
    "\n",
    "    # Additionally, include Ref_gene as both Ref_gene and Entity_ID\n",
    "    ref_genes = set([pair[0] for pair in ref_entity_pairs])\n",
    "    for ref_gene in ref_genes:\n",
    "        ref_entity_pairs.add((ref_gene, ref_gene))\n",
    "\n",
    "    # Load the second table\n",
    "    samples_data_df = pd.read_csv(samples_data_path, sep='\\t')\n",
    "    samples_data_df = samples_data_df[(samples_data_df['Ref_gene_chromosome']== chrom_to_keep)] # Filter only genes from test chromosome\n",
    "\n",
    "    samples_data_df = extend_df(samples_data_df)\n",
    "\n",
    "    # Filter rows based on whether the (Ref_gene, Entity_ID) tuple matches the reference set\n",
    "    filtered_df = samples_data_df[\n",
    "        samples_data_df.apply(lambda row: (row['Ref_gene'], row['Entity_ID']) in ref_entity_pairs, axis=1)\n",
    "    ]\n",
    "\n",
    "    # Save the filtered DataFrame to a new CSV file if needed\n",
    "    filtered_df.to_csv(perturbed_table_path, index=False)\n",
    "\n",
    "def calculate_area_gene(line_p, line_a, ID, pos_dict, N_BINS):\n",
    "    \"\"\" \n",
    "    Args:\n",
    "        line_p: vector of predicted values\n",
    "        line_a: vector of actual values\n",
    "        ID: reference gene, i.e. sample name\n",
    "        pos_dict: dictionary with relative gene coordinates\n",
    "        N_BINS: which length do we set as a threshold to integrate (in the new resolution).\n",
    "    \"\"\"\n",
    "    seq_center = int(len(line_p)//2)\n",
    "    rel_start = seq_center + pos_dict[ID][ID][0] if (seq_center + pos_dict[ID][ID][0]) > 0 else 0\n",
    "    rel_end = seq_center + pos_dict[ID][ID][1] if (seq_center + pos_dict[ID][ID][1]) < 2*N_BINS+1 else 2*N_BINS+1\n",
    "\n",
    "    WIDTH = max(1, int(rel_end - rel_start))\n",
    "\n",
    "    # Appending area\n",
    "    pred_area = simpson(line_p[rel_start:rel_end])/WIDTH\n",
    "    actual_area = simpson(line_a[rel_start:rel_end])/WIDTH\n",
    "\n",
    "    return pred_area, actual_area, rel_start, rel_end\n",
    "\n",
    "\n",
    "\n",
    "def calculate_correlations(results_path: Path, gene_pos_path: Path, figure_path: Path, input_table_path: Path,  N_BINS: int = 200, condition_list = [\"_ctrl\"], resolution: int = 1000):\n",
    "    \"\"\"\n",
    "    This function returns a set of dictionaries containing the actual EU-seq values and the predicted values. \n",
    "    This is done both on a bin by bin basis and integrated over the target gene length (normalized by gene length).\n",
    "    \"\"\"\n",
    "\n",
    "    # Storing gene position's dict:\n",
    "\n",
    "    pos_dict = read_gene_positions(gene_pos_path, resolution)\n",
    "    ids, predicted, actual = _get_predictions(results_path, N_BINS, condition_list)\n",
    "    df = pd.read_csv(input_table_path)\n",
    "\n",
    "    # Normalized Area per bin for the target gene (central gene in observation window)\n",
    "    pred_list_A_per_bin_dict = {}\n",
    "    actual_list_A_per_bin_dict = {}\n",
    "\n",
    "    # Predicted and actual EU-seq values in all positions:\n",
    "    pred_list_values_dict = {}\n",
    "    actual_list_values_dict = {}\n",
    "\n",
    "    for condition in condition_list:\n",
    "        pred_list_A_per_bin_dict[condition] = []\n",
    "        actual_list_A_per_bin_dict[condition] = []\n",
    "        pred_list_values_dict[condition] = []\n",
    "        actual_list_values_dict[condition] = []\n",
    "\n",
    "    for ID,line_p,line_a in zip(ids,predicted.values,actual.values):\n",
    "        if (\"_rev\" not in ID): #Only add the forward strand to the analyses\n",
    "            for condition in condition_list:\n",
    "                # Extend binwise prediction and target lists\n",
    "                pred_list_values_dict[condition].extend(list(line_p))\n",
    "                actual_list_values_dict[condition].extend(list(line_a))\n",
    "\n",
    "                # Extend area predictions\n",
    "                pred_area, actual_area, rel_start, rel_end = calculate_area_gene(line_p, line_a, ID, pos_dict, N_BINS)\n",
    "\n",
    "                pred_list_A_per_bin_dict[condition].extend([pred_area])\n",
    "                actual_list_A_per_bin_dict[condition].extend([actual_area])\n",
    "\n",
    "                # Adding area to extended table:\n",
    "                gene = ID.split('_')[0]\n",
    "                df.loc[(df['Ref_gene'] == gene) & (df['Entity_ID'] == gene),'Area'] = pred_area #Edit existing table\n",
    "\n",
    "            PLOT = True\n",
    "            ID_LIST = [\"ENSMUSG00000078673.10_forward\",\"ENSMUSG00000067261.4_forward\",\"ENSMUSG00000028234.6_forward\",\"ENSMUSG00000028280.9_forward\",\"ENSMUSG00000035969.15_forward\"]\n",
    "            if PLOT and (ID in ID_LIST):\n",
    "                print(gene, pred_area)\n",
    "                plot_target_predictions(figure_path, line_p, line_p, ID, rel_start, rel_end, pred_area)\n",
    "\n",
    "    df.to_csv(input_table_path, index=False)\n",
    "\n",
    "    return pred_list_A_per_bin_dict, actual_list_A_per_bin_dict, pred_list_values_dict, actual_list_values_dict\n",
    "\n",
    "def add_gene_area_after_enhancer_perturbation(results_path: Path, gene_pos_path: Path, figure_path: Path, input_table_path: Path,  N_BINS: int = 200, condition_list = [\"_ctrl\"], resolution: int = 1000):\n",
    "    \"\"\"\n",
    "    This function reopens the csv file with reference genes and enhancers and edits the area under the reference gene when silencing a given enhancer.\n",
    "    \"\"\"\n",
    "    # Storing gene position's dict:\n",
    "\n",
    "    pos_dict = read_gene_positions(gene_pos_path, resolution)\n",
    "    ids, predicted, actual = _get_predictions(results_path, N_BINS, condition_list)\n",
    "    df = pd.read_csv(input_table_path)\n",
    "\n",
    "    for ID,line_p,line_a in zip(ids,predicted.values,actual.values):\n",
    "        seq_center = int(len(line_p)//2)\n",
    "        gene = ID.split('_')[0]\n",
    "        enhancer = ID.split('_')[1]\n",
    "        rel_start = seq_center + pos_dict[gene+'_forward'][gene+'_forward'][0] if (seq_center + pos_dict[gene+'_forward'][gene+'_forward'][0]) > 0 else 0\n",
    "        rel_end = seq_center + pos_dict[gene+'_forward'][gene+'_forward'][1] if (seq_center + pos_dict[gene+'_forward'][gene+'_forward'][1]) < 2*N_BINS+1 else 2*N_BINS+1\n",
    "\n",
    "        WIDTH = max(1, int(rel_end - rel_start))\n",
    "\n",
    "        # Appending area\n",
    "        pred_area = simpson(line_p[rel_start:rel_end])/WIDTH\n",
    "\n",
    "        # Adding area to extended table:\n",
    "        df.loc[(df['Ref_gene'] == gene) & (df['Entity_ID'] == enhancer),'Area'] = pred_area #Edit existing table\n",
    "\n",
    "        PLOT = True\n",
    "        ID_LIST = [\"ENSMUSG00000078673.10\",\"ENSMUSG00000067261.4\"]\n",
    "        if PLOT and (gene in ID_LIST):\n",
    "            plot_target_predictions(figure_path, line_p, line_a, ID, rel_start, rel_end, pred_area)\n",
    "\n",
    "    df.to_csv(input_table_path, index=False)\n",
    "\n",
    "def plot_correlations(figure_path: Path, pred_list_values: list, actual_list_values: list, title: str, cmap: colors.Colormap = \"afmhot\", binlims: tuple = (0,50), density: bool =True, DELTA:bool=False):\n",
    "    \"\"\" \n",
    "    This function plots predicted vs. actual values and provides the Spearman and Pearson correlations of the regressions. \n",
    "    The style of the plot changes depending on whether the predictions are at a bin level or a gene level.\n",
    "    \"\"\"\n",
    "    # Get the counts histogram to plot correlation with density colormap\n",
    "    counts, xedges, yedges, _ = plt.hist2d(actual_list_values,pred_list_values, bins=[np.linspace(binlims[0],binlims[1],200),np.linspace(binlims[0],binlims[1],200)], density=False)\n",
    "\n",
    "    # Plot regression of single predicted values vs. real\n",
    "    pearson_r = pearsonr(actual_list_values,pred_list_values)[0]\n",
    "    spearman_r = spearmanr(actual_list_values,pred_list_values)[0]\n",
    "    m, b = np.polyfit(actual_list_values, pred_list_values, deg=1)\n",
    "    # Actual fit:\n",
    "    a1, a2, a3 = np.polyfit(actual_list_values, pred_list_values, deg=2)\n",
    "    # Regression from origin:\n",
    "    #m_0 , _, _, _ = np.linalg.lstsq(np.array(actual_list_values)[:,np.newaxis], pred_list_values) #x needs to be a column vector for this function\n",
    "\n",
    "    # Create the figure\n",
    "    fig, ax = plt.subplots(figsize=(4,4))    \n",
    "    min_val, max_val, step = (0,10,.1)\n",
    "    if density:\n",
    "        eps= 1\n",
    "        # Counts matrix is transposed: the origin convention for plt.imshow is (top, left), for plt.hist2D is (bottom,left)\n",
    "        counts[counts < eps] =1\n",
    "        hist = ax.imshow(counts.T, origin='lower', extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]], cmap=cmap,  norm=colors.LogNorm())\n",
    "        _ = fig.colorbar(hist)\n",
    "        _ = ax.axline(xy1=(0, b), slope=m, label=f'Linear fit:\\n$ y = {m:.3f}x {b:+.3f}$\\n$r_p:{pearson_r:.4f}$\\n$r_s:{spearman_r:.4f}$', color=\"royalblue\", ls=\"-\", lw=1)\n",
    "        #_ = ax.plot(np.arange(min_val,max_val,step), a1*(np.arange(min_val,max_val,step)**2)+a2*(np.arange(min_val,max_val,step))+a3, color='royalblue', ls = \"--\", lw=1, label= f\"Polynomial fit:\\n$y={a1:.3f}x^2+{a2:.3f}x+{a3:.3f}$\")\n",
    "        _ = ax.axis(\"scaled\")\n",
    "        _ = ax.set_xlim(min_val,max_val)\n",
    "        _ = ax.set_ylim(min_val,max_val)\n",
    "        _ = ax.set_xlabel(\"log$_2$(True +1)\")\n",
    "        _ = ax.set_ylabel(\"log$_2$(Predicted +1)\")\n",
    "        fig.legend(bbox_to_anchor=(.48,.8), fancybox=True, shadow=False, fontsize=7)\n",
    "    else:\n",
    "        _ = ax.plot(actual_list_values,pred_list_values, lw=0, marker='o',  markersize=1, markeredgewidth=0, color='k')\n",
    "        _ = ax.axis(\"scaled\")\n",
    "        _ = ax.axline(xy1=(0, b), slope=m, label=f'Linear fit:\\n$ y = {m:.3f}x {b:+.3f}$\\n$r_p:{pearson_r:.4f}$\\n$r_s:{spearman_r:.4f}$', color=\"royalblue\", ls=\"-\", lw=1)\n",
    "        #ax.plot(np.arange(-20,200,1), a1*(np.arange(-20,200,1)**2)+a2*(np.arange(-20,200,1))+a3, color='royalblue', ls = \"--\", lw=1, label= f\"Polynomial fit:\\n$y={a1:.3f}x^2+{a2:.3f}x{a3:.3f}$\")\n",
    "        if DELTA:\n",
    "            _ = ax.set_xlim(-20,20)\n",
    "            _ = ax.set_ylim(-20, 20)\n",
    "            _ = ax.set_xlabel(\"Actual $\\Delta_{exp}$  (reads/kbp)\")\n",
    "            _ = ax.set_ylabel(\"Predicted $\\Delta_{exp}$ (reads/kbp)\")\n",
    "            fig.legend(bbox_to_anchor=(.53,.87), fancybox=True, shadow=True, fontsize=8)\n",
    "        else:\n",
    "            _ = ax.set_xlim(0,200)\n",
    "            _ = ax.set_ylim(0, 200)\n",
    "            _ = ax.set_xlabel(\"Actual averaged expression (reads/kbp)\")\n",
    "            _ = ax.set_ylabel(\"Predicted averaged expression (reads/kbp)\")\n",
    "            fig.legend(bbox_to_anchor=(.53,.87), fancybox=True, shadow=True, fontsize=8)\n",
    "\n",
    "    #ax.plot(np.arange(0,500,1), np.arange(0,500,1), color='k', ls = \"dashed\", lw=1)\n",
    "\n",
    "    fig.savefig(figure_path / f\"Regression_histogram_{title}.png\", dpi=200, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "\n",
    "def plot_chromatin_state_combinations(array_path: Path, figure_path: Path, skip_n_bins: int):\n",
    "    \"\"\"\n",
    "    This function is aimed to visualize the different combinations of values for different chromatin marks, which define the chromatin states.   \n",
    "    \"\"\"\n",
    "    H3K4me3 = np.array([])\n",
    "    H3K27ac = np.array([])\n",
    "    H3K27me3 = np.array([])\n",
    "\n",
    "    for file in os.scandir(array_path):\n",
    "            if \"_rev\" not in file.name:\n",
    "                    array = np.load(file)\n",
    "\n",
    "                    # Adding mark enrichment values to a long array\n",
    "                    H3K4me3 = np.concatenate((H3K4me3,array[1][::skip_n_bins]), axis=0)\n",
    "                    H3K27ac = np.concatenate((H3K27ac,array[2][::skip_n_bins]), axis=0)\n",
    "                    H3K27me3 = np.concatenate((H3K27me3,array[3][::skip_n_bins]), axis=0)\n",
    "\n",
    "\n",
    "    # Plotting the 3d combinations at all sites\n",
    "    scaling_factor = 1 # Inputs were directly given in reads\n",
    "    altitude, azimuth = 45,45\n",
    "    fig = plt.figure(figsize=(7, 7))\n",
    "    ax = fig.add_subplot(projection=\"3d\")\n",
    "    ax.view_init(altitude, azimuth)\n",
    "    ax.grid(False)\n",
    "\n",
    "    #ax.plot_surface(x_val, y_val, avg_att_mat, rstride=1,cstride=1,linewidth=0, cmap=\"terrain\")#,  vmin=-2e-4, vmax=2e-4)\n",
    "    ax.scatter(H3K4me3*scaling_factor, H3K27ac*scaling_factor, H3K27me3*scaling_factor,linewidth=0, alpha=.5, marker=\"o\", edgecolors=\"none\", c=\"k\", s=.1, label=\"All states combined\")#,  vmin=-2e-4, vmax=2e-4)\n",
    "\n",
    "    # Directions\n",
    "    zdirs = ((-1,-.2,0), 'y','z','y','y')\n",
    "    xs = (250, 0,60,0, 30)\n",
    "    ys = (10, 50, 10,20,120)\n",
    "    zs = (2, -20, 50,17,5)\n",
    "\n",
    "    labels = (\"Active promoters\", \"Active enhancers\", \"Silenced\\nchromatin\",\"$P2$\",\"P1\")\n",
    "    label_colors = (\"#ff0000\",\"blue\",\"darkgreen\",\"purple\", \"goldenrod\")\n",
    "\n",
    "    for zdir, x, y, z, label, c in zip(zdirs, xs, ys, zs, labels, label_colors):\n",
    "        ax.text(x, y, z, label, zdir, color=c)\n",
    "\n",
    "\n",
    "    # Arrow towards silenced branch:\n",
    "    ax.quiver(0, 100, 5, 0,-20,0, color='goldenrod',lw=.7)\n",
    "    ax.quiver(0, 50, 5, 0,-20,0, color='goldenrod',lw=.7)\n",
    "    ax.quiver(30, 100, 5, 0,-20,0, color='goldenrod',lw=.7)\n",
    "    ax.quiver(30, 50, 5, 0,-20,0, color='goldenrod',lw=.7)\n",
    "\n",
    "    # Inwards arrow towards silenced branch for P2:\n",
    "    ax.quiver(-10, 20, 8, -10,-20,0, color='purple',lw=.7)\n",
    "    ax.quiver(-10, 20, -2, -10,-20,0, color='purple',lw=.7)\n",
    "    #ax.quiver(50, 10, 10, -20,0,0, color='purple',lw=.7)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ax.set_xlabel(\"H3K4me3\")\n",
    "    ax.set_xticks([0,200,400],[0,200,400], rotation=40)\n",
    "\n",
    "    ax.set_ylabel(\"H3K27ac\")\n",
    "    ax.set_yticks([0,100,200],[0,100,200], rotation=-40)\n",
    "\n",
    "    ax.set_zlabel(\"H3K27me3\", rotation=180)\n",
    "    ax.set_zticks([0,40,80],[0,40,80])\n",
    "\n",
    "    fig.subplots_adjust(left=0.2, right=0.9, top=.95, bottom=0.05)\n",
    "    fig.savefig(figure_path / \"chromstates.png\", dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def read_target_attributions(results_path: Path,\n",
    "                             figure_path: Path,\n",
    "                             n_central_bins: int,\n",
    "                             track_dict: dict,\n",
    "                             l_in:int = 10001,\n",
    "                             n_out:int=401,\n",
    "                             n_in: int=4,\n",
    "                             TRAIN_ATTR: bool = True,\n",
    "                             SPLIT: int = None):\n",
    "\n",
    "    \"\"\"\n",
    "    This function is aimed to read attribution arrays from the chromatin landscape branch. Attribution arrays score the \n",
    "    contribution of each position in the input arrays (4,10.001) towards each output node/position (401).\n",
    "    Args:\n",
    "        results_path: path where the attribution arrays are stored.\n",
    "        figure_path: path where we want to store the outputs\n",
    "        n_central_bins: number of central bins (from -N_BINS to  N_BINS+1)\n",
    "        track_dict: dictionary containing the names of the tracks and plot details\n",
    "        l_in: input length \n",
    "        n_out: number of output nodes\n",
    "        n_in: number of input channels\n",
    "        TRAIN_ATTR: whether this is a training or test run (folder structure changes).\n",
    "        SPLIT: if Train, we need to know which of the stored batches it is\n",
    "\n",
    "    Returns:\n",
    "        Plots with attribution scores.\n",
    "    \"\"\"\n",
    "    figure_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    N_BINS = n_central_bins // 2  # Number of bins from TSS that we want on one side\n",
    "    N_IN_BINS = l_in // 2\n",
    "    x_val = np.arange(-N_IN_BINS,N_IN_BINS+1)\n",
    "\n",
    "    xlim = (-500,500)\n",
    "\n",
    "    # Getting attributions:\n",
    "    condition_list = [\"_ctrl\"]\n",
    "    landscape_avg = {condition : np.zeros((n_in,l_in)) for condition in condition_list}\n",
    "\n",
    "    for i in range(-N_BINS, N_BINS + 1):\n",
    "        for condition in condition_list:\n",
    "            # CHROMATIN MARK ATTRIBUTIONS:\n",
    "            added_folders = f\"/samples/{SPLIT}\" if TRAIN_ATTR else \"\" # Different folder structure\n",
    "            att_path = Path(results_path / f\"{i}{condition}{added_folders}/attributions/gene_expression/{i}{condition}.npy\")\n",
    "            if os.path.exists(att_path):\n",
    "                attributions = np.load(att_path)\n",
    "                #if (i == -73) and (condition == \"_ctrl\"):\n",
    "                fig, axs = plt.subplots(4, figsize=(8,2.7)) \n",
    "                for j,key in enumerate(track_dict.keys()):\n",
    "                    name = track_dict[key][\"name\"]\n",
    "                    axs[j].plot(x_val, abs(attributions[j]), label=name, lw=0, marker='o', markersize=0.8, markeredgecolor=\"none\", color=track_dict[key][\"color\"])\n",
    "                    axs[j].set_xlim((-5000,5001))\n",
    "                    axs[j].set_ylim((0,np.max(attributions[1])))\n",
    "                    axs[j].set_yticks([])\n",
    "                    axs[-1].set_yticks([0,np.max(attributions[1])],[0,1])\n",
    "                fig.savefig(figure_path / f\"Chromatin_landscape_attributions_{i}_ctrl.png\",dpi=200)\n",
    "                plt.close(fig)\n",
    "\n",
    "                # Add a small phase to remove high frequency fluctuations:\n",
    "                eps = 10*np.random.rand(1)*np.random.choice([-1,1])\n",
    "                \n",
    "                centered_attr = np.roll(attributions, -10*i+int(eps),axis=1)\n",
    "                landscape_avg[condition] += 1/n_out*(abs(centered_attr))\n",
    "\n",
    "    SCALING_FACTOR = np.max([landscape_avg[cond] for cond in condition_list])\n",
    "\n",
    "    \n",
    "    for condition in condition_list:\n",
    "        fig, ax = plt.subplots(figsize=(8,2.7))   \n",
    "        sub_axes = plt.axes([.615, .57, .27, .27])  \n",
    "        for i,key in enumerate(track_dict.keys()):\n",
    "            name = track_dict[key][\"name\"]\n",
    "            ax.plot(x_val, landscape_avg[condition][i]/SCALING_FACTOR, label=name, lw=0, marker='o', markersize=2, markeredgecolor=\"none\", color=track_dict[key][\"color\"])\n",
    "            sub_axes.plot(x_val, landscape_avg[condition][i]/SCALING_FACTOR, label=name, lw=0,marker='o',markersize=.5,markeredgecolor=\"none\", color=track_dict[key][\"color\"])\n",
    "        \n",
    "        #sub_axes.set_xlim(xlim)\n",
    "        sub_axes.set_xticks(np.arange(-5000,5001,5000),np.arange(-500,501,500))\n",
    "        sub_axes.set_xlim((-5000,5000))\n",
    "        sub_axes.set_yticks([])\n",
    "        ax.set_xlim(xlim)\n",
    "        ax.set_ylim(0,1) #(0,.002))\n",
    "        ax.legend(loc='upper left')\n",
    "        ax.set_ylabel(\"Attribution score\")\n",
    "        ax.set_xlabel(\"Distance to the predicted locus (kbp)\")\n",
    "        plt.subplots_adjust(bottom=0.2)\n",
    "        ax.set_xticks(np.arange(-400,401,100),np.arange(-40,41,10))\n",
    "        fig.savefig(figure_path / f\"Landscape_absolute_average_{condition}.png\", dpi=200)\n",
    "        plt.close(fig)\n",
    "\n",
    "def model_comparison(figure_path: Path, performance_dict: dict, color_list: list):\n",
    "    \"\"\"\n",
    "    Creates bar plots comparin pearson and spearman correlations of the different models.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(2.5,5))\n",
    "    pos = 1\n",
    "    shift = 8\n",
    "    for i,model in enumerate(performance_dict.keys()):\n",
    "        # if i == 0:\n",
    "        #     for j in range(2):\n",
    "        #         plt.plot(pos,complementary_dict[i][\"Spearman\"], marker='d', color='k', markersize=10, markeredgewidth=0)\n",
    "        #         plt.plot(pos+shift,complementary_dict[i][\"Pearson\"], marker='d', color='k', markersize=10, markeredgewidth=0)\n",
    "                \n",
    "        plt.bar(pos, performance_dict[model][\"Spearman\"], label=f\"{model}\", color=color_list[i])\n",
    "        plt.bar(pos+shift, performance_dict[model][\"Pearson\"], color=color_list[i])\n",
    "        #plt.plot(pos,performance_dict[model][\"Spearman\"], marker='d', color='k', markersize=3, markeredgewidth=0)\n",
    "        #plt.plot(pos+shift,performance_dict[model][\"Pearson\"], marker='d', color='k', markersize=3, markeredgewidth=0)\n",
    "        pos += 1\n",
    "\n",
    "\n",
    "    plt.xticks([3.5,11.5],[\"Spearman\",\"Pearson\"])\n",
    "    plt.legend(loc=\"lower left\", bbox_to_anchor=(-0.04,1.08), fontsize=6)\n",
    "    plt.ylim((0,1))\n",
    "    plt.ylabel(\" Binwise correlation for log$_2$(reads +1)\")\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(figure_path / \"Model_comparison.png\", dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "def model_comparison_II(figure_path: Path, performance_dicts: list , color_list: list):\n",
    "    \"\"\"\n",
    "    Creates bar plots comparing pearson and spearman correlations of the different models.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(4,4))\n",
    "    for j,performance_dict in enumerate(performance_dicts):\n",
    "        print(performance_dict)\n",
    "        marker = 'o' if j== 0 else '^'\n",
    "        task = 'binwise' if j==0 else 'area'\n",
    "        for i,model in enumerate(performance_dict.keys()):\n",
    "            plt.plot(performance_dict[model][\"Pearson\"],performance_dict[model][\"Spearman\"], color=color_list[i], marker=marker,lw=0, label=f'{model} {task}')\n",
    "\n",
    "    plt.legend(loc=\"lower center\", ncols=2, fontsize=6)\n",
    "    plt.ylim((.4,1))\n",
    "    plt.xlim((.4,1))\n",
    "    plt.ylabel(\"Spearman\")\n",
    "    plt.xlabel(\"Pearson\")\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(figure_path / \"Model_comparison_II.png\", dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "####################### ATTRIBUTIONS ####################\n",
    "\n",
    " \n",
    "def plot_microc_original_and_rotated(pathlist: list, figure_path: Path, num_bins: int, window_of_observation: float, name: str):\n",
    "    \"\"\" \n",
    "    This function plots the Micro-C contact maps (both original and rotated-cropped)  given a sample name.\n",
    "\n",
    "    \"\"\"\n",
    "    bin_per_kbp = num_bins/window_of_observation #Unit (bin/kbp)\n",
    "    max_shift = int(200*bin_per_kbp) # Distance to crop because it was rolled from the other side\n",
    "\n",
    "    for p,path in enumerate(pathlist):\n",
    "        for sample in os.scandir(path):\n",
    "            if name in sample.name: # Nice example: \"ENSMUST00000105369.7.npy\" common \"ENSMUST00000001565.14.npy\" \n",
    "                microc_map = np.load(sample)\n",
    "                fig = plt.figure(figsize=(8,4))\n",
    "                #plt.imshow(microc_map, cmap=\"YlOrRd\")\n",
    "                # plt.plot(200*bin_per_kbp,50,marker=\"x\")\n",
    "                # plt.plot(-200*bin_per_kbp+313,50,marker=\"x\")\n",
    "                # plt.plot(313,50,marker=\"x\")\n",
    "                plt.imshow(microc_map, cmap=\"YlOrRd\", vmin = -5, vmax = 0)\n",
    "                # Real scale\n",
    "                center_bin = np.shape(microc_map)[1]//2\n",
    "                bin_pos = np.arange(13,np.shape(microc_map)[1],50)\n",
    "                kbp_per_bin = 1/bin_per_kbp\n",
    "                bin_val = (bin_pos-np.shape(microc_map)[1]//2)*kbp_per_bin #Unit (kbp/bin)\n",
    "                bin_val = [\"%.2f\"%item for item in bin_val]\n",
    "                plt.xticks(bin_pos,bin_val, rotation=90, fontsize=8)\n",
    "                plt.xlabel(\"Distance in 1D sequence (kbp)\", fontsize=10)\n",
    "                # Careful! Scale is not the distance between points but the distance between interacting points in 1D! \n",
    "                bin_pos = np.arange(0,np.shape(microc_map)[0],25)\n",
    "                bin_val = 2*bin_pos*kbp_per_bin #*(kbp_per_bin) + np.shape(avg_att_mat)[0]\n",
    "                bin_val = [\"%.2f\"%item for item in bin_val]\n",
    "                plt.yticks(bin_pos,bin_val, fontsize=8)\n",
    "                if p == 0:\n",
    "                    plt.xticks([])\n",
    "                    plt.yticks([])\n",
    "                plt.ylabel(\"Separation between\\n interacting elements\\n in 1D sequence (kbp)\", fontsize=10)\n",
    "                plt.colorbar()\n",
    "                fig.savefig(figure_path / f\"MicroC_input_{name}_{p}.png\", dpi=200)\n",
    "                plt.close(fig)\n",
    "\n",
    "\n",
    "\n",
    "def plot_microc_attributions(figure_path, sample_ids, num_bins, window_of_observation, ATTR_type_list, conditions, sigma):\n",
    "\n",
    "    \"\"\" \n",
    "    Plot the attribution maps for micro-c matrices\n",
    "    \n",
    "    \"\"\"\n",
    "    bin_per_kbp = num_bins/window_of_observation\n",
    "    print(bin_per_kbp)\n",
    "    max_shift = int(200*bin_per_kbp) # Distance to crop because it was rolled from the other side\n",
    "\n",
    "    avg_att_mat_dict = {}\n",
    "    for condition in conditions:\n",
    "        avg_att_mat_dict[condition] = {}\n",
    "        for ATTR_type in ATTR_type_list:\n",
    "            avg_att_mat_dict[condition][ATTR_type] = np.zeros((129,626))\n",
    "\n",
    "    for condition in conditions:\n",
    "        for i in sample_ids:\n",
    "            #path = Path(f\"../runs/training_runs/gene_expression_exformer_unlimited_all_rotated/results/expression_output/{i}{condition}/samples/17880/attributions/contact_maps/{i}{condition}.npy\") \n",
    "            path = Path(f\"../runs/test_runs/gene_expression_microc_rotated_pure_conv/expression_output/{i}{condition}/attributions/contact_maps/{i}{condition}.npy\") \n",
    "            att_mat = np.load(path)\n",
    "            att_mat_rolled = np.roll(att_mat,-int(i*bin_per_kbp), axis = 1)\n",
    "\n",
    "            avg_att_mat_dict[condition][\"abs\"] += 1/(len(sample_ids))*abs(att_mat_rolled)\n",
    "            avg_att_mat_dict[condition][\"signed\"] += 1/(len(sample_ids))*att_mat_rolled\n",
    "\n",
    "            if (i==199):\n",
    "\n",
    "                fig_1 = plt.figure(figsize=(8,3))#(11.5, 5))\n",
    "                gs = gridspec.GridSpec(2, 2, hspace=.2, wspace=.1, width_ratios=(5,0.1))\n",
    "                ax = fig_1.add_subplot(gs[0, 0])\n",
    "                ax.set_xticks([])\n",
    "                fig_display = ax.imshow(np.flip(att_mat,axis=0),  cmap=\"seismic\", vmin = -0.0002, vmax=0.0002)\n",
    "                ax_II = fig_1.add_subplot(gs[1, 0])\n",
    "                fig_display = ax_II.imshow(np.flip(att_mat_rolled,axis=0),  cmap=\"seismic\", vmin = -0.0002, vmax=0.0002)\n",
    "                cax = fig_1.add_subplot(gs[0, 1])\n",
    "                # Add the colorbar\n",
    "                plt.colorbar(fig_display, cax=cax)\n",
    "                plt.tight_layout()\n",
    "                fig_1.savefig(figure_path / f\"Signed_attributions_{i}{condition}.png\", dpi=200)\n",
    "                plt.close(fig_1)\n",
    "\n",
    "    for CONDITION in conditions:\n",
    "        for ATTR_type in ATTR_type_list:\n",
    "            avg_att_mat = avg_att_mat_dict[condition][ATTR_type] \n",
    "            avg_att_mat = gaussian_filter(avg_att_mat, sigma=sigma)\n",
    "            avg_att_mat = np.flip(avg_att_mat[:,max_shift:-max_shift], axis = 0)\n",
    "\n",
    "            # Assuming 'img' is your image data\n",
    "            y_projected_mat = np.mean(avg_att_mat, axis=0)  # Compress along x-axis\n",
    "            x_projected_mat = np.mean(avg_att_mat, axis=1)  # Compress along y-axis\n",
    "\n",
    "            fig = plt.figure(figsize=(8,3))#(11.5, 5))\n",
    "            gs = gridspec.GridSpec(1, 2, hspace=.2, wspace=.1, width_ratios=(5,0.1))\n",
    "\n",
    "            # Main image\n",
    "            ax = fig.add_subplot(gs[0, 0])\n",
    "            # Create an axis for the colorbar\n",
    "            cax = fig.add_subplot(gs[0, 1])\n",
    "            if ATTR_type == \"abs\":\n",
    "                img_display = ax.imshow(avg_att_mat, aspect='auto', cmap=\"inferno\", vmin=0, vmax=0.5e-4)\n",
    "                cbar = plt.colorbar(img_display, cax=cax)\n",
    "                cbar.set_ticks([0,5e-5])\n",
    "                cbar.set_ticklabels([0,1])\n",
    "            else:\n",
    "                img_display = ax.imshow(avg_att_mat, aspect='auto', cmap=\"seismic\", vmin=-0.5e-4, vmax=0.5e-4)\n",
    "                cbar = plt.colorbar(img_display, cax=cax)\n",
    "                cbar.set_ticks([-5e-5,0,5e-5])\n",
    "                cbar.set_ticklabels([-1,0,1])\n",
    "\n",
    "\n",
    "            # Plot vertical dashed line\n",
    "            ref_y = np.arange(len(avg_att_mat))\n",
    "            ref_x = np.shape(avg_att_mat)[1]//2*np.ones_like(ref_y)\n",
    "            im = ax.plot(ref_x,ref_y, \"k--\")\n",
    "\n",
    "\n",
    "            # Plot diagonals from prediction point:\n",
    "            diag_x = np.arange(np.shape(avg_att_mat)[0]) + np.shape(avg_att_mat)[1]//2\n",
    "            diag_x2 = -np.arange(np.shape(avg_att_mat)[0]) + np.shape(avg_att_mat)[1]//2\n",
    "            diag_y = np.arange(np.shape(avg_att_mat)[0])\n",
    "            ax.plot(diag_x,diag_y, \"b--\", lw=.5)\n",
    "            ax.plot(diag_x2,diag_y, \"b--\", lw=.5)\n",
    "\n",
    "            # Real scale\n",
    "            bin_pos = np.arange(11,np.shape(avg_att_mat)[1],25)\n",
    "            #bin_val = (bin_pos-np.shape(avg_att_mat)[1]//2)*kbp_per_bin\n",
    "            kbp_per_bin = 1/bin_per_kbp\n",
    "            bin_val = (bin_pos-np.shape(avg_att_mat)[1]//2)*kbp_per_bin #Unit (kbp/bin)\n",
    "            bin_val = [\"%.2f\"%item for item in bin_val]\n",
    "            ax.set_xticks(bin_pos,bin_val, rotation=90)\n",
    "\n",
    "            # Careful! Scale is not the distance between points but the distance between interacting points in 1D! \n",
    "            bin_pos = np.arange(0,np.shape(avg_att_mat)[0],25)\n",
    "            bin_val = 2*bin_pos*kbp_per_bin # Unit (kbp/bin)\n",
    "            bin_val = [\"%.2f\"%item for item in bin_val]\n",
    "            ax.set_yticks(bin_pos,bin_val)\n",
    "\n",
    "\n",
    "            ax.set_xlabel(\"Distance from the predicted point (kbp)\",fontsize= 12)\n",
    "            ax.set_ylabel(\"Separation between\\n interacting elements\\n in 1D sequence (kbp)\",fontsize= 12)\n",
    "\n",
    "            # # Top plot\n",
    "            # axb = fig.add_subplot(gs[0, 0], sharex=ax)\n",
    "            # axb.plot(np.arange(len(y_projected_mat)), y_projected_mat, color='k')\n",
    "            # plt.setp(axb.get_xticklabels(), visible=False)\n",
    "            # axb.set_ylabel(\"Averaged attribution score\", fontsize= 8)\n",
    "\n",
    "            # # Right plot\n",
    "            # axl = fig.add_subplot(gs[1, 1], sharey=ax)\n",
    "            # axl.plot(x_projected_mat, np.arange(len(x_projected_mat)), color='k')\n",
    "            # plt.setp(axl.get_yticklabels(), visible=False)\n",
    "            # axb.set_ylabel(\"Averaged attribution score\", fontsize = 8)\n",
    "            plt.subplots_adjust(bottom=0.25, left=.20)\n",
    "            plt.tight_layout()\n",
    "            fig.savefig(figure_path / f\"Structural_attributions_{ATTR_type}_{CONDITION}_{sigma}.png\", dpi=200)\n",
    "            plt.close(fig)\n",
    "\n",
    "##### Model benchmarks ######\n",
    "\n",
    "def plot_benchmark_models_predictions(data_and_path_dict, results_path,target_samples, condition_list, READ_SCALING_FACTOR,PLOT_LIST, gene_pos_path, resolution, cmap = \"binary\"):\n",
    "    \"\"\" \n",
    "    This function is aimed to compare the predicted profiles and performances between different models.\n",
    "    \"\"\"\n",
    "    CREATE_DFS = True\n",
    "    param_dict = {\"CLASTER\":{\"n_central_bins\":401,\"N_BINS\":200,\"OUT_LEN\":401},\n",
    "                             \"hyenadna\":{\"n_central_bins\":31,\"N_BINS\":15,\"OUT_LEN\":31},\n",
    "                             \"Enformer\":{\"n_central_bins\":115,\"N_BINS\":57,\"OUT_LEN\":115}}\n",
    "\n",
    "\n",
    "    N_SAMPLES = 1295\n",
    "    pos_dict = read_gene_positions(gene_pos_path, resolution)  # Storing gene position's dict:\n",
    "\n",
    "    ### Claster and targets \n",
    "    MODEL = \"CLASTER\" #\"Enformer\"\n",
    "    ids, predicted, actual = _get_predictions(results_path, param_dict[MODEL][\"N_BINS\"], condition_list)\n",
    "    ids = ids[::2] # Keep only forward!\n",
    "    predicted = predicted[~predicted.index.str.contains('_rev')].sort_index() \n",
    "    actual = actual[~actual.index.str.contains('_rev')].sort_index()\n",
    "\n",
    "\n",
    "    ### hyenadna and Enformer\n",
    "    MODEL = \"hyenadna\"\n",
    "    pred_list_df_hyena= pd.DataFrame(data_and_path_dict[MODEL][\"predictions\"].reshape(N_SAMPLES,param_dict[MODEL][\"OUT_LEN\"]), columns = [f\"{i}{cond}\" for cond in condition_list for i in range(-param_dict[MODEL][\"N_BINS\"],param_dict[MODEL][\"N_BINS\"]+1)], index=target_samples.index).sort_index() \n",
    "    actual_list_df_hyena = pd.DataFrame(data_and_path_dict[MODEL][\"actual\"].reshape(N_SAMPLES,param_dict[MODEL][\"OUT_LEN\"]), columns = [f\"{i}{cond}\" for cond in condition_list for i in range(-param_dict[MODEL][\"N_BINS\"],param_dict[MODEL][\"N_BINS\"]+1)], index=target_samples.index).sort_index() \n",
    "    \n",
    "    MODEL = \"Enformer\"\n",
    "    pred_list_df_enformer= pd.DataFrame(data_and_path_dict[MODEL][\"predictions\"].reshape(N_SAMPLES,param_dict[MODEL][\"OUT_LEN\"]), columns = [f\"{i}{cond}\" for cond in condition_list for i in range(-param_dict[MODEL][\"N_BINS\"],param_dict[MODEL][\"N_BINS\"]+1)], index=target_samples.index).sort_index() \n",
    "    actual_list_df_enformer = pd.DataFrame(data_and_path_dict[MODEL][\"actual\"].reshape(N_SAMPLES,param_dict[MODEL][\"OUT_LEN\"]), columns = [f\"{i}{cond}\" for cond in condition_list for i in range(-param_dict[MODEL][\"N_BINS\"],param_dict[MODEL][\"N_BINS\"]+1)], index=target_samples.index).sort_index() \n",
    "    ###\n",
    "    counter = 0\n",
    "\n",
    "    # Normalized Area per bin for the target gene (central gene in observation window)\n",
    "    A_per_gene_dict = {\"Enformer\":{\"Predicted\":[],\"Actual\":[]},\"hyenadna\":{\"Predicted\":[],\"Actual\":[]}}\n",
    "\n",
    "    for id, line_pc, line_ac, line_ph, line_pe  in zip(ids,predicted.values,actual.values,pred_list_df_hyena.values, pred_list_df_enformer.values):\n",
    "        #if counter < 50:   \n",
    "         \n",
    "        MODEL = \"Enformer\"\n",
    "        N_BINS = param_dict[MODEL][\"N_BINS\"]\n",
    "        MODEL_SHIFT = (param_dict[\"CLASTER\"][\"n_central_bins\"] - param_dict[MODEL][\"n_central_bins\"])//2\n",
    "        #MODEL_SHIFT = (param_dict[\"CLASTER\"][\"n_central_bins\"] - param_dict[\"hyenadna\"][\"n_central_bins\"])//2\n",
    "        #ENFORMER_SHIFT = (param_dict[\"Enformer\"][\"n_central_bins\"] - param_dict[\"hyenadna\"][\"n_central_bins\"])//2\n",
    "        pred_area_e, actual_area_e, *_ = calculate_area_gene(line_pe, line_ac[MODEL_SHIFT:-MODEL_SHIFT], id, pos_dict, N_BINS)\n",
    "        #pred_area_e, actual_area_e, *_ = calculate_area_gene(line_pe[ENFORMER_SHIFT:-ENFORMER_SHIFT], line_ac[MODEL_SHIFT:-MODEL_SHIFT], id, pos_dict, N_BINS)\n",
    "        A_per_gene_dict[MODEL][\"Predicted\"].append(pred_area_e)\n",
    "        A_per_gene_dict[MODEL][\"Actual\"].append(actual_area_e)\n",
    "\n",
    "        MODEL = \"hyenadna\"\n",
    "        N_BINS = param_dict[MODEL][\"N_BINS\"]\n",
    "        MODEL_SHIFT = (param_dict[\"CLASTER\"][\"n_central_bins\"] - param_dict[MODEL][\"n_central_bins\"])//2\n",
    "        pred_area_h, actual_area_h, *_ = calculate_area_gene(line_ph, line_ac[MODEL_SHIFT:-MODEL_SHIFT], id, pos_dict,N_BINS)\n",
    "        A_per_gene_dict[MODEL][\"Predicted\"].append(pred_area_h)\n",
    "        A_per_gene_dict[MODEL][\"Actual\"].append(actual_area_h)\n",
    "\n",
    "        if id in PLOT_LIST:\n",
    "            fig, axs =plt.subplots(nrows=4,ncols=1, figsize=(8,4), sharex = True)\n",
    "            MODEL = \"CLASTER\" \n",
    "            axs[2].fill_between(np.arange(-param_dict[MODEL][\"N_BINS\"],param_dict[MODEL][\"N_BINS\"]+1),line_pc[:param_dict[MODEL][\"n_central_bins\"]]*READ_SCALING_FACTOR, lw=1, color='silver',alpha=.8, label=\"CLASTER\")\n",
    "            axs[3].fill_between(np.arange(-param_dict[MODEL][\"N_BINS\"],param_dict[MODEL][\"N_BINS\"]+1),line_ac[:param_dict[MODEL][\"n_central_bins\"]]*READ_SCALING_FACTOR, lw=1, color='black', alpha=.5, label=\"EU-seq target\")\n",
    "            axs[3].plot([-param_dict[MODEL][\"N_BINS\"]+MODEL_SHIFT,-param_dict[MODEL][\"N_BINS\"]+ MODEL_SHIFT+param_dict[\"hyenadna\"][\"n_central_bins\"]],[10,10],color='k')\n",
    "            #axs[2][1].fill_between(np.arange(-param_dict[MODEL][\"N_BINS\"],param_dict[MODEL][\"N_BINS\"]+1),line_pc[param_dict[MODEL][\"n_central_bins\"]:]*READ_SCALING_FACTOR, lw=1, color='royalblue', alpha=.9, label=\"Predicted expression treated\")\n",
    "            #axs[3][1].fill_between(np.arange(-param_dict[MODEL][\"N_BINS\"],param_dict[MODEL][\"N_BINS\"]+1),line_ac[param_dict[MODEL][\"n_central_bins\"]:]*READ_SCALING_FACTOR, lw=1, color='k', alpha=.6, label=\"Actual expression treated\")\n",
    "            \n",
    "            MODEL = \"hyenadna\"\n",
    "            axs[0].fill_between(np.arange(-param_dict[MODEL][\"N_BINS\"],param_dict[MODEL][\"N_BINS\"]+1),line_ph[:param_dict[MODEL][\"n_central_bins\"]]*READ_SCALING_FACTOR, lw=1, color='silver',alpha=.8,label=f\"HyenaDNA-32k {pred_area_h}-{actual_area_h}\" )\n",
    "            #axs[0][1].fill_between(np.arange(-param_dict[MODEL][\"N_BINS\"],param_dict[MODEL][\"N_BINS\"]+1),line_ph[param_dict[MODEL][\"n_central_bins\"]:]*READ_SCALING_FACTOR, lw=1, color='royalblue', alpha=.9)\n",
    "\n",
    "            MODEL = \"Enformer\"\n",
    "            axs[1].fill_between(np.arange(-param_dict[MODEL][\"N_BINS\"],param_dict[MODEL][\"N_BINS\"]+1),line_pe[:param_dict[MODEL][\"n_central_bins\"]]*READ_SCALING_FACTOR, lw=1, color='silver',alpha=.8,label=f\"Enformer {pred_area_e}-{actual_area_e}\")\n",
    "            #axs[1][1].fill_between(np.arange(-param_dict[MODEL][\"N_BINS\"],param_dict[MODEL][\"N_BINS\"]+1),line_pe[param_dict[MODEL][\"n_central_bins\"]:]*READ_SCALING_FACTOR, lw=1, color='royalblue', alpha=.9)\n",
    "            \n",
    "            model_list = ['HyenaDNA-32k','Enformer','CLASTER','Target']\n",
    "\n",
    "            for i in range(len(axs)):\n",
    "                axs[i].set_ylim((0,50))\n",
    "                axs[i].legend()\n",
    "                axs[3].set_xlabel(f\"Distance from TSS (kbp)\", fontsize=12)\n",
    "                    \n",
    "            axs[3].set_ylabel(\"Reads\", fontsize=12)     \n",
    "            #fig.legend( bbox_to_anchor=(.84,1.003),ncol=2, fancybox=True, shadow=True, fontsize=8)\n",
    "            fig.savefig(Path(\"../figures/supplementary_figures/\") / f\"{id}.png\", dpi=200)\n",
    "            plt.close(fig)\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    # Correlations Hyena:\n",
    "    MODEL = \"hyenadna\"\n",
    "    plot_correlations(data_and_path_dict[MODEL][\"figure_path\"], np.log2(READ_SCALING_FACTOR*pred_list_df_hyena.values[:,:param_dict[MODEL][\"n_central_bins\"]].flatten()+1),  np.log2(READ_SCALING_FACTOR*actual_list_df_hyena.values[:,:param_dict[MODEL][\"n_central_bins\"]].flatten()+1), title=f\"{MODEL}_performance_ctrl_smoothL1.png\", cmap= \"binary\", binlims= (0,10), density =True, DELTA=False)\n",
    "    plot_correlations(data_and_path_dict[MODEL][\"figure_path\"], A_per_gene_dict[MODEL][\"Predicted\"],A_per_gene_dict[MODEL][\"Actual\"] , title=f\"{MODEL}_performance_ctrl_gene_avg.png\", cmap= \"binary\", binlims= (0,10), density =False, DELTA=False)\n",
    "    \n",
    "    # Correlations Enformer:\n",
    "    MODEL = \"Enformer\"\n",
    "    plot_correlations(data_and_path_dict[MODEL][\"figure_path\"], np.log2(READ_SCALING_FACTOR*pred_list_df_enformer.values[:,:param_dict[MODEL][\"n_central_bins\"]].flatten()+1),  np.log2(READ_SCALING_FACTOR*actual_list_df_enformer.values[:,:param_dict[MODEL][\"n_central_bins\"]].flatten()+1), title=f\"{MODEL}_performance_ctrl_smoothL1.png\", cmap= \"binary\", binlims= (0,10), density =True, DELTA=False)\n",
    "    plot_correlations(data_and_path_dict[MODEL][\"figure_path\"], A_per_gene_dict[MODEL][\"Predicted\"],A_per_gene_dict[MODEL][\"Actual\"] , title=f\"{MODEL}_performance_ctrl_gene_avg.png\", cmap= \"binary\", binlims= (0,10), density =False, DELTA=False)\n",
    "\n",
    "\n",
    "def plot_perturbed_profiles(results_path: Path, figure_path: Path, N_BINS: int, SCALE : int, sample_name: str, PERT_TYPE: str):\n",
    "\n",
    "    ids, predicted, actual = _get_predictions(results_path, N_BINS, condition_list = [\"_ctrl\"])\n",
    "\n",
    "    cmap = plt.cm.terrain  # define the colormap\n",
    "    # extract all colors from the .jet map\n",
    "    cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "    # force the first color entry to be grey\n",
    "\n",
    "    # create the new map\n",
    "    cmap = mpl.colors.LinearSegmentedColormap.from_list(\n",
    "        'Custom cmap', cmaplist, cmap.N)\n",
    "\n",
    "    norm = mpl.colors.Normalize(vmin=0, vmax=18)\n",
    "\n",
    "    fig = plt.figure(figsize=(8,2))\n",
    "    i = 0\n",
    "    for index in predicted.index:\n",
    "        ref_gene, entity_id, _ = index.split('_')\n",
    "        if sample_name in index:\n",
    "            # Get the normalized color index for the current loop iteration\n",
    "            color = cmap(norm(i))\n",
    "            plt.plot(np.arange(-N_BINS,N_BINS+1), predicted.loc[f\"{index}\"], color=color, lw=.8)\n",
    "            start = table[(table[\"Ref_gene\"] == ref_gene) & (table[\"Entity_ID\"] == entity_id)]['Entity_rel_Start']/SCALE\n",
    "            end = table[(table[\"Ref_gene\"] == ref_gene) & (table[\"Entity_ID\"] == entity_id)]['Entity_rel_End']/SCALE\n",
    "            plt.plot([start,end],i+np.array([25,25]), color=color, lw=1.5)\n",
    "            plt.xlabel(\"Distance from TSS (kbp)\")\n",
    "            plt.xlim((-200,200))\n",
    "            plt.ylim((0,40))\n",
    "            plt.ylabel(\"Reads\")\n",
    "            i += 1\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(figure_path / f\"Perturbed_profiles_{sample_name}_{PERT_TYPE}.png\", dpi=200)\n",
    "\n",
    "\n",
    "# Latent representation analysis\n",
    "def visualize_filters(path: Path, \n",
    "                       savepath: Path,\n",
    "                       first_validation_sample_index: int = 0, \n",
    "                       last_validation_sample_index: int=100, \n",
    "                       step: int = 1,\n",
    "                       layer_name: str = \"\"): \n",
    "    \"\"\"\n",
    "     This function plots the intermediate outputs of the network. This is useful to understand what filters  \n",
    "     are being created and what are their roles. \n",
    "    \"\"\"\n",
    "\n",
    "    import matplotlib as mpl\n",
    "  \n",
    "    latents = np.load(path / \"latents.npy\") \n",
    "  \n",
    "    for i,sample in enumerate(latents[first_validation_sample_index:last_validation_sample_index:step]): \n",
    "        sample_name = sample[1] \n",
    "        sample_values = sample[0]\n",
    "        n_filters_in_layer = sample_values.shape[0]\n",
    "        print(np.min(sample_values), np.max(sample_values))\n",
    "        fig, axs = plt.subplots(n_filters_in_layer, figsize=(10,10))\n",
    "        #fig_stack, axs_stack = plt.subplots(n_filters_in_layer,1, figsize=(8,8))\n",
    "        for j,filter in enumerate(sample_values): \n",
    "            x = j//int(n_filters_in_layer/4)\n",
    "            y = j%int(n_filters_in_layer/4)\n",
    "            #filter = rotate(filter, angle=45, reshape=False)\n",
    "            im = axs[j].imshow(filter, cmap=\"seismic\") \n",
    "            #plt.colorbar(cm.ScalarMappable( cmap=\"inferno\"), ax=axs[j//4, j%4])\n",
    "            #cb = plt.colorbar(im, ax=axs[x, y], fraction=0.046, pad=0.04) \n",
    "            #cb.ax.tick_params(labelsize=12)  \n",
    "            #axs[x,y].set_title(f\"filter {j}\", fontsize=12) \n",
    "            axs[j].set_xticks([])\n",
    "            axs[j].set_yticks([])\n",
    "\n",
    "            # Stacked plot\n",
    "            # fx = np.zeros(len(filter))\n",
    "            # shift = -int(len(filter)/2)\n",
    "            # for i in range(len(filter)):\n",
    "            #     roll_im = np.roll(filter[i],shift-i)\n",
    "            #     fx += roll_im/len(filter)\n",
    "            # axs_stack[j].plot(np.arange(len(fx)),fx, lw=.5, label=j)\n",
    "            # gradient = np.convolve(fx,np.array([1,0,-1]))\n",
    "            # axs_stack[j].plot(np.arange(len(gradient)), gradient, ls = \"dashed\",lw=.5, label=f\"{j}_conv\")  \n",
    "        fig.savefig(savepath / f\"{sample_name}_{layer_name}.png\", dpi=250)\n",
    "        #fig_stack.savefig(savepath / f\"{sample_name}_{layer_name}_stacked.png\", dpi=250)\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "    return fig "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model performance on the baseline prediction task\n",
    "**Figure 1 and related supplementary figures**\n",
    "\n",
    "> Note: ⏰ This took 42 min when analysing all genes and enhancers in chr4:\n",
    "\n",
    "We will first add meaningful columns to the gene-enhancer coordinate information table like EP-distance, number of genes between E and P... We will do that for the samples in the test set (chr4). This information will be used in the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "samples_index_path = Path(\"../targets/perturbed_targets.csv\")\n",
    "samples_data_path = Path(\"../annotations/gene_enhancer_relationships_corrected.tsv\")\n",
    "perturbed_table_path = Path(\"../annotations/gene_enhancer_relationships_perturbed.csv\")\n",
    "\n",
    "filter_and_extend_gene_enhancer_table(samples_index_path, samples_data_path, perturbed_table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load the predictions and compare them with the ground truth values. We will do that for point-wise predictions and also integrating the area under each reference gene. The area under the curve within the central gene's coordinates, a proxy for length normalized nascent expression, will be added to the table that we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Compute the correlations and add area for the central gene to the table\n",
    "results_path = Path(\"../runs/test_runs/gene_expression_only_chrom_pure_conv/\") #Path(\"../runs/test_runs/gene_expression_only_chrom_pure_conv/\") #chrom_and_microc_with_attention\") #Path(\"/Users/wjq311/Desktop/PhD/Enhancer_logic_project/Data/results_all/results_no_H3K27ac_uncoupled/expression_output\")\n",
    "gene_pos_path: Path = Path(\"../annotations/gene_enhancer_relationships_corrected.tsv\")\n",
    "input_table_path: Path = Path(\"../annotations/gene_enhancer_relationships_perturbed.csv\")\n",
    "\n",
    "figure_path = Path(\"../figures/Figure_1/\")\n",
    "figure_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "condition_list = [\"_ctrl\"] #[\"\"]\n",
    "cmap = \"binary\"\n",
    "\n",
    "pred_list_A_per_bin_dict, actual_list_A_per_bin_dict, pred_list_values_dict, actual_list_values_dict = calculate_correlations(results_path, gene_pos_path,figure_path, input_table_path)\n",
    "\n",
    "for condition in condition_list:\n",
    "    # Prediction of all bins (1kbp resolution)\n",
    "    pred_list_values = np.log2(np.array(pred_list_values_dict[condition])+1) # Multiplying by 10 to convert from reads to decareads\n",
    "    actual_list_values = np.log2(np.array(actual_list_values_dict[condition])+1)\n",
    "    #pointwise = plot_correlations(figure_path, pred_list_values, actual_list_values, condition + \"_binpred_noH3K27ac\", cmap=cmap, binlims=(-10,100))\n",
    "    pointwise = plot_correlations(figure_path, pred_list_values, actual_list_values, condition + \"_pointwise_prediction\", cmap=cmap, binlims=(0,10))\n",
    "\n",
    "    # Central gene area (divided by gene length, equiv. to averaged EU-seq signal for the target gene)\n",
    "    pred_list_values = np.array(pred_list_A_per_bin_dict[condition]) \n",
    "    actual_list_values = np.array(actual_list_A_per_bin_dict[condition])\n",
    "    area = plot_correlations(figure_path, pred_list_values, actual_list_values, condition + \"_gene_area_lengthnorm_prediction\", cmap=cmap, density=False)\n",
    "\n",
    "\n",
    "pointwise_avg = plot_correlations(figure_path, np.concatenate([pred_list_values_dict[condition] for condition in condition_list]), np.concatenate([actual_list_values_dict[condition] for condition in condition_list]), \"_binpred_noH3K27ac_both_conditions\", cmap=\"afmhot\", binlims=(-10,100))\n",
    "area = plot_correlations(figure_path, np.concatenate([pred_list_A_per_bin_dict[condition] for condition in condition_list]), np.concatenate([actual_list_A_per_bin_dict[condition] for condition in condition_list]), condition + \"_gene_area_norm_noH3K27ac_both_conditions\", cmap=\"afmhot\", density=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Benchmark models predictions: Enformer and Hyenadna**\n",
    "\n",
    "> Note: Create a folder ../benchmarks/Hyena/ and move the predictions (which were by default saved in the scripts directory).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "#####\n",
    "# Paths:    \n",
    "data_and_path_dict = {\"hyenadna\":{\"predictions\":np.load(Path(\"../benchmarks/Hyena/Hyena_finetunned_predictions.npy\")).flatten(),\n",
    "                                  \"actual\": np.load(Path(\"../benchmarks/Hyena/Hyena_finetunned_targets.npy\")).flatten(),\n",
    "                                  \"figure_path\":Path(\"../figures/supplementary_figures/Hyena_benchmarks/\")},\n",
    "                      \"Enformer\":{\"predictions\":np.load(Path(\"../benchmarks/Enformer/enformer_test_predictions.npy\")).flatten(),\n",
    "                                  \"actual\":np.load(Path(\"../benchmarks/Enformer/enformer_test_targets.npy\")).flatten(),\n",
    "                                  \"figure_path\":Path(\"../figures/supplementary_figures/Enformer_benchmarks/\")}}\n",
    "\n",
    "gene_pos_path: Path = Path(\"../annotations/gene_enhancer_relationships_corrected.tsv\")\n",
    "resolution = 1000\n",
    "results_path = Path(\"../runs/test_runs/gene_expression_only_chrom_pure_conv/\") #Path(\"/Users/wjq311/Desktop/PhD/Enhancer_logic_project/Data/results_all/results_no_H3K27ac_uncoupled/expression_output\")\n",
    "target_samples = pd.read_csv(Path(\"../targets/test_targets.csv\"), sep=\",\").set_index(\"ID\")\n",
    "target_samples = target_samples[~target_samples.index.str.contains('_rev')]\n",
    "\n",
    "print(target_samples.index[::-23])\n",
    "condition_list = [\"_ctrl\"] #[\"\"]\n",
    "\n",
    "READ_SCALING_FACTOR = 1\n",
    "PLOT_LIST =  ['ENSMUSG00000078673.10_forward', 'ENSMUSG00000054885.11_forward',\n",
    "       'ENSMUSG00000089773.7_forward', 'ENSMUSG00000046133.3_forward',\n",
    "       'ENSMUSG00000040372.2_forward', 'ENSMUSG00000028784.14_forward',\n",
    "       'ENSMUSG00000050234.7_forward', 'ENSMUSG00000057375.13_forward',\n",
    "       'ENSMUSG00000111410.1_forward', 'ENSMUSG00000006442.10_forward',\n",
    "       'ENSMUSG00000078672.2_forward', 'ENSMUSG00000028602.12_forward',\n",
    "       'ENSMUSG00000024793.14_forward', 'ENSMUSG00000078626.2_forward',       \n",
    "       'ENSMUSG00000078490.10_forward', 'ENSMUSG00000028902.4_forward',\n",
    "       'ENSMUSG00000042380.8_forward', 'ENSMUSG00000115115.1_forward',\n",
    "       'ENSMUSG00000033253.18_forward', 'ENSMUSG00000028788.14_forward',\n",
    "       'ENSMUSG00000036896.5_forward', 'ENSMUSG00000017264.16_forward',\n",
    "       'ENSMUSG00000028743.7_forward', 'ENSMUSG00000028699.9_forward',\n",
    "       'ENSMUSG00000063172.13_forward', 'ENSMUSG00000111611.1_forward',\n",
    "       'ENSMUSG00000033326.15_forward', 'ENSMUSG00000057722.17_forward',\n",
    "       'ENSMUSG00000023153.9_forward', 'ENSMUSG00000028635.7_forward',\n",
    "       'ENSMUSG00000036896.5_forward',\n",
    "       'ENSMUSG00000115115.1_forward', 'ENSMUSG00000078490.10_forward',\n",
    "       'ENSMUSG00000028476.13_forward', 'ENSMUSG00000037692.14_forward',\n",
    "       'ENSMUSG00000028950.3_forward', 'ENSMUSG00000045589.7_forward',\n",
    "       'ENSMUSG00000078674.2_forward', 'ENSMUSG00000028634.17_forward',\n",
    "       'ENSMUSG00000036887.5_forward', 'ENSMUSG00000028948.16_forward',\n",
    "       'ENSMUSG00000028214.13_forward', 'ENSMUSG00000028410.13_forward',\n",
    "       'ENSMUSG00000061455.13_forward', 'ENSMUSG00000028576.12_forward',\n",
    "       'ENSMUSG00000020220.16_forward', 'ENSMUSG00000057375.13_forward',\n",
    "       'ENSMUSG00000028339.17_forward', 'ENSMUSG00000039774.12_forward',\n",
    "       'ENSMUSG00000066191.12_forward', 'ENSMUSG00000041120.6_forward',\n",
    "       'ENSMUSG00000028917.14_forward', 'ENSMUSG00000010517.7_forward',\n",
    "       'ENSMUSG00000061887.14_forward', 'ENSMUSG00000048626.5_forward',\n",
    "       'ENSMUSG00000028549.17_forward', 'ENSMUSG00000009640.11_forward',\n",
    "       'ENSMUSG00000078719.2_forward', 'ENSMUSG00000037443.13_forward',\n",
    "       'ENSMUSG00000028467.15_forward', 'ENSMUSG00000028553.12_forward',\n",
    "       'ENSMUSG00000028847.8_forward', 'ENSMUSG00000028976.10_forward',\n",
    "       'ENSMUSG00000028245.15_forward', 'ENSMUSG00000028980.14_forward',\n",
    "       'ENSMUSG00000039911.13_forward', 'ENSMUSG00000042608.15_forward',\n",
    "       'ENSMUSG00000028312.19_forward', 'ENSMUSG00000041153.9_forward',\n",
    "       'ENSMUSG00000054659.13_forward', 'ENSMUSG00000047613.10_forward',\n",
    "       'ENSMUSG00000039546.9_forward', 'ENSMUSG00000039577.17_forward',\n",
    "       'ENSMUSG00000028688.13_forward', 'ENSMUSG00000005045.16_forward',\n",
    "       'ENSMUSG00000043383.5_forward', 'ENSMUSG00000063077.15_forward',\n",
    "       'ENSMUSG00000070985.3_forward', 'ENSMUSG00000062545.4_forward',\n",
    "       'ENSMUSG00000061894.15_forward', 'ENSMUSG00000028974.13_forward',\n",
    "       'ENSMUSG00000028857.16_forward', 'ENSMUSG00000055900.14_forward',\n",
    "       'ENSMUSG00000028779.16_forward', 'ENSMUSG00000003810.13_forward',\n",
    "       'ENSMUSG00000028614.14_forward', 'ENSMUSG00000040372.2_forward',\n",
    "       'ENSMUSG00000028648.13_forward', 'ENSMUSG00000029005.4_forward',\n",
    "       'ENSMUSG00000078716.10_forward', 'ENSMUSG00000043085.14_forward',\n",
    "       'ENSMUSG00000050395.9_forward', 'ENSMUSG00000094293.1_forward',\n",
    "       'ENSMUSG00000028417.3_forward', 'ENSMUSG00000006215.12_forward',\n",
    "       'ENSMUSG00000025413.13_forward', 'ENSMUSG00000073988.13_forward',\n",
    "       'ENSMUSG00000029076.14_forward', 'ENSMUSG00000039410.16_forward',\n",
    "       'ENSMUSG00000060268.12_forward', 'ENSMUSG00000002384.2_forward',\n",
    "       'ENSMUSG00000078772.2_forward', 'ENSMUSG00000028560.11_forward',\n",
    "       'ENSMUSG00000029029.14_forward', 'ENSMUSG00000028403.15_forward',\n",
    "       'ENSMUSG00000028656.14_forward', 'ENSMUSG00000087166.9_forward',\n",
    "       'ENSMUSG00000042608.15_forward', 'ENSMUSG00000048706.3_forward',\n",
    "       'ENSMUSG00000037157.8_forward', 'ENSMUSG00000042616.8_forward',\n",
    "       'ENSMUSG00000043257.15_forward', 'ENSMUSG00000028786.15_forward',\n",
    "       'ENSMUSG00000078513.2_forward', 'ENSMUSG00000028393.10_forward',\n",
    "       'ENSMUSG00000028277.13_forward', 'ENSMUSG00000040536.15_forward',\n",
    "       'ENSMUSG00000045699.4_forward', 'ENSMUSG00000025328.9_forward',\n",
    "       'ENSMUSG00000028851.6_forward', 'ENSMUSG00000028794.13_forward',\n",
    "       'ENSMUSG00000028772.19_forward', 'ENSMUSG00000028763.18_forward',] \n",
    "\n",
    "plot_benchmark_models_predictions(data_and_path_dict, results_path,target_samples,condition_list, READ_SCALING_FACTOR,PLOT_LIST,  gene_pos_path, resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Model performance comparison**\n",
    "\n",
    "Comparison shown in Figure 3.\n",
    "> *Note: Since we only want forward strand sequence predictions, we will remove the \"_rev\" samples from the enformer's test (both in target csv and input npy arrays)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "figure_path = Path(\"../figures/Figure_2/\")\n",
    "performance_dicts = [{\"Enformer embeddings\":{\"Spearman\":0.8067,\"Pearson\":0.8739},\n",
    "                    \"HyenaDNA-32k fine tuned\":{\"Spearman\":0.7514,\"Pearson\":0.7403},\n",
    "                    \"Convolutions \\non chomatin marks\":{\"Spearman\":0.7814,\"Pearson\":0.8852},\n",
    "                    \"Convolutions and attention \\non chromatin marks\":{\"Spearman\":0.7815,\"Pearson\":0.8808},\n",
    "                    \"Convolutions \\non chromatin marks and structure rotated\":{\"Spearman\":0.7717,\"Pearson\":0.8690},\n",
    "                    \"Convolutions \\non chromatin marks and structure original\":{\"Spearman\":0.7066,\"Pearson\":0.8025}},\n",
    "                    {\"Enformer embeddings\":{\"Spearman\":0.9093,\"Pearson\":0.7874}, #Areas from here\n",
    "                    \"HyenaDNA-32k fine tuned\":{\"Spearman\":0.8053,\"Pearson\":0.5247},\n",
    "                    \"CLASTER\":{\"Spearman\":0.9275,\"Pearson\":0.8859}}]\n",
    "                    \n",
    "color_list = [\"orange\",\"darkred\",\"silver\",\"black\",\"royalblue\",\"purple\"]\n",
    "\n",
    "cmap = plt.cm.inferno  # define the colormap\n",
    "# extract all colors from the .jet map\n",
    "cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "# force the first color entry to be grey\n",
    "\n",
    "cmap = mpl.colors.LinearSegmentedColormap.from_list(\n",
    "        'Custom cmap', cmaplist, cmap.N)\n",
    "norm = mpl.colors.Normalize(vmin=0, vmax=6)\n",
    "color_list = [cmap(norm(i)) for i in range(6)]\n",
    "\n",
    "model_comparison(figure_path, performance_dicts[0], color_list)\n",
    "\n",
    "performance_dicts = [{\"Enformer embeddings\":{\"Spearman\":0.8067,\"Pearson\":0.8739},\n",
    "                    \"HyenaDNA-32k fine tuned\":{\"Spearman\":0.7514,\"Pearson\":0.7403},\n",
    "                    \"CLASTER\":{\"Spearman\":0.7717,\"Pearson\":0.8690}},\n",
    "                    {\"Enformer embeddings\":{\"Spearman\":0.9093,\"Pearson\":0.7874}, #Areas from here\n",
    "                    \"HyenaDNA-32k fine tuned\":{\"Spearman\":0.8053,\"Pearson\":0.5247},\n",
    "                    \"CLASTER\":{\"Spearman\":0.9275,\"Pearson\":0.8859}}]\n",
    "\n",
    "model_comparison_II(figure_path, performance_dicts, color_list[::2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _In silico_ perturbations\n",
    "\n",
    "**Figure 2 and related supplementary figures**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ⏰ The following step takes a few minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first add the area under the reference gene when silencing a given enhancer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = Path(\"../runs/perturbation_runs/gene_expression_only_chrom_pure_conv/\")\n",
    "figure_path = Path(\"../figures/Figure_1/\")\n",
    "gene_pos_path: Path = Path(\"../annotations/gene_enhancer_relationships_corrected.tsv\")\n",
    "input_table_path: Path = Path(\"../annotations/gene_enhancer_relationships_perturbed.csv\")\n",
    "\n",
    "add_gene_area_after_enhancer_perturbation(results_path, gene_pos_path, figure_path, input_table_path,  N_BINS =  200, condition_list = [\"_ctrl\"], resolution = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "input_table_path: Path = Path(\"../annotations/gene_enhancer_relationships_perturbed.csv\")\n",
    "table = pd.read_csv(input_table_path)\n",
    "\n",
    "table[table[\"Ref_gene\"] == \"ENSMUSG00000039693.11\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "# Plot perturbed profile predictions\n",
    "results_path = Path(\"../runs/perturbation_runs/gene_expression_pure_conv_perturbed_only_H3K27ac\") # Path(\"../runs/perturbation_runs/gene_expression_only_chrom_pure_conv/\") # Change plot name to _original\n",
    "N_BINS = 200\n",
    "SCALE = 1000\n",
    "PERT_TYPE = \"only_H3K27ac\"\n",
    "figure_path = Path(\"../figures/Figure_2/\")\n",
    "sample_names = [\"ENSMUSG00000028345.15\", \"ENSMUSG00000028347.14\",\"ENSMUSG00000028344.12\" ]#table['Ref_gene'].unique()[::10] #[ \"ENSMUSG00000000409.14\", \"ENSMUSG00000000085.16\", ]\n",
    "\n",
    "for sample_name in sample_names:\n",
    "    plot_perturbed_profiles(results_path, figure_path, N_BINS, SCALE, sample_name, PERT_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now compute:\n",
    "1) The distance dependence of the in-silico enhancer silencing impact.\n",
    "2) Which genes are the most affected and what's their relative location to the affected gene.\n",
    "- We will first compute all the E-P pairs that are available. \n",
    "- We will store separately, for each enhancer, the gene that was the most affected and its relative properties to the enhancer (distance, adjacency and number of genes in between E and P).\n",
    "- We will histogram the properties of the most affected gene and normalize by the background frequencies: While most enhancers might have zero or 1 gene in between, it is rare that an enhancer will have 15 genes between it and another gene that appears in the same sample. Weapplied a reciprocal normalization, i.e. as if we had a dataset with the same number of cases for all categories, which allows us to compare the enrichment on certain categories relative to the \"background\".\n",
    "\n",
    "⚠️ The different histograms follow parallel normalization schemes, yielding an apparent discrepancy between adjacent cases and \"0 genes in between\".\n",
    "\n",
    "> ⚠️ IMPORTANT NOTE: Run first the cell where correlations are predicted for the model you used for the perturbed predictions. Otherwise the comparisons do not make sense!!!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "\n",
    "def calculate_histogram(data, bins):\n",
    "    \"\"\"\n",
    "    Calculate the histogram of given data.\n",
    "    \n",
    "    Args:\n",
    "        data (array-like): The data for which to calculate the histogram.\n",
    "        bins (array-like): The bin edges.\n",
    "\n",
    "    Returns:\n",
    "        hist (np.ndarray): The counts of data in each bin.\n",
    "        bins (np.ndarray): The edges of the bins.\n",
    "    \"\"\"\n",
    "    hist, bins = np.histogram(data, bins=bins)\n",
    "    return hist.astype(np.float32), bins\n",
    "\n",
    "def plot_normalized_histogram(ax, data, reference_data=None, bins=np.arange(-0.5, 15.6, 1), eps=0.1, color='royalblue', xlabel='', ylabel='Frequency', xticks=None, xticklabels=None):\n",
    "    \"\"\"\n",
    "    Plot a normalized histogram, optionally normalizing against a reference distribution, with explicit x-ticks.\n",
    "    \n",
    "    Args:\n",
    "        ax (matplotlib.axes.Axes): The axes object to plot on.\n",
    "        data (array-like): The data to plot.\n",
    "        reference_data (array-like, optional): The reference data for normalization.\n",
    "        bins (array-like): The bin edges.\n",
    "        eps (float): Epsilon value for adjusting bar width.\n",
    "        color (str): Color of the histogram bars.\n",
    "        xlabel (str): X-axis label.\n",
    "        ylabel (str): Y-axis label.\n",
    "        xticks (list, optional): Positions of the x-ticks. If None, automatic x-ticks will be used.\n",
    "        xticklabels (list of str, optional): Labels of the x-ticks. If None, labels are generated from xticks.\n",
    "    \"\"\"\n",
    "    hist, bins = calculate_histogram(data, bins)\n",
    "    frequency_vector = hist / hist.sum()  # Normalize to sum to 1\n",
    "    \n",
    "    if reference_data is not None:\n",
    "        reference_hist, _ = calculate_histogram(reference_data, bins)\n",
    "        reference_frequency = reference_hist / reference_hist.sum()\n",
    "        frequency_vector = frequency_vector / reference_frequency\n",
    "        frequency_vector /= frequency_vector.sum()  # Re-normalize to sum to 1\n",
    "    \n",
    "    ax.bar(bins[:-1], frequency_vector, width=(bins[1]-bins[0] - eps), color=color)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    \n",
    "    # Set x-ticks if specified\n",
    "    if xticks is not None and xticklabels is not None:\n",
    "        ax.set_xticks(xticks)\n",
    "        ax.set_xticklabels(xticklabels, rotation=0)  # Adjust rotation as needed for readability\n",
    "    elif xticks is not None:\n",
    "        ax.set_xticks(xticks)\n",
    "        # Generate automatic xticklabels if only xticks are provided\n",
    "        ax.set_xticklabels([f\"{tick:.1f}\" for tick in xticks], rotation=0)\n",
    "\n",
    "\n",
    "def plot_perturbation_histograms(df, figure_path):\n",
    "    # Filter out rows where Area is 0.0 for reference genes and their enhancers\n",
    "    non_zero_ref_genes = df[df['Area'] > 0.0]['Ref_gene'].unique()\n",
    "    df_filtered = df[df['Ref_gene'].isin(non_zero_ref_genes)]\n",
    "    \n",
    "    # Calculate Absolute Relative Change\n",
    "    df_filtered['Absolute_Relative_Change'] = abs(\n",
    "        (df_filtered['Area'] - df_filtered.groupby('Ref_gene')['Area'].transform('first')) /\n",
    "        df_filtered.groupby('Ref_gene')['Area'].transform('first')\n",
    "    )\n",
    "\n",
    "    # Exclude rows where Ref_gene equals Entity_ID (gene rows) to focus on enhancers\n",
    "    enhancers_filtered = df_filtered[df_filtered['Ref_gene'] != df_filtered['Entity_ID']]\n",
    "\n",
    "    # Bin EP_distance into 1000 bp bins\n",
    "    max_distance = 15000\n",
    "    min_distance = 0\n",
    "    scale = 1000\n",
    "    bins = np.arange(min_distance - min_distance % scale, max_distance + scale, scale)  # Create bins edges\n",
    "    enhancers_filtered['EP_distance_bin'] = pd.cut(enhancers_filtered['EP_distance'], bins=bins, include_lowest=True)\n",
    "\n",
    "    # Plot Swarm Plots with Binned EP_distance\n",
    "    fig = plt.figure(figsize=(8, 3))\n",
    "    sns.swarmplot(x='EP_distance_bin', y='Absolute_Relative_Change', data=enhancers_filtered, color='k', size=1)\n",
    "    plt.xlabel('Enhancer-Promoter distance (kbp)')\n",
    "    plt.ylabel('Relative Change')\n",
    "    plt.xticks(np.arange(15), [f\"{a}-{a+1}\" for a in range(15)])  # Improve label readability\n",
    "    plt.ylim((0, .6))\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(figure_path / \"Distance_effect.png\", dpi=200)\n",
    "\n",
    "    # Filter DataFrame to keep only rows with the largest Absolute_Relative_Change for each enhancer\n",
    "    enhancers_largest_change = enhancers_filtered.groupby('Entity_ID').apply(lambda x: x.nlargest(1, 'Absolute_Relative_Change')).reset_index(drop=True)\n",
    "\n",
    "    gs = gridspec.GridSpec(1, 2, width_ratios=[3, 1])\n",
    "    fig = plt.figure(figsize=(10, 3))\n",
    "\n",
    "    # Plot frequency histograms for all Enhancer-Promoter interactions\n",
    "    ax1 = fig.add_subplot(gs[0, 0]) \n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "\n",
    "    plot_normalized_histogram(\n",
    "        ax1,\n",
    "        data=enhancers_filtered['Genes_in_between'],\n",
    "        bins=np.arange(-0.01, 15.6, 1),\n",
    "        xlabel='Genes in between',\n",
    "        ylabel='Frequency',\n",
    "        color='k'\n",
    "    )\n",
    "    plot_normalized_histogram(\n",
    "        ax2,\n",
    "        data=enhancers_filtered['Adjacent'],\n",
    "        xlabel='',\n",
    "        bins=[0,1,2],\n",
    "        ylabel='Frequency',\n",
    "        xticks = np.arange(2),\n",
    "        xticklabels = ['Skip','Adjacent'],\n",
    "        color='k'\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(figure_path / \"Background_frequencies.png\", dpi=200)\n",
    "\n",
    "    # Plot histograms for most relevant EP interactions with normalization against the background distribution\n",
    "\n",
    "    gs = gridspec.GridSpec(2, 1)\n",
    "    fig = plt.figure(figsize=(8, 10))\n",
    "\n",
    "    # Plot frequency histograms for all Enhancer-Promoter interactions\n",
    "    ax1 = fig.add_subplot(gs[0, 0]) \n",
    "    ax2 = fig.add_subplot(gs[1, 0])\n",
    "\n",
    "    plot_normalized_histogram(\n",
    "    ax1,\n",
    "    data=enhancers_filtered['Genes_in_between'],\n",
    "    bins=np.arange(-0.01, 15.6, 1),\n",
    "    xlabel='Genes in between',\n",
    "    ylabel='Frequency',\n",
    "    color='k'\n",
    "    )\n",
    "\n",
    "    plot_normalized_histogram(\n",
    "        ax2,\n",
    "        data=enhancers_largest_change['Genes_in_between'],\n",
    "        reference_data=enhancers_filtered['Genes_in_between'],\n",
    "        bins=np.arange(-0.01, 15.6, 1),\n",
    "        xlabel='Genes in between',\n",
    "        ylabel='Frequency',\n",
    "        xticks = np.arange(15),\n",
    "        xticklabels = np.arange(15),\n",
    "        color='k'\n",
    "\n",
    "    )\n",
    "    # plot_normalized_histogram(\n",
    "    #     ax2,\n",
    "    #     data=enhancers_largest_change['Adjacent'],\n",
    "    #     #reference_data=enhancers_filtered['Adjacent'],\n",
    "    #     bins=[0,1,2],\n",
    "    #     xlabel='',\n",
    "    #     ylabel='Frequency',\n",
    "    #     xticks = np.arange(2),\n",
    "    #     xticklabels = ['Skip','Adjacent'],\n",
    "    #     color='k'\n",
    "    # )\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(figure_path / \"Normalized_frequency_hist.png\", dpi=200)\n",
    "\n",
    "\n",
    "plot_perturbation_histograms(table, Path(\"../figures/Figure_2/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 2b: Chromatin mark correlations**\n",
    "\n",
    "Visualize possible chromatin states found in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "array_path = Path(\"../inputs/landscape_arrays/test/\")\n",
    "figure_path = Path(\"../figures/Figure_2/\")\n",
    "figure_path.mkdir(parents=True, exist_ok=True)\n",
    "skip_n_bins = 20 #20\n",
    "\n",
    "plot_chromatin_state_combinations(array_path, figure_path, skip_n_bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attributions\n",
    "\n",
    "Now we will compute the attribution scores for every input position (4,10.001) towards every output (401). This mechanism tells how important is every input position (i.e. a given bin in each input track in the landscape or a given bin in a Micro-C matrix) for the prediction of the EU-seq levels at the position described by a given target node (401 targets). It also tells us the direction of the association.\n",
    "The original paper can be found at:\n",
    "\n",
    "https://arxiv.org/abs/1703.01365\n",
    "\n",
    "EIR uses the integrated gradients method as it is implemented in the captum library:\n",
    "\n",
    "https://captum.ai/docs/extension/integrated_gradients\n",
    "\n",
    "\n",
    "**Figure 3 and related supplementary figures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "results_path = Path(\"../runs/test_runs/gene_expression_microc_rotated_pure_conv/expression_output/\") #Path(\"../runs/gene_expression_only_chrom_pure_conv/results/expression_output/\") #Path(\"../runs/training_runs/gene_expression_exformer_unlimited_all_rotated/results/expression_output/\")\n",
    "figure_path = Path(\"../figures/Figure_3/\")\n",
    "figure_path.mkdir(parents=True, exist_ok=True)\n",
    "n_central_bins = 401\n",
    "l_in:int = 10001\n",
    "n_out:int=401\n",
    "n_in: int=4\n",
    "\n",
    "track_dict: dict = {0:{\"name\":\"ATAC-seq\",\"function\":\"Chromatin accessibility\",\"color\":\"k\"},\n",
    "        1:{\"name\":\"H3K4me3\",\"function\":\"Promoter\",\"color\":\"r\"},\n",
    "        2:{\"name\":\"H3K27ac\",\"function\":\"Enhancer\",\"color\":\"blue\"},\n",
    "        3:{\"name\":\"H3K27me3\",\"function\":\"Chromatin silencing\",\"color\":\"g\"}}\n",
    "TRAIN_ATTR: bool = False\n",
    "SPLIT = 60600 #17880\n",
    "read_target_attributions(results_path, figure_path, n_central_bins, track_dict,l_in,n_out,n_in, TRAIN_ATTR,SPLIT)\n",
    "\n",
    "\n",
    "results_path = Path(\"../runs/gene_expression_only_chrom_pure_conv/results/expression_output/\")\n",
    "figure_path = Path(\"../figures/Figure_3_only_chrom/\")\n",
    "figure_path.mkdir(parents=True, exist_ok=True)\n",
    "TRAIN_ATTR: bool = True\n",
    "SPLIT = 60600 #17880\n",
    "read_target_attributions(results_path, figure_path, n_central_bins, track_dict,l_in,n_out,n_in, TRAIN_ATTR,SPLIT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Micro-C Matrices: Original and rotated**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "name = \"ENSMUSG00000059552.13_forward.npy\" #\"ENSMUSG00000024406.16_forward.npy\" # ENSMUSG00000028948.16_forward ENSMUSG00000078626.2_forward ENSMUSG00000046667.14_rev # Original paper example \"ENSMUST00000105369.7.npy\" #\"ENSMUST00000002350.10_flipped\"\n",
    "pathlist = [Path('../inputs/microC/training/'), Path('../inputs/microC_rotated/training/')]\n",
    "figure_path = Path(\"../figures/supplementary_figures/\")\n",
    "figure_path.mkdir(exist_ok=True, parents=True)\n",
    "num_bins = 626\n",
    "window_of_observation = 707.10678118  # kbp (diagonal is not observing the same length in the genome.)\n",
    "\n",
    "plot_microc_original_and_rotated(pathlist, figure_path, num_bins, window_of_observation, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Micro-C attributions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "figure_path = Path(\"../figures/Figure_3/\")\n",
    "\n",
    "sample_ids = range(-200,201)\n",
    "num_bins = 626\n",
    "window_of_observation = 707.10678118  # kbp (diagonal is not observing the same length in the genome.)\n",
    "#bins_per_kbp = 0.885297690046\n",
    "ATTR_type_list = [\"abs\",\"signed\"]\n",
    "conditions = [\"_ctrl\"]\n",
    "sigma = 3\n",
    "\n",
    "plot_microc_attributions(figure_path, sample_ids, num_bins, window_of_observation, ATTR_type_list, conditions, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter visualization**\n",
    "\n",
    "We can visualize the activations of a set of predefined filters, in this case for the structural branch handling Micro-C matrices. We can see what is the network propagating forward or what features are extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Conv 0: first set of filters\n",
    "root_path = Path(\"../runs/gene_expression_microc_pure_conv_latents/latents/latent_outputs/3600/\")\n",
    "path = root_path / \"input_modules.contact_maps.feature_extractor.conv.0.conv_1\" \n",
    "savepath = Path(\"../figures/supplementary_figures/filters/\")\n",
    "#path = Path(\"/Users/wjq311/Desktop/eir/runs/gene_expression_exformer_unlimited_all/latents/latent_outputs/29800/input_modules.contact_maps.feature_extractor.conv.0.conv_1/\") \n",
    "visualize_filters(path,savepath, first_validation_sample_index = 1, last_validation_sample_index = 100, step= 1, layer_name= \"layer_0.1\") \n",
    "\n",
    "# # Conv 1: deeper set of filters\n",
    "path = root_path / \"input_modules.contact_maps.feature_extractor.conv.1.conv_1\"\n",
    "visualize_filters(path,savepath, first_validation_sample_index = 1, last_validation_sample_index = 100, step= 1, layer_name= \"layer_1.1\") \n",
    "\n",
    "# # Conv 2: deeper set of filters\n",
    "path = root_path / \"input_modules.contact_maps.feature_extractor.conv.2.conv_1\" \n",
    "visualize_filters(path, savepath, first_validation_sample_index = 1, last_validation_sample_index = 100, step= 1, layer_name= \"layer_2.1\") \n",
    "\n",
    "# # Conv 3: deeper set of filters\n",
    "path = root_path / \"input_modules.contact_maps.feature_extractor.conv.3.conv_1\" \n",
    "visualize_filters(path, savepath, first_validation_sample_index = 1, last_validation_sample_index = 100, step= 1, layer_name= \"layer_3.1\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Smoothing and downresolving output profiles**\n",
    "The main signal in the target EU-seq profiles oscillates at a hbp-kbp scale. Keeping bp resolution would:\n",
    "- create a high number of outputs, complicating the computations of attributions for every input output pair.\n",
    "- Constrain us to reduce the output window per sample. We could not be able to assess the effects of the perturbations on as many genes.\n",
    "- It is not clear that the information to predict those wiggles in the signal is encoded in the inputs (core chrom. landscape marks).\n",
    "\n",
    "Supplementary figure showing the downresolution of the targets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "import pyBigWig\n",
    "\n",
    "def smoothed_profiles(data_path: Path,\n",
    "                      savepath: Path,\n",
    "                      output_shift: int,\n",
    "                      n_output_bins:int,\n",
    "                      TSS: int,\n",
    "                      chrom: str,\n",
    "                      sigma:int,\n",
    "                      binsize: int):\n",
    "    \"\"\"\n",
    "    Targets are centered at the TSS, and obtained at a 20 bp resolution, smoothed and downsized to 1kbp resolution.\n",
    "    \"\"\"\n",
    "    bw = pyBigWig.open(str(data_path), \"r\")\n",
    "    stats = bw.stats(chrom,TSS-output_shift,TSS+output_shift,type=\"mean\",nBins=n_output_bins)\n",
    "    bw.close()\n",
    "    stats = np.array([float(value) if value is not None else 0. for value in stats])\n",
    "    stats = np.clip(np.array(stats),0,None)\n",
    "    \n",
    "    original_array = stats.copy()\n",
    "    stats = gaussian_filter1d(stats, sigma=sigma)\n",
    "    target_cond = np.zeros_like(stats)\n",
    "\n",
    "    # Averaging over a number of bins\n",
    "    for j in range(binsize):\n",
    "        target_cond += 1 / binsize * np.roll(stats, -j)\n",
    "\n",
    "    averaged_array = target_cond.copy()\n",
    "\n",
    "    final_array = target_cond[::binsize]\n",
    "\n",
    "    fig, axs = plt.subplots(3, figsize=(10,5))\n",
    "\n",
    "    for i in range(len(axs)):\n",
    "        axs[i].set_ylim((0,60))\n",
    "        axs[i].set_ylabel(\"Reads\")\n",
    "    axs[-1].set_xlabel(\"Bin number\")\n",
    "    axs[0].fill_between(np.arange(len(original_array)), original_array, color='k', lw=.1)\n",
    "    axs[1].fill_between(np.arange(len(averaged_array)), averaged_array, color='k')\n",
    "    axs[2].fill_between(np.arange(len(final_array)), final_array, color='k')\n",
    "    fig.savefig(savepath / \"Smoothed_targets.png\", dpi=200)\n",
    "\n",
    "data_path = Path(\"../GEO_files/EU_Seq_Ctrl.bw\")\n",
    "savepath = Path(\"../figures/supplementary_figures/\")\n",
    "output_shift = 200010 # Output in 20 bp resolution (to be smoothed and downsampled)\n",
    "n_output_bins = 20001\n",
    "chrom = 'chr4'\n",
    "TSS = 152039321\n",
    "sigma = 50\n",
    "binsize = 50\n",
    "\n",
    "smoothed_profiles(data_path,\n",
    "                  savepath, \n",
    "                  output_shift,\n",
    "                  n_output_bins,\n",
    "                  TSS,\n",
    "                  chrom,\n",
    "                  sigma,\n",
    "                  binsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyena and Enformer Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path \n",
    "N_BINS = 57\n",
    "SHIFT = 200\n",
    "\n",
    "path = Path(\"../inputs/DNA_sequences\")\n",
    "sample_id = \"ENSMUSG00000066324.2_forward.npy\" #\"ENSMUSG00000003282.9_forward.npy\" \n",
    "a = np.load(path / \"test_embeddings_Enformer\" / sample_id )\n",
    "b = pd.read_csv(\"../targets/test_targets.csv\").set_index('ID')\n",
    "d = np.load(path / \"test_embeddings_Hyena-DNA\" / sample_id)\n",
    "\n",
    "savepath = Path(\"../figures/supplementary_figures/\")\n",
    "\n",
    "fig, axs = plt.subplots(1, figsize=(5,2))\n",
    "axs.fill_between(np.arange(-N_BINS,N_BINS+1),b.loc[sample_id[:-4]][SHIFT-N_BINS:SHIFT+N_BINS+1], lw=1, color='darkgreen', label = \"Control EU-seq target\")\n",
    "#axs.fill_between(np.arange(len(b.loc[sample_id[:-4]])//2),b.loc[sample_id[:-4]][len(b.loc[sample_id[:-4]])//2:], lw=1, color='goldenrod', alpha=.5, label=\"Treated\")\n",
    "axs.set_ylabel(\"EU-seq (reads)\", rotation = 90, fontsize=14)\n",
    "axs.set_xlim((-N_BINS,N_BINS+1))\n",
    "#axs.set_yticks([0,6],[0,6])\n",
    "#axs.set_ylim((0,6.5))\n",
    "axs.set_xlabel(\"Distance from TSS (kbp)\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(savepath / f\"Profile.png\", dpi=200)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, figsize=(6,15))\n",
    "plt.imshow(a[0].T, cmap='inferno', vmin = 0, vmax=2)\n",
    "axs.set_xlabel(\"Distance to the TSS (kbp)\", fontsize=10)\n",
    "axs.set_xticks([0,448.5,896],[-57.408,0,57.408])\n",
    "axs.set_yticks([0,3071],[1,3072])\n",
    "axs.set_ylabel(\"Embedding depth\")\n",
    "plt.xlim((0,896))\n",
    "fig.savefig(savepath / \"Enformer_embedding.png\", dpi = 200)\n",
    "\n",
    "\n",
    "central_bins = 896\n",
    "bp_per_bin = 128\n",
    "central_width = central_bins*bp_per_bin\n",
    "crop_distance = (160002-central_width)//2\n",
    "\n",
    "fig3, axs = plt.subplots(1, figsize=(4.5,5))\n",
    "data = d[0].T[:,crop_distance:-crop_distance:1]\n",
    "axs.imshow(data, aspect='auto', cmap=\"seismic\", vmin = -4, vmax = 4)\n",
    "axs.set_xticks([0,data.shape[1]//2,data.shape[1]],[-57.408,0,57.408])\n",
    "axs.set_xlabel(\"Distance to the TSS (kbp)\", fontsize=10)\n",
    "axs.set_ylabel(\"Embedding depth\")\n",
    "fig3.savefig(savepath / \"Hyena_embeddings.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Benchmark model loss evolution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_losses(hyena_log_file, enformer_log_file, savepath):\n",
    "    # Function to read log file and extract losses\n",
    "    def extract_losses(log_file, train_pattern, test_pattern=None):\n",
    "        with open(log_file, 'r') as file:\n",
    "            log_data = file.read()\n",
    "        \n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        epochs = []\n",
    "\n",
    "        train_matches = re.findall(train_pattern, log_data)\n",
    "        if test_pattern:\n",
    "            test_matches = re.findall(test_pattern, log_data)\n",
    "        else:\n",
    "            test_matches = []\n",
    "\n",
    "        for match in train_matches:\n",
    "            epochs.append(int(match[0]))\n",
    "            train_losses.append(float(match[1]))\n",
    "            if len(match) > 2:\n",
    "                val_losses.append(float(match[2]))\n",
    "\n",
    "        if test_pattern:\n",
    "            for match in test_matches:\n",
    "                val_losses.append(float(match))\n",
    "\n",
    "        return epochs, train_losses, val_losses\n",
    "\n",
    "    # Extract HyenaDNA-32k data\n",
    "    hyena_train_pattern = r\"Train Epoch: (\\d+)\\s+Average Loss: ([\\d.]+)\"\n",
    "    hyena_test_pattern = r\"Test set: Average loss: ([\\d.]+)\"\n",
    "    hyena_epochs, hyena_train_losses, hyena_test_losses = extract_losses(hyena_log_file, hyena_train_pattern, hyena_test_pattern)\n",
    "\n",
    "    # Extract Enformer data\n",
    "    enformer_pattern = r\"Epoch \\[(\\d+)/\\d+\\], Training Loss: ([\\d.]+), Training R\\^2: [\\d.]+, Validation Loss: ([\\d.]+), Validation R\\^2: [\\d.]+\"\n",
    "    enformer_epochs, enformer_train_losses, enformer_val_losses = extract_losses(enformer_log_file, enformer_pattern)\n",
    "\n",
    "    # Plotting\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "    # HyenaDNA-32k plot\n",
    "    axes[0].plot(hyena_epochs, hyena_train_losses, label='Training Loss', marker='')\n",
    "    axes[0].plot(hyena_epochs, hyena_test_losses, label='Test Loss', marker='')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('HyenaDNA-32k')\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Enformer plot\n",
    "    axes[1].plot(enformer_epochs, enformer_train_losses, label='Training Loss', marker='')\n",
    "    axes[1].plot(enformer_epochs, enformer_val_losses, label='Validation Loss', marker='')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].set_title('Enformer')\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(savepath, dpi=200)\n",
    "\n",
    "# File paths\n",
    "hyena_log_file = '../checkpoints/hyenadna_32k.log'\n",
    "enformer_log_file = '../benchmarks/Enformer/training.log'\n",
    "savepath = '../figures/supplementary_figures/Train_test_loss_curves_benchmark.png'\n",
    "\n",
    "# Plot the losses\n",
    "plot_losses(hyena_log_file, enformer_log_file, savepath)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
