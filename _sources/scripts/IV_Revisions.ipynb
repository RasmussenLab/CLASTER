{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> REVISIONS: CODE ADDED <center>\n",
    "\n",
    "**Summary:**\n",
    "\n",
    "This notebook contains the analyses that were added during the reviewing process. It borrows elements from the rest of the notebooks, but it was made so that it can be run completely independently from the others. Some cells can be run locally, but others will create separate python files that were run on a slurm-based HPC.\n",
    "\n",
    "**SECTIONS:**\n",
    "-  I) Packages & Functions.\n",
    "- II) Creation of EIR config files to define CLASTER.\n",
    "- III) Enhancer-centric perturbational analysis.\n",
    "- IV) New models and analyses.\n",
    "- V) Performance evaluations.\n",
    "- VI) Data distributions.\n",
    "- VII) Obtain Promoter-Capture Hi-C for mESCs.\n",
    "- VIII) Perform analyses for K562.\n",
    "\n",
    "\n",
    "> Notes:\n",
    "> - We used eir version 0.1.42.\n",
    "> - We run the models on an A100 GPU.\n",
    "> - We added the genomic context of the predictions as IGV tracks describing:\n",
    ">   - Gene annotations (Default IGV annotations for mm10)\n",
    ">   - Regulatory element annotations: latest ENCODE release fro mm10 candidate Cis-Regulatory Elements (cCREs). Data was obtained from [this download server](https://hgdownload.soe.ucsc.edu/gbdb/mm10/encode3/ccre/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I) Packages &  Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global packages:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Packages: ########\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.patches as patches\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "plt.rcParams['font.family'] = 'Nimbus Roman'\n",
    "from scipy.integrate import simpson\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import import_ipynb\n",
    "import claster_utils # Import functions from data analysis notebook\n",
    "from typing import List, Tuple\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import shutil\n",
    "import seaborn as sns\n",
    "import wget\n",
    "from matplotlib import gridspec\n",
    "from scipy.ndimage import gaussian_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Old functions:**\n",
    "\n",
    "These are mostly functions used in the original analyses with slight adaptations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_dict = {0:{\"name\":\"ATAC-seq\",\"function\":\"Chromatin accessibility\",\"color\":\"k\"},\n",
    "            1:{\"name\":\"H3K4me3\",\"function\":\"Promoter\",\"color\":\"r\"},\n",
    "            2:{\"name\":\"H3K27ac\",\"function\":\"Enhancer\",\"color\":\"b\"},\n",
    "            3:{\"name\":\"H3K27me3\",\"function\":\"Chromatin silencing\",\"color\":\"g\"}}\n",
    "\n",
    "######### Functions: ########\n",
    "def visualize_input_array(a,\n",
    "                     cropped_bins: int = 4800, #4600\n",
    "                     scaling_factor: float = 1.,\n",
    "                     track_dict: dict = track_dict):\n",
    "    \"\"\"\n",
    "    Function to visualize an input numpy array with inset zoomed plots.\n",
    "    \"\"\"\n",
    "    # Create single column of plots\n",
    "    fig, axs = plt.subplots(len(track_dict), 1, figsize=(10,8))\n",
    "    \n",
    "    # Handle case where there's only one track (axs would not be array)\n",
    "    if len(track_dict) == 1:\n",
    "        axs = [axs]\n",
    "\n",
    "    # Plot the scaled arrays\n",
    "    for j, line in enumerate(a):\n",
    "        # Main plot (full window)\n",
    "        x_full = np.arange(-len(line)//2, len(line)//2)\n",
    "        axs[j].plot(x_full, line*scaling_factor, \n",
    "                   color=track_dict[j][\"color\"], \n",
    "                   label=track_dict[j][\"name\"], \n",
    "                   lw=.2)\n",
    "        axs[j].fill_between(x_full, line*scaling_factor, \n",
    "                          color=track_dict[j][\"color\"], \n",
    "                          alpha=1)\n",
    "        axs[j].set_ylabel(track_dict[j][\"name\"])\n",
    "        \n",
    "        # Create inset axes\n",
    "        inset_ax = inset_axes(axs[j], \n",
    "                             width=\"40%\", # width = 30% of parent_bbox\n",
    "                             height=\"50%\", # height : 30%\n",
    "                             loc='upper left')\n",
    "        \n",
    "        # Plot zoomed data in inset\n",
    "        x_zoom = x_full[cropped_bins:-cropped_bins]\n",
    "        y_zoom = line[cropped_bins:-cropped_bins]*scaling_factor\n",
    "        inset_ax.plot(x_zoom, y_zoom, \n",
    "                     color=track_dict[j][\"color\"], \n",
    "                     lw=.2)\n",
    "        inset_ax.fill_between(x_zoom, y_zoom, \n",
    "                            color=track_dict[j][\"color\"], \n",
    "                            alpha=.6)\n",
    "        inset_ax.set_xlim(x_zoom[0], x_zoom[-1])\n",
    "        inset_ax.yaxis.tick_right()\n",
    "        \n",
    "\n",
    "    # Set the bottom x-label\n",
    "    axs[-1].set_xlabel(\"Distance to TSS (hbp)\", fontsize=16)\n",
    "\n",
    "    # Adjust spacing between subplots\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig\n",
    "\n",
    "def _get_predictions(\n",
    "    results_path: Path,\n",
    "    N_BINS: int,\n",
    "    condition_list: List[str],\n",
    "    is_training: bool = False,\n",
    "    batch_num: str = None,\n",
    "    CLIP: bool = True\n",
    ") -> Tuple[List[str], pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Reads prediction files from EIR for each target bin condition.\n",
    "    \n",
    "    Args:\n",
    "        results_path: Path where the predictions are stored\n",
    "        N_BINS: Number of bins to one side of the central bin (e.g., 200 or 57 for Enformer)\n",
    "        condition_list: List of conditions (e.g., [\"_ctrl\"])\n",
    "        is_training: If True, uses training file structure, else uses test structure\n",
    "        batch_num: Batch number for training predictions (e.g., \"30300\")\n",
    "        CLIP: If True, applies ReLU to predictions (clips at 0)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "            - ids: List of sample names\n",
    "            - predicted: DataFrame of predicted untransformed values\n",
    "            - actual: DataFrame of actual values\n",
    "    \"\"\"\n",
    "    predicted_dfs = []\n",
    "    actual_dfs = []\n",
    "    \n",
    "    for condition in condition_list:\n",
    "        for i in range(-N_BINS, N_BINS + 1):\n",
    "            if is_training:\n",
    "                if batch_num is None:\n",
    "                    raise ValueError(\"batch_num must be specified for training predictions\")\n",
    "                file_path = results_path / f\"expression_output/{i}{condition}/samples/{batch_num}/regression_predictions.csv\"\n",
    "            else:\n",
    "                file_path = results_path / f\"expression_output/{i}{condition}/predictions.csv\"\n",
    "            \n",
    "            if not file_path.exists():\n",
    "                raise FileNotFoundError(f\"Prediction file not found: {file_path}\")\n",
    "                \n",
    "            predictions = pd.read_csv(file_path).set_index('ID')\n",
    "\n",
    "            # Apply ReLU to model predictions if specified\n",
    "            if CLIP:\n",
    "                predictions = predictions.clip(lower=0)\n",
    "\n",
    "            # Handle different column names for training vs test\n",
    "            if is_training:\n",
    "                predicted_column = predictions[\"Predicted\"].rename(f\"{i}{condition}\")\n",
    "                actual_column = predictions[\"Actual\"].rename(f\"{i}{condition}\")\n",
    "            else:\n",
    "                predicted_column = predictions[f\"{i}{condition} Untransformed\"].rename(f\"{i}{condition}\")\n",
    "                actual_column = predictions[\"True Label Untransformed\"].rename(f\"{i}{condition}\")\n",
    "\n",
    "            predicted_dfs.append(predicted_column)\n",
    "            actual_dfs.append(actual_column)\n",
    "\n",
    "    # Concatenate all DataFrames horizontally\n",
    "    predicted = pd.concat(predicted_dfs, axis=1)\n",
    "    actual = pd.concat(actual_dfs, axis=1)\n",
    "    ids = list(predicted.index)\n",
    "\n",
    "    return ids, predicted, actual\n",
    "\n",
    "def plot_target_predictions(line_p, line_a, N_BINS=200):\n",
    "    \"\"\" \n",
    "    Plot the predicted profiles. \n",
    "    \"\"\"\n",
    "    fig, axs =plt.subplots(2, figsize=(8,4), sharex = True)\n",
    "    axs[0].fill_between(np.arange(-N_BINS,N_BINS+1),line_p, lw=1, color='royalblue',alpha=.3, label=\"Predicted EU-seq\")\n",
    "    axs[1].fill_between(np.arange(-N_BINS,N_BINS+1),line_a, lw=1, color='silver', alpha=.6, label=\"EU-seq\")          \n",
    "    axs[1].set_xlabel(\"Distance from TSS (kbp)\", fontsize=12)    \n",
    "    axs[1].set_xlim(-200,200)  \n",
    "    fig.legend(ncol=2, fancybox=True, shadow=True, fontsize=8)\n",
    "\n",
    "    return fig\n",
    "\n",
    "def plot_generalized_predictions(predictions, actual_data, N_BINS=200):\n",
    "    \"\"\" \n",
    "    Plot multiple predicted profiles and actual data.\n",
    "    \n",
    "    Parameters:\n",
    "    predictions: list of dicts, each containing:\n",
    "        - 'profile': array-like, the prediction data\n",
    "        - 'color': str, color for the plot\n",
    "        - 'alpha': float, transparency value\n",
    "        - 'label': str, label for the legend\n",
    "    actual_data: dict containing:\n",
    "        - 'profile': array-like, the actual data\n",
    "        - 'color': str, default 'silver'\n",
    "        - 'alpha': float, default 0.6\n",
    "        - 'label': str, default 'EU-seq'\n",
    "    \"\"\"\n",
    "    # Create figure\n",
    "    fig, axs = plt.subplots(2, figsize=(8,4), sharex=True)\n",
    "    \n",
    "    # Plot all predictions in top subplot\n",
    "    x_range = np.arange(-N_BINS, N_BINS+1)\n",
    "    for pred in predictions:\n",
    "        axs[0].fill_between(x_range,\n",
    "                           pred['profile'],\n",
    "                           lw=1,\n",
    "                           color=pred['color'],\n",
    "                           alpha=pred['alpha'])\n",
    "        axs[0].plot(x_range,\n",
    "                           pred['profile'],\n",
    "                           lw=1,\n",
    "                           color=pred['color'],\n",
    "                           alpha=1,\n",
    "                           ls=pred['ls'],\n",
    "                           label=pred['label'])\n",
    "    \n",
    "    # Plot actual data in bottom subplot\n",
    "    for actual in actual_data:\n",
    "        axs[1].fill_between(x_range,\n",
    "                            actual['profile'],\n",
    "                            lw=1,\n",
    "                            color=actual['color'],\n",
    "                            alpha=actual['alpha'])\n",
    "        \n",
    "        axs[1].plot(x_range,\n",
    "                    actual['profile'],\n",
    "                    lw=1,\n",
    "                    color=actual['color'],\n",
    "                    ls=actual['ls'],\n",
    "                    label=actual['label'])\n",
    "        \n",
    "    # Set labels and limits\n",
    "    axs[1].set_xlabel(\"Distance from TSS (kbp)\", fontsize=12)    \n",
    "    axs[1].set_xlim(-200, 200)\n",
    "    axs[0].set_ylim(0, 65)\n",
    "    axs[1].set_ylim(0, 65)\n",
    "    \n",
    "    # Add legend\n",
    "    fig.legend(ncol=2, fancybox=True, shadow=True, fontsize=8, loc='upper center')\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def read_target_attributions(results_path: Path,\n",
    "                             figure_path: Path,\n",
    "                             track_dict: dict,\n",
    "                             N_BINS: int,\n",
    "                             l_in:int = 10001,\n",
    "                             n_in: int=4,\n",
    "                             TRAIN_ATTR: bool = True,\n",
    "                             SPLIT: int = None):\n",
    "\n",
    "    \"\"\"\n",
    "    This function is aimed to read attribution arrays from the chromatin landscape branch. Attribution arrays score the \n",
    "    contribution of each position in the input arrays (4,10.001) towards each output node/position (401).\n",
    "    Args:\n",
    "        results_path: path where the attribution arrays are stored.\n",
    "        figure_path: path where we want to store the outputs\n",
    "        n_central_bins: number of central bins (from -N_BINS to  N_BINS+1)\n",
    "        track_dict: dictionary containing the names of the tracks and plot details\n",
    "        l_in: input length \n",
    "        n_out: number of output nodes\n",
    "        n_in: number of input channels\n",
    "        TRAIN_ATTR: whether this is a training or test run (folder structure changes).\n",
    "        SPLIT: if Train, we need to know which of the stored batches it is\n",
    "\n",
    "    Returns:\n",
    "        Plots with attribution scores.\n",
    "    \"\"\"\n",
    "    figure_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    N_IN_BINS = l_in // 2\n",
    "    x_val = np.arange(-N_IN_BINS,N_IN_BINS+1)\n",
    "\n",
    "    xlim = (-500,500)\n",
    "\n",
    "    # Getting attributions:\n",
    "    condition_list = [\"_ctrl\"]\n",
    "    landscape_avg = {condition : np.zeros((n_in,l_in)) for condition in condition_list}\n",
    "\n",
    "    for i in range(-N_BINS, N_BINS + 1):\n",
    "        for condition in condition_list:\n",
    "            # CHROMATIN MARK ATTRIBUTIONS:\n",
    "            added_folders = f\"/samples/{SPLIT}\" if TRAIN_ATTR else \"\" # Different folder structure\n",
    "            att_path = Path(results_path / f\"{i}{condition}{added_folders}/attributions/gene_expression/{i}{condition}.npy\")\n",
    "            if os.path.exists(att_path):\n",
    "                attributions = np.load(att_path)\n",
    "                #if (i == -73) and (condition == \"_ctrl\"):\n",
    "                fig, axs = plt.subplots(4, figsize=(8,2.7)) \n",
    "                for j,key in enumerate(track_dict.keys()):\n",
    "                    name = track_dict[key][\"name\"]\n",
    "                    axs[j].plot(x_val, abs(attributions[j]), label=name, lw=0, marker='o', markersize=0.8, markeredgecolor=\"none\", color=track_dict[key][\"color\"])\n",
    "                    axs[j].set_xlim((-5000,5001))\n",
    "                    axs[j].set_ylim((0,np.max(attributions[1])))\n",
    "                    axs[j].set_yticks([])\n",
    "                    axs[-1].set_yticks([0,np.max(attributions[1])],[0,1])\n",
    "                fig.savefig(figure_path / f\"Chromatin_landscape_attributions_{i}_ctrl.png\",dpi=200)\n",
    "                plt.close(fig)\n",
    "\n",
    "                # Add a small phase to remove high frequency fluctuations:\n",
    "                eps = 10*np.random.rand(1)*np.random.choice([-1,1])\n",
    "                \n",
    "                centered_attr = np.roll(attributions, -10*i+int(eps),axis=1)\n",
    "                landscape_avg[condition] += 1/(2*N_BINS + 1)*(abs(centered_attr))\n",
    "\n",
    "    SCALING_FACTOR = np.max([landscape_avg[cond] for cond in condition_list])\n",
    "\n",
    "    for condition in condition_list:\n",
    "        fig, ax = plt.subplots(figsize=(8,2.7))   \n",
    "        sub_axes = plt.axes([.615, .57, .27, .27])  \n",
    "        for i,key in enumerate(track_dict.keys()):\n",
    "            name = track_dict[key][\"name\"]\n",
    "            ax.plot(x_val, landscape_avg[condition][i]/SCALING_FACTOR, label=name, lw=0, marker='o', markersize=2, markeredgecolor=\"none\", color=track_dict[key][\"color\"])\n",
    "            sub_axes.plot(x_val, landscape_avg[condition][i]/SCALING_FACTOR, label=name, lw=0,marker='o',markersize=.5,markeredgecolor=\"none\", color=track_dict[key][\"color\"])\n",
    "        \n",
    "        #sub_axes.set_xlim(xlim)\n",
    "        sub_axes.set_xticks(np.arange(-5000,5001,5000),np.arange(-500,501,500))\n",
    "        sub_axes.set_xlim((-5000,5000))\n",
    "        sub_axes.set_yticks([])\n",
    "        ax.set_xlim(xlim)\n",
    "        ax.set_ylim(0,1) #(0,.002))\n",
    "        ax.legend(loc='upper left')\n",
    "        ax.set_ylabel(\"Attribution score\")\n",
    "        ax.set_xlabel(\"Distance to the predicted locus (kbp)\")\n",
    "        plt.subplots_adjust(bottom=0.2)\n",
    "        ax.set_xticks(np.arange(-400,401,100),np.arange(-40,41,10))\n",
    "        fig.savefig(figure_path / f\"Landscape_absolute_average_{condition}.png\", dpi=200)\n",
    "        plt.close(fig)\n",
    "\n",
    "def read_gene_positions(csv_path, resolution=1):\n",
    "    \"\"\" \n",
    "    This function reads a csv containing genes and enhancers (entities), and creates a dictionary with the relative coordinates of\n",
    "    all entities to the TSS of the reference gene (central gene), which gives name to the sample.\n",
    "\n",
    "    Args:\n",
    "        csv_path: path to the csv with ref genes, entities and their relative coordinates.\n",
    "        resolution: distance unit we want our relative distances to be given with (1-> bp, 1000->kbp)\n",
    "\n",
    "    Returns:\n",
    "        pos_dict: dictionary given pos_dict[ref gene][entity_name] = (rel_start, rel_end)\n",
    "    \"\"\"\n",
    "    # Load the results from a CSV file\n",
    "    df = pd.read_csv(csv_path, sep=\"\\t\")\n",
    "    \n",
    "    # Initialize the dictionary to store positions\n",
    "    pos_dict = {}\n",
    "    \n",
    "    # Filter the DataFrame to get only gene entities \n",
    "    gene_entities = df[df['Entity_ID'].str.startswith('ENSMUSG')]  # Adjust the condition according to your data\n",
    "    \n",
    "    for _, row in gene_entities.iterrows():\n",
    "        ref_gene_id = row['Ref_gene']\n",
    "        entity_id = row['Entity_ID']\n",
    "        rel_start = int(round(row['Entity_rel_Start'] / resolution))\n",
    "        rel_end = int(round(row['Entity_rel_End'] / resolution))\n",
    "        \n",
    "        # Ensure the dictionary has the necessary structure\n",
    "        if ref_gene_id+'_forward' not in pos_dict:\n",
    "            pos_dict[ref_gene_id+'_forward'] = {}\n",
    "        \n",
    "        # Store the scaled and rounded start and end positions\n",
    "        pos_dict[ref_gene_id+'_forward'][entity_id+'_forward'] = [rel_start, rel_end]\n",
    "    \n",
    "    return pos_dict\n",
    "\n",
    "def calculate_area_gene(line_p, line_a, ID, pos_dict, N_BINS):\n",
    "    \"\"\" \n",
    "    Args:\n",
    "        line_p: vector of predicted values\n",
    "        line_a: vector of actual values\n",
    "        ID: reference gene, i.e. sample name\n",
    "        pos_dict: dictionary with relative gene coordinates\n",
    "        N_BINS: which length do we set as a threshold to integrate (in the new resolution).\n",
    "    \"\"\"\n",
    "    seq_center = int(len(line_p)//2)\n",
    "    rel_start = seq_center + pos_dict[ID][ID][0] if (seq_center + pos_dict[ID][ID][0]) > 0 else 0\n",
    "    rel_end = seq_center + pos_dict[ID][ID][1] if (seq_center + pos_dict[ID][ID][1]) < 2*N_BINS+1 else 2*N_BINS+1\n",
    "\n",
    "    WIDTH = max(1, int(rel_end - rel_start))\n",
    "\n",
    "    # Appending area\n",
    "    if rel_start != rel_end:\n",
    "        pred_area = simpson(line_p[rel_start:rel_end])/WIDTH\n",
    "        actual_area = simpson(line_a[rel_start:rel_end])/WIDTH\n",
    "\n",
    "    else: # Annotation is shorter than bin_width:\n",
    "        pred_area = line_p[seq_center]\n",
    "        actual_area = line_a[seq_center]\n",
    "\n",
    "    return pred_area, actual_area, rel_start, rel_end\n",
    "\n",
    "def calculate_correlations(results_path: Path, gene_pos_path: Path, figure_path: Path, input_table_path: Path,  N_BINS: int = 200, condition_list = [\"_ctrl\"], resolution: int = 1000, is_training: bool=False, batch_num: str = None, CLIP: bool=True ):\n",
    "    \"\"\"\n",
    "    This function returns a set of dictionaries containing the actual EU-seq values and the predicted values. \n",
    "    This is done both on a bin by bin basis and integrated over the target gene length (normalized by gene length).\n",
    "    \"\"\"\n",
    "\n",
    "    # Storing gene position's dict:\n",
    "    pos_dict = read_gene_positions(gene_pos_path, resolution)\n",
    "    ids, predicted, actual = _get_predictions(results_path, N_BINS, condition_list,is_training=is_training,batch_num=batch_num, CLIP=CLIP)\n",
    "    df = pd.read_csv(input_table_path)\n",
    "\n",
    "    # Normalized Area per bin for the target gene (central gene in observation window)\n",
    "    pred_list_A_per_bin_dict = {}\n",
    "    actual_list_A_per_bin_dict = {}\n",
    "\n",
    "    # Predicted and actual EU-seq values in all positions:\n",
    "    pred_list_values_dict = {}\n",
    "    actual_list_values_dict = {}\n",
    "\n",
    "    for condition in condition_list:\n",
    "        pred_list_A_per_bin_dict[condition] = []\n",
    "        actual_list_A_per_bin_dict[condition] = []\n",
    "        pred_list_values_dict[condition] = []\n",
    "        actual_list_values_dict[condition] = []\n",
    "\n",
    "    for ID,line_p,line_a in zip(ids,predicted.values,actual.values):\n",
    "        if (\"_rev\" not in ID): #Only add the forward strand to the analyses\n",
    "            for condition in condition_list:\n",
    "                # Extend binwise prediction and target lists\n",
    "                pred_list_values_dict[condition].extend(list(line_p))\n",
    "                actual_list_values_dict[condition].extend(list(line_a))\n",
    "\n",
    "                # Extend area predictions\n",
    "                pred_area, actual_area, rel_start, rel_end = calculate_area_gene(line_p, line_a, ID, pos_dict, N_BINS)\n",
    "\n",
    "                pred_list_A_per_bin_dict[condition].extend([pred_area])\n",
    "                actual_list_A_per_bin_dict[condition].extend([actual_area])\n",
    "\n",
    "                # Adding area to extended table:\n",
    "                gene = ID.split('_')[0]\n",
    "                df.loc[(df['Ref_gene'] == gene) & (df['Entity_ID'] == gene),'Area'] = pred_area #Edit existing table\n",
    "\n",
    "            PLOT = True\n",
    "            ID_LIST = [\"ENSMUSG00000078673.10_forward\",\"ENSMUSG00000067261.4_forward\",\"ENSMUSG00000028234.6_forward\",\"ENSMUSG00000028280.9_forward\",\"ENSMUSG00000035969.15_forward\"]\n",
    "            if PLOT and (ID in ID_LIST):\n",
    "                fig = plot_target_predictions(line_p, line_a, N_BINS) \n",
    "                fig.savefig(figure_path / f'{ID}.png', dpi=200)\n",
    "\n",
    "    df.to_csv(input_table_path, index=False)\n",
    "\n",
    "    return pred_list_A_per_bin_dict, actual_list_A_per_bin_dict, pred_list_values_dict, actual_list_values_dict\n",
    "\n",
    "def plot_correlations(figure_path: Path, pred_list_values: list, actual_list_values: list, title: str, cmap: colors.Colormap = \"afmhot\", binlims: tuple = (0,50), density: bool =True, DELTA:bool=False, min_val: float = .0, max_val: int = 10):\n",
    "    \"\"\" \n",
    "    This function plots predicted vs. actual values and provides the Spearman and Pearson correlations of the regressions. \n",
    "    The style of the plot changes depending on whether the predictions are at a bin level or a gene level.\n",
    "    \"\"\"\n",
    "    # Get the counts histogram to plot correlation with density colormap\n",
    "    counts, xedges, yedges, _ = plt.hist2d(actual_list_values,pred_list_values, bins=[np.linspace(binlims[0],binlims[1],200),np.linspace(binlims[0],binlims[1],200)], density=False)\n",
    "\n",
    "    # Plot regression of single predicted values vs. real\n",
    "    pearson_r = pearsonr(actual_list_values,pred_list_values)[0]\n",
    "    spearman_r = spearmanr(actual_list_values,pred_list_values)[0]\n",
    "    m, b = np.polyfit(actual_list_values, pred_list_values, deg=1)\n",
    "    # Actual fit:\n",
    "    a1, a2, a3 = np.polyfit(actual_list_values, pred_list_values, deg=2)\n",
    "    # Regression from origin:\n",
    "    #m_0 , _, _, _ = np.linalg.lstsq(np.array(actual_list_values)[:,np.newaxis], pred_list_values) #x needs to be a column vector for this function\n",
    "\n",
    "    # Create the figure\n",
    "    fig, ax = plt.subplots(figsize=(4,4))    \n",
    "    if density:\n",
    "        eps= 1\n",
    "        # Counts matrix is transposed: the origin convention for plt.imshow is (top, left), for plt.hist2D is (bottom,left)\n",
    "        counts[counts < eps] =1\n",
    "        hist = ax.imshow(counts.T, origin='lower', extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]], cmap=cmap,  norm=colors.LogNorm())\n",
    "        _ = fig.colorbar(hist)\n",
    "        _ = ax.axline(xy1=(0, b), slope=m, label=f'Linear fit:\\n$ y = {m:.3f}x {b:+.3f}$\\n$r_p:{pearson_r:.4f}$\\n$r_s:{spearman_r:.4f}$', color=\"royalblue\", ls=\"-\", lw=1)\n",
    "        #_ = ax.plot(np.arange(min_val,max_val,step), a1*(np.arange(min_val,max_val,step)**2)+a2*(np.arange(min_val,max_val,step))+a3, color='royalblue', ls = \"--\", lw=1, label= f\"Polynomial fit:\\n$y={a1:.3f}x^2+{a2:.3f}x+{a3:.3f}$\")\n",
    "        _ = ax.axis(\"scaled\")\n",
    "        _ = ax.set_xlim(min_val,max_val)\n",
    "        _ = ax.set_ylim(min_val,max_val)\n",
    "        _ = ax.set_xlabel(\"log$_2$(True +1)\")\n",
    "        _ = ax.set_ylabel(\"log$_2$(Predicted +1)\")\n",
    "        fig.legend(bbox_to_anchor=(.48,.8), fancybox=True, shadow=False, fontsize=7)\n",
    "    else:\n",
    "        _ = ax.plot(actual_list_values,pred_list_values, lw=0, marker='o',  markersize=1, markeredgewidth=0, color='k')\n",
    "        _ = ax.axis(\"scaled\")\n",
    "        _ = ax.axline(xy1=(0, b), slope=m, label=f'Linear fit:\\n$ y = {m:.3f}x {b:+.3f}$\\n$r_p:{pearson_r:.4f}$\\n$r_s:{spearman_r:.4f}$', color=\"royalblue\", ls=\"-\", lw=1)\n",
    "        #ax.plot(np.arange(-20,200,1), a1*(np.arange(-20,200,1)**2)+a2*(np.arange(-20,200,1))+a3, color='royalblue', ls = \"--\", lw=1, label= f\"Polynomial fit:\\n$y={a1:.3f}x^2+{a2:.3f}x{a3:.3f}$\")\n",
    "        if DELTA:\n",
    "            _ = ax.set_xlim(-20,20)\n",
    "            _ = ax.set_ylim(-20, 20)\n",
    "            _ = ax.set_xlabel(\"Actual $\\Delta_{exp}$  (reads/kbp)\")\n",
    "            _ = ax.set_ylabel(\"Predicted $\\Delta_{exp}$ (reads/kbp)\")\n",
    "            fig.legend(bbox_to_anchor=(.53,.87), fancybox=True, shadow=True, fontsize=8)\n",
    "        else:\n",
    "            _ = ax.set_xlim(0,200)\n",
    "            _ = ax.set_ylim(0, 200)\n",
    "            _ = ax.set_xlabel(\"Actual averaged expression (reads/kbp)\")\n",
    "            _ = ax.set_ylabel(\"Predicted averaged expression (reads/kbp)\")\n",
    "            fig.legend(bbox_to_anchor=(.53,.87), fancybox=True, shadow=True, fontsize=8)\n",
    "\n",
    "    #ax.plot(np.arange(0,500,1), np.arange(0,500,1), color='k', ls = \"dashed\", lw=1)\n",
    "\n",
    "    fig.savefig(figure_path / f\"Regression_histogram_{title}.png\", dpi=200, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "def r2_score_computed(targets, outputs):\n",
    "    \"\"\"\n",
    "    Function to compute R2 coefficient between predictions and targets.\n",
    "    This allows for all operations to happen in the GPU )if available)\n",
    "    without moving back and forth to the CPU.\n",
    "    \"\"\"\n",
    "    target_mean = np.mean(targets)\n",
    "    ss_tot = np.sum((targets - target_mean) ** 2)\n",
    "    ss_res = np.sum((targets - outputs) ** 2)\n",
    "    r2 = 1 - ss_res / ss_tot\n",
    "    return r2\n",
    "\n",
    "def download_files(links, destination_folder):\n",
    "    \"\"\"\n",
    "    This function downloads the files in links and stores them in destination_folder.\n",
    "    Args:\n",
    "        links: dictionary where keys are file names and values are links.\n",
    "        destination_folder: path where we'll store the files.\n",
    "    \"\"\"\n",
    "    for name,link in links.items():\n",
    "        try:\n",
    "            # Ensure the destination folder exists\n",
    "            if not os.path.exists(destination_folder):\n",
    "                os.makedirs(destination_folder)\n",
    "\n",
    "            # Specify the output path for the downloaded file\n",
    "            output_path = os.path.join(destination_folder, name)\n",
    "\n",
    "            # Download the file to the specified destination folder\n",
    "            wget.download(link, out=output_path)\n",
    "            print(f\"\\nSuccessfully downloaded {name} to {destination_folder}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nFailed to download {link} because : {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II) Creation of EIR config files to define CLASTER\n",
    "\n",
    "This cell writes all config files at once, easing reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_paths = [Path(\"../configurations/conf_pure_conv_predict_prom/\"), #0\n",
    "                Path(\"../configurations/conf_pure_conv_20kbp_context/\"), #1\n",
    "                Path(\"../configurations/conf_pure_conv_softplus_no_bias/\"), #2\n",
    "                Path(\"../configurations/conf_pure_conv_softplus_poisson/\"),#3\n",
    "                Path(\"../configurations/conf_pure_conv_20kbp_context_test/\"),#4\n",
    "                Path(\"../configurations/conf_pure_conv_chr19_holdout_train/\"),#5\n",
    "                Path(\"../configurations/conf_pure_conv_chr19_holdout_test/\"),#6\n",
    "                Path(\"../configurations/conf_pure_conv_no_H3K27ac_train/\"),#7\n",
    "                Path(\"../configurations/conf_pure_conv_no_H3K27ac_test/\"),#8\n",
    "                Path(\"../configurations/conf_pure_conv_20kbp_context_test/\"),#9\n",
    "                Path(\"../configurations/conf_pure_conv_1kbp_binning_train/\"),#10\n",
    "                Path(\"../configurations/conf_pure_conv_1kbp_binning_test/\"),#11\n",
    "                Path(\"../configurations/conf_pure_conv_chr19_enhancer_centric/\"),#12\n",
    "                Path(\"../configurations/conf_pure_conv_K562_train/\"),#13\n",
    "                Path(\"../configurations/conf_pure_conv_K562_test/\"),#14\n",
    "                Path(\"../configurations/conf_pure_conv_prom_CHiC_train/\"), # 15\n",
    "                Path(\"../configurations/conf_pure_conv_prom_CHiC_test/\"), # 16\n",
    "                Path(\"../configurations/conf_pure_conv_K562_POLR2A_train/\"),#17\n",
    "                Path(\"../configurations/conf_pure_conv_K562_POLR2A_enhancer_centric/\"),#18\n",
    "                Path(\"../configurations/conf_pure_conv_K562_RNA_enhancer_centric/\"),#19\n",
    "                ]\n",
    "               \n",
    "for config_path in config_paths:\n",
    "  config_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "predict_prom_yaml_contents = {\"fusion.yaml\":\"\"\"  \n",
    "model_config:\n",
    "  fc_do: 0.4\n",
    "  fc_task_dim: 256\n",
    "  layers:\n",
    "  - 2\n",
    "  rb_do: 0.4\n",
    "  stochastic_depth_p: 0.5\n",
    "model_type: \"mlp-residual\"\n",
    "\"\"\",\n",
    "\"globals.yaml\": \"\"\"\n",
    "checkpoint_interval: 30300 # 100 epochs #60000\n",
    "sample_interval: 30300 #60000\n",
    "n_epochs: 120\n",
    "batch_size: 64\n",
    "optimizer: \"adamw\"\n",
    "lr: 0.0001 #0.0001\n",
    "device: \"cuda\"\n",
    "compute_attributions: false\n",
    "\n",
    "\"\"\",\n",
    "\"input_cnn.yaml\": \"\"\"\n",
    "input_info:\n",
    "  input_source: ./inputs/perturbed_landscape_arrays/inserted_promoter_arrays/  #inserted_enhancer_test #inputs/silenced_arrays/silenced_arrays_H2B_S.D \n",
    "  #./data/parsed_data/inputs/arrays_train_100bp_no_H3K27ac/ #arrays_train_100bp_no_H3K27ac_uncoupled/\n",
    "  input_name: gene_expression\n",
    "  input_type: array\n",
    "\n",
    "model_config:\n",
    "  model_type: cnn\n",
    "  #pre_normalization: \"instancenorm\"\n",
    "  model_init_config:\n",
    "    num_output_features: 512 # before fc_repr_dim\n",
    "    layers: [4,4]\n",
    "    kernel_height: 1\n",
    "    down_stride_width: 2\n",
    "    first_stride_expansion_width: 1\n",
    "    first_kernel_expansion_height: 4 #5\n",
    "    kernel_width: 10  #10\n",
    "    dilation_factor_width: 2\n",
    "    dilation_factor_height: 1\n",
    "    channel_exp_base: 5  #3 #-1\n",
    "    first_channel_expansion: 2\n",
    "    rb_do: .3\n",
    "    stochastic_depth_p: .1\n",
    "    attention_inclusion_cutoff: 1 #50 #256\n",
    "    \"\"\",\n",
    "\"outputs_2_cond.yaml\": \"\"\"\n",
    "output_info:\n",
    "  output_name: expression_output\n",
    "  output_source: ./targets/prom_perturbed_targets.csv \n",
    "  #./data/parsed_data/targets/target_arrays_perturbational_inserted_enhancer.csv #target_arrays_perturbational_S.D.csv #target_arrays_1kbp_401_bins_2_conditions_decareads_abs.csv\n",
    "  output_type: tabular\n",
    "\n",
    "model_config: # <- new\n",
    "  model_type: linear # <- new\n",
    "\n",
    "output_type_info:\n",
    "  con_loss_name: \"SmoothL1Loss\"\n",
    "  target_con_columns:\n",
    "  - \"-200_ctrl\"\n",
    "  - \"-199_ctrl\"\n",
    "  - \"-198_ctrl\"\n",
    "  - \"-197_ctrl\"\n",
    "  - \"-196_ctrl\"\n",
    "  - \"-195_ctrl\"\n",
    "  - \"-194_ctrl\"\n",
    "  - \"-193_ctrl\"\n",
    "  - \"-192_ctrl\"\n",
    "  - \"-191_ctrl\"\n",
    "  - \"-190_ctrl\"\n",
    "  - \"-189_ctrl\"\n",
    "  - \"-188_ctrl\"\n",
    "  - \"-187_ctrl\"\n",
    "  - \"-186_ctrl\"\n",
    "  - \"-185_ctrl\"\n",
    "  - \"-184_ctrl\"\n",
    "  - \"-183_ctrl\"\n",
    "  - \"-182_ctrl\"\n",
    "  - \"-181_ctrl\"\n",
    "  - \"-180_ctrl\"\n",
    "  - \"-179_ctrl\"\n",
    "  - \"-178_ctrl\"\n",
    "  - \"-177_ctrl\"\n",
    "  - \"-176_ctrl\"\n",
    "  - \"-175_ctrl\"\n",
    "  - \"-174_ctrl\"\n",
    "  - \"-173_ctrl\"\n",
    "  - \"-172_ctrl\"\n",
    "  - \"-171_ctrl\"\n",
    "  - \"-170_ctrl\"\n",
    "  - \"-169_ctrl\"\n",
    "  - \"-168_ctrl\"\n",
    "  - \"-167_ctrl\"\n",
    "  - \"-166_ctrl\"\n",
    "  - \"-165_ctrl\"\n",
    "  - \"-164_ctrl\"\n",
    "  - \"-163_ctrl\"\n",
    "  - \"-162_ctrl\"\n",
    "  - \"-161_ctrl\"\n",
    "  - \"-160_ctrl\"\n",
    "  - \"-159_ctrl\"\n",
    "  - \"-158_ctrl\"\n",
    "  - \"-157_ctrl\"\n",
    "  - \"-156_ctrl\"\n",
    "  - \"-155_ctrl\"\n",
    "  - \"-154_ctrl\"\n",
    "  - \"-153_ctrl\"\n",
    "  - \"-152_ctrl\"\n",
    "  - \"-151_ctrl\"\n",
    "  - \"-150_ctrl\"\n",
    "  - \"-149_ctrl\"\n",
    "  - \"-148_ctrl\"\n",
    "  - \"-147_ctrl\"\n",
    "  - \"-146_ctrl\"\n",
    "  - \"-145_ctrl\"\n",
    "  - \"-144_ctrl\"\n",
    "  - \"-143_ctrl\"\n",
    "  - \"-142_ctrl\"\n",
    "  - \"-141_ctrl\"\n",
    "  - \"-140_ctrl\"\n",
    "  - \"-139_ctrl\"\n",
    "  - \"-138_ctrl\"\n",
    "  - \"-137_ctrl\"\n",
    "  - \"-136_ctrl\"\n",
    "  - \"-135_ctrl\"\n",
    "  - \"-134_ctrl\"\n",
    "  - \"-133_ctrl\"\n",
    "  - \"-132_ctrl\"\n",
    "  - \"-131_ctrl\"\n",
    "  - \"-130_ctrl\"\n",
    "  - \"-129_ctrl\"\n",
    "  - \"-128_ctrl\"\n",
    "  - \"-127_ctrl\"\n",
    "  - \"-126_ctrl\"\n",
    "  - \"-125_ctrl\"\n",
    "  - \"-124_ctrl\"\n",
    "  - \"-123_ctrl\"\n",
    "  - \"-122_ctrl\"\n",
    "  - \"-121_ctrl\"\n",
    "  - \"-120_ctrl\"\n",
    "  - \"-119_ctrl\"\n",
    "  - \"-118_ctrl\"\n",
    "  - \"-117_ctrl\"\n",
    "  - \"-116_ctrl\"\n",
    "  - \"-115_ctrl\"\n",
    "  - \"-114_ctrl\"\n",
    "  - \"-113_ctrl\"\n",
    "  - \"-112_ctrl\"\n",
    "  - \"-111_ctrl\"\n",
    "  - \"-110_ctrl\"\n",
    "  - \"-109_ctrl\"\n",
    "  - \"-108_ctrl\"\n",
    "  - \"-107_ctrl\"\n",
    "  - \"-106_ctrl\"\n",
    "  - \"-105_ctrl\"\n",
    "  - \"-104_ctrl\"\n",
    "  - \"-103_ctrl\"\n",
    "  - \"-102_ctrl\"\n",
    "  - \"-101_ctrl\"\n",
    "  - \"-100_ctrl\"\n",
    "  - \"-99_ctrl\"\n",
    "  - \"-98_ctrl\"\n",
    "  - \"-97_ctrl\"\n",
    "  - \"-96_ctrl\"\n",
    "  - \"-95_ctrl\"\n",
    "  - \"-94_ctrl\"\n",
    "  - \"-93_ctrl\"\n",
    "  - \"-92_ctrl\"\n",
    "  - \"-91_ctrl\"\n",
    "  - \"-90_ctrl\"\n",
    "  - \"-89_ctrl\"\n",
    "  - \"-88_ctrl\"\n",
    "  - \"-87_ctrl\"\n",
    "  - \"-86_ctrl\"\n",
    "  - \"-85_ctrl\"\n",
    "  - \"-84_ctrl\"\n",
    "  - \"-83_ctrl\"\n",
    "  - \"-82_ctrl\"\n",
    "  - \"-81_ctrl\"\n",
    "  - \"-80_ctrl\"\n",
    "  - \"-79_ctrl\"\n",
    "  - \"-78_ctrl\"\n",
    "  - \"-77_ctrl\"\n",
    "  - \"-76_ctrl\"\n",
    "  - \"-75_ctrl\"\n",
    "  - \"-74_ctrl\"\n",
    "  - \"-73_ctrl\"\n",
    "  - \"-72_ctrl\"\n",
    "  - \"-71_ctrl\"\n",
    "  - \"-70_ctrl\"\n",
    "  - \"-69_ctrl\"\n",
    "  - \"-68_ctrl\"\n",
    "  - \"-67_ctrl\"\n",
    "  - \"-66_ctrl\"\n",
    "  - \"-65_ctrl\"\n",
    "  - \"-64_ctrl\"\n",
    "  - \"-63_ctrl\"\n",
    "  - \"-62_ctrl\"\n",
    "  - \"-61_ctrl\"\n",
    "  - \"-60_ctrl\"\n",
    "  - \"-59_ctrl\"\n",
    "  - \"-58_ctrl\"\n",
    "  - \"-57_ctrl\"\n",
    "  - \"-56_ctrl\"\n",
    "  - \"-55_ctrl\"\n",
    "  - \"-54_ctrl\"\n",
    "  - \"-53_ctrl\"\n",
    "  - \"-52_ctrl\"\n",
    "  - \"-51_ctrl\"\n",
    "  - \"-50_ctrl\"\n",
    "  - \"-49_ctrl\"\n",
    "  - \"-48_ctrl\"\n",
    "  - \"-47_ctrl\"\n",
    "  - \"-46_ctrl\"\n",
    "  - \"-45_ctrl\"\n",
    "  - \"-44_ctrl\"\n",
    "  - \"-43_ctrl\"\n",
    "  - \"-42_ctrl\"\n",
    "  - \"-41_ctrl\"\n",
    "  - \"-40_ctrl\"\n",
    "  - \"-39_ctrl\"\n",
    "  - \"-38_ctrl\"\n",
    "  - \"-37_ctrl\"\n",
    "  - \"-36_ctrl\"\n",
    "  - \"-35_ctrl\"\n",
    "  - \"-34_ctrl\"\n",
    "  - \"-33_ctrl\"\n",
    "  - \"-32_ctrl\"\n",
    "  - \"-31_ctrl\"\n",
    "  - \"-30_ctrl\"\n",
    "  - \"-29_ctrl\"\n",
    "  - \"-28_ctrl\"\n",
    "  - \"-27_ctrl\"\n",
    "  - \"-26_ctrl\"\n",
    "  - \"-25_ctrl\"\n",
    "  - \"-24_ctrl\"\n",
    "  - \"-23_ctrl\"\n",
    "  - \"-22_ctrl\"\n",
    "  - \"-21_ctrl\"\n",
    "  - \"-20_ctrl\"\n",
    "  - \"-19_ctrl\"\n",
    "  - \"-18_ctrl\"\n",
    "  - \"-17_ctrl\"\n",
    "  - \"-16_ctrl\"\n",
    "  - \"-15_ctrl\"\n",
    "  - \"-14_ctrl\"\n",
    "  - \"-13_ctrl\"\n",
    "  - \"-12_ctrl\"\n",
    "  - \"-11_ctrl\"\n",
    "  - \"-10_ctrl\"\n",
    "  - \"-9_ctrl\"\n",
    "  - \"-8_ctrl\"\n",
    "  - \"-7_ctrl\"\n",
    "  - \"-6_ctrl\"\n",
    "  - \"-5_ctrl\"\n",
    "  - \"-4_ctrl\"\n",
    "  - \"-3_ctrl\"\n",
    "  - \"-2_ctrl\"\n",
    "  - \"-1_ctrl\"\n",
    "  - \"0_ctrl\"\n",
    "  - \"1_ctrl\"\n",
    "  - \"2_ctrl\"\n",
    "  - \"3_ctrl\"\n",
    "  - \"4_ctrl\"\n",
    "  - \"5_ctrl\"\n",
    "  - \"6_ctrl\"\n",
    "  - \"7_ctrl\"\n",
    "  - \"8_ctrl\"\n",
    "  - \"9_ctrl\"\n",
    "  - \"10_ctrl\"\n",
    "  - \"11_ctrl\"\n",
    "  - \"12_ctrl\"\n",
    "  - \"13_ctrl\"\n",
    "  - \"14_ctrl\"\n",
    "  - \"15_ctrl\"\n",
    "  - \"16_ctrl\"\n",
    "  - \"17_ctrl\"\n",
    "  - \"18_ctrl\"\n",
    "  - \"19_ctrl\"\n",
    "  - \"20_ctrl\"\n",
    "  - \"21_ctrl\"\n",
    "  - \"22_ctrl\"\n",
    "  - \"23_ctrl\"\n",
    "  - \"24_ctrl\"\n",
    "  - \"25_ctrl\"\n",
    "  - \"26_ctrl\"\n",
    "  - \"27_ctrl\"\n",
    "  - \"28_ctrl\"\n",
    "  - \"29_ctrl\"\n",
    "  - \"30_ctrl\"\n",
    "  - \"31_ctrl\"\n",
    "  - \"32_ctrl\"\n",
    "  - \"33_ctrl\"\n",
    "  - \"34_ctrl\"\n",
    "  - \"35_ctrl\"\n",
    "  - \"36_ctrl\"\n",
    "  - \"37_ctrl\"\n",
    "  - \"38_ctrl\"\n",
    "  - \"39_ctrl\"\n",
    "  - \"40_ctrl\"\n",
    "  - \"41_ctrl\"\n",
    "  - \"42_ctrl\"\n",
    "  - \"43_ctrl\"\n",
    "  - \"44_ctrl\"\n",
    "  - \"45_ctrl\"\n",
    "  - \"46_ctrl\"\n",
    "  - \"47_ctrl\"\n",
    "  - \"48_ctrl\"\n",
    "  - \"49_ctrl\"\n",
    "  - \"50_ctrl\"\n",
    "  - \"51_ctrl\"\n",
    "  - \"52_ctrl\"\n",
    "  - \"53_ctrl\"\n",
    "  - \"54_ctrl\"\n",
    "  - \"55_ctrl\"\n",
    "  - \"56_ctrl\"\n",
    "  - \"57_ctrl\"\n",
    "  - \"58_ctrl\"\n",
    "  - \"59_ctrl\"\n",
    "  - \"60_ctrl\"\n",
    "  - \"61_ctrl\"\n",
    "  - \"62_ctrl\"\n",
    "  - \"63_ctrl\"\n",
    "  - \"64_ctrl\"\n",
    "  - \"65_ctrl\"\n",
    "  - \"66_ctrl\"\n",
    "  - \"67_ctrl\"\n",
    "  - \"68_ctrl\"\n",
    "  - \"69_ctrl\"\n",
    "  - \"70_ctrl\"\n",
    "  - \"71_ctrl\"\n",
    "  - \"72_ctrl\"\n",
    "  - \"73_ctrl\"\n",
    "  - \"74_ctrl\"\n",
    "  - \"75_ctrl\"\n",
    "  - \"76_ctrl\"\n",
    "  - \"77_ctrl\"\n",
    "  - \"78_ctrl\"\n",
    "  - \"79_ctrl\"\n",
    "  - \"80_ctrl\"\n",
    "  - \"81_ctrl\"\n",
    "  - \"82_ctrl\"\n",
    "  - \"83_ctrl\"\n",
    "  - \"84_ctrl\"\n",
    "  - \"85_ctrl\"\n",
    "  - \"86_ctrl\"\n",
    "  - \"87_ctrl\"\n",
    "  - \"88_ctrl\"\n",
    "  - \"89_ctrl\"\n",
    "  - \"90_ctrl\"\n",
    "  - \"91_ctrl\"\n",
    "  - \"92_ctrl\"\n",
    "  - \"93_ctrl\"\n",
    "  - \"94_ctrl\"\n",
    "  - \"95_ctrl\"\n",
    "  - \"96_ctrl\"\n",
    "  - \"97_ctrl\"\n",
    "  - \"98_ctrl\"\n",
    "  - \"99_ctrl\"\n",
    "  - \"100_ctrl\"\n",
    "  - \"101_ctrl\"\n",
    "  - \"102_ctrl\"\n",
    "  - \"103_ctrl\"\n",
    "  - \"104_ctrl\"\n",
    "  - \"105_ctrl\"\n",
    "  - \"106_ctrl\"\n",
    "  - \"107_ctrl\"\n",
    "  - \"108_ctrl\"\n",
    "  - \"109_ctrl\"\n",
    "  - \"110_ctrl\"\n",
    "  - \"111_ctrl\"\n",
    "  - \"112_ctrl\"\n",
    "  - \"113_ctrl\"\n",
    "  - \"114_ctrl\"\n",
    "  - \"115_ctrl\"\n",
    "  - \"116_ctrl\"\n",
    "  - \"117_ctrl\"\n",
    "  - \"118_ctrl\"\n",
    "  - \"119_ctrl\"\n",
    "  - \"120_ctrl\"\n",
    "  - \"121_ctrl\"\n",
    "  - \"122_ctrl\"\n",
    "  - \"123_ctrl\"\n",
    "  - \"124_ctrl\"\n",
    "  - \"125_ctrl\"\n",
    "  - \"126_ctrl\"\n",
    "  - \"127_ctrl\"\n",
    "  - \"128_ctrl\"\n",
    "  - \"129_ctrl\"\n",
    "  - \"130_ctrl\"\n",
    "  - \"131_ctrl\"\n",
    "  - \"132_ctrl\"\n",
    "  - \"133_ctrl\"\n",
    "  - \"134_ctrl\"\n",
    "  - \"135_ctrl\"\n",
    "  - \"136_ctrl\"\n",
    "  - \"137_ctrl\"\n",
    "  - \"138_ctrl\"\n",
    "  - \"139_ctrl\"\n",
    "  - \"140_ctrl\"\n",
    "  - \"141_ctrl\"\n",
    "  - \"142_ctrl\"\n",
    "  - \"143_ctrl\"\n",
    "  - \"144_ctrl\"\n",
    "  - \"145_ctrl\"\n",
    "  - \"146_ctrl\"\n",
    "  - \"147_ctrl\"\n",
    "  - \"148_ctrl\"\n",
    "  - \"149_ctrl\"\n",
    "  - \"150_ctrl\"\n",
    "  - \"151_ctrl\"\n",
    "  - \"152_ctrl\"\n",
    "  - \"153_ctrl\"\n",
    "  - \"154_ctrl\"\n",
    "  - \"155_ctrl\"\n",
    "  - \"156_ctrl\"\n",
    "  - \"157_ctrl\"\n",
    "  - \"158_ctrl\"\n",
    "  - \"159_ctrl\"\n",
    "  - \"160_ctrl\"\n",
    "  - \"161_ctrl\"\n",
    "  - \"162_ctrl\"\n",
    "  - \"163_ctrl\"\n",
    "  - \"164_ctrl\"\n",
    "  - \"165_ctrl\"\n",
    "  - \"166_ctrl\"\n",
    "  - \"167_ctrl\"\n",
    "  - \"168_ctrl\"\n",
    "  - \"169_ctrl\"\n",
    "  - \"170_ctrl\"\n",
    "  - \"171_ctrl\"\n",
    "  - \"172_ctrl\"\n",
    "  - \"173_ctrl\"\n",
    "  - \"174_ctrl\"\n",
    "  - \"175_ctrl\"\n",
    "  - \"176_ctrl\"\n",
    "  - \"177_ctrl\"\n",
    "  - \"178_ctrl\"\n",
    "  - \"179_ctrl\"\n",
    "  - \"180_ctrl\"\n",
    "  - \"181_ctrl\"\n",
    "  - \"182_ctrl\"\n",
    "  - \"183_ctrl\"\n",
    "  - \"184_ctrl\"\n",
    "  - \"185_ctrl\"\n",
    "  - \"186_ctrl\"\n",
    "  - \"187_ctrl\"\n",
    "  - \"188_ctrl\"\n",
    "  - \"189_ctrl\"\n",
    "  - \"190_ctrl\"\n",
    "  - \"191_ctrl\"\n",
    "  - \"192_ctrl\"\n",
    "  - \"193_ctrl\"\n",
    "  - \"194_ctrl\"\n",
    "  - \"195_ctrl\"\n",
    "  - \"196_ctrl\"\n",
    "  - \"197_ctrl\"\n",
    "  - \"198_ctrl\"\n",
    "  - \"199_ctrl\"\n",
    "  - \"200_ctrl\"\n",
    "  \"\"\" }\n",
    "\n",
    "training_20kbp_yaml_contents = {\"globals.yaml\":\"\"\"\n",
    "output_folder: ./runs/gene_expression_only_chrom_pure_conv_20kbp_context/  #gene_expression_prediction_no_H3K27ac_uncoupled/     #gene_expression_cnn_1kbp_401_bins_2_cond_reloaded/ \n",
    "manual_valid_ids_file: ./annotations/manual_validation_ids_chr17.txt  #manual_validation_ids_chr17_uncoupled.txt\n",
    "checkpoint_interval: 30300 # 100 epochs is 30300 #60000\n",
    "sample_interval: 30300 #60000\n",
    "n_epochs: 120\n",
    "batch_size: 64\n",
    "optimizer: \"adamw\"\n",
    "lr: 0.0001 #0.0001\n",
    "device: \"cuda\"\n",
    "compute_attributions: true\n",
    "\"\"\",\n",
    "\"input_cnn.yaml\": \"\"\" \n",
    "input_info:\n",
    "  input_source: ./inputs/landscape_arrays/training_20kbp_context/ \n",
    "  input_name: gene_expression\n",
    "  input_type: array\n",
    "\n",
    "model_config:\n",
    "  model_type: cnn\n",
    "  #pre_normalization: \"instancenorm\"\n",
    "  model_init_config:\n",
    "    num_output_features: 512 # before fc_repr_dim\n",
    "    layers: [1,1]\n",
    "    kernel_height: 1\n",
    "    down_stride_width: 2\n",
    "    first_stride_expansion_width: 1\n",
    "    first_kernel_expansion_height: 4 #5\n",
    "    kernel_width: 10  #10\n",
    "    dilation_factor_width: 2\n",
    "    dilation_factor_height: 1\n",
    "    channel_exp_base: 5  #3 #-1\n",
    "    first_channel_expansion: 2\n",
    "    rb_do: .3\n",
    "    stochastic_depth_p: .1\n",
    "    attention_inclusion_cutoff: 1 #50 #256\n",
    "\n",
    "\"\"\",\n",
    "\"fusion.yaml\": \"\"\"\n",
    "model_config:\n",
    "  fc_do: 0.4\n",
    "  fc_task_dim: 256\n",
    "  layers:\n",
    "  - 1\n",
    "  rb_do: 0.4\n",
    "  stochastic_depth_p: 0.5\n",
    "model_type: \"mlp-residual\"\n",
    "\"\"\",\n",
    "\"outputs_2_cond.yaml\":\"\"\"\n",
    "output_info:\n",
    "  output_name: expression_output\n",
    "  output_source: ./targets/training_targets.csv \n",
    "  output_type: tabular\n",
    "\n",
    "model_config: # <- new\n",
    "  model_type: linear # <- new\n",
    "\n",
    "output_type_info:\n",
    "  con_loss_name: \"SmoothL1Loss\"\n",
    "  target_con_columns:\n",
    "  - \"-9_ctrl\"\n",
    "  - \"-8_ctrl\"\n",
    "  - \"-7_ctrl\"\n",
    "  - \"-6_ctrl\"\n",
    "  - \"-5_ctrl\"\n",
    "  - \"-4_ctrl\"\n",
    "  - \"-3_ctrl\"\n",
    "  - \"-2_ctrl\"\n",
    "  - \"-1_ctrl\"\n",
    "  - \"0_ctrl\"\n",
    "  - \"1_ctrl\"\n",
    "  - \"2_ctrl\"\n",
    "  - \"3_ctrl\"\n",
    "  - \"4_ctrl\"\n",
    "  - \"5_ctrl\"\n",
    "  - \"6_ctrl\"\n",
    "  - \"7_ctrl\"\n",
    "  - \"8_ctrl\"\n",
    "  - \"9_ctrl\"\n",
    "  \"\"\"\n",
    "}\n",
    "\n",
    "training_softplus_contents = {\"globals.yaml\":\"\"\"\n",
    "output_folder: ./runs/gene_expression_only_chrom_pure_conv_softplus_no_bias/ \n",
    "manual_valid_ids_file: ./annotations/manual_validation_ids_chr17.txt  #manual_validation_ids_chr17_uncoupled.txt\n",
    "checkpoint_interval: 30300 # 100 epochs #60000\n",
    "sample_interval: 30300 #60000\n",
    "n_epochs: 120\n",
    "batch_size: 64\n",
    "optimizer: \"adamw\"\n",
    "lr: 0.0001 #0.0001\n",
    "device: \"cuda\"\n",
    "compute_attributions: false\n",
    "# latent_sampling: \n",
    "#   layers_to_sample:\n",
    "#     - \"input_modules.contact_maps.feature_extractor.conv.0.conv_1\"\n",
    "#     - \"input_modules.contact_maps.feature_extractor.conv.1.conv_2\"\n",
    "#     - \"input_modules.contact_maps.feature_extractor.conv.2.conv_2\"\n",
    "#     - \"input_modules.contact_maps.feature_extractor.conv.3.conv_2\"\n",
    "# attribution_background_samples: 512\n",
    "# attributions_every_sample_factor: 1\n",
    "#pretrained_checkpoint: best_models/gene_expression_exformer_unlimited_chrom_and_micro_with_attention_model_117600_perf-average=0.8435.pt \n",
    "\n",
    "\"\"\",\n",
    "\"input_cnn.yaml\": \"\"\" \n",
    "input_info:\n",
    "  input_source: ./inputs/landscape_arrays/training/ \n",
    "  input_name: gene_expression\n",
    "  input_type: array\n",
    "\n",
    "model_config:\n",
    "  model_type: cnn\n",
    "  #pre_normalization: \"instancenorm\"\n",
    "  model_init_config:\n",
    "    num_output_features: 512 # before fc_repr_dim\n",
    "    layers: [4,4]\n",
    "    kernel_height: 1\n",
    "    down_stride_width: 2\n",
    "    first_stride_expansion_width: 1\n",
    "    first_kernel_expansion_height: 4 #5\n",
    "    kernel_width: 10  #10\n",
    "    dilation_factor_width: 2\n",
    "    dilation_factor_height: 1\n",
    "    channel_exp_base: 5  #3 #-1\n",
    "    first_channel_expansion: 2\n",
    "    rb_do: .3\n",
    "    stochastic_depth_p: .1\n",
    "    attention_inclusion_cutoff: 1 #50 #256\n",
    "\n",
    "\"\"\",\n",
    "\"fusion.yaml\": \"\"\"\n",
    "model_config:\n",
    "  fc_do: 0.4\n",
    "  fc_task_dim: 256\n",
    "  layers:\n",
    "  - 2\n",
    "  rb_do: 0.4\n",
    "  stochastic_depth_p: 0.5\n",
    "model_type: \"mlp-residual\"\n",
    "\"\"\",\n",
    "\"outputs_2_cond.yaml\":\"\"\"\n",
    "output_info:\n",
    "  output_name: expression_output\n",
    "  output_source: ./targets/training_targets.csv \n",
    "  output_type: tabular\n",
    "\n",
    "model_config: # <- new\n",
    "  model_type: linear # <- new\n",
    "\n",
    "output_type_info:\n",
    "  con_loss_name: \"SmoothL1Loss\"\n",
    "  target_con_columns:\n",
    "  - \"-200_ctrl\"\n",
    "  - \"-199_ctrl\"\n",
    "  - \"-198_ctrl\"\n",
    "  - \"-197_ctrl\"\n",
    "  - \"-196_ctrl\"\n",
    "  - \"-195_ctrl\"\n",
    "  - \"-194_ctrl\"\n",
    "  - \"-193_ctrl\"\n",
    "  - \"-192_ctrl\"\n",
    "  - \"-191_ctrl\"\n",
    "  - \"-190_ctrl\"\n",
    "  - \"-189_ctrl\"\n",
    "  - \"-188_ctrl\"\n",
    "  - \"-187_ctrl\"\n",
    "  - \"-186_ctrl\"\n",
    "  - \"-185_ctrl\"\n",
    "  - \"-184_ctrl\"\n",
    "  - \"-183_ctrl\"\n",
    "  - \"-182_ctrl\"\n",
    "  - \"-181_ctrl\"\n",
    "  - \"-180_ctrl\"\n",
    "  - \"-179_ctrl\"\n",
    "  - \"-178_ctrl\"\n",
    "  - \"-177_ctrl\"\n",
    "  - \"-176_ctrl\"\n",
    "  - \"-175_ctrl\"\n",
    "  - \"-174_ctrl\"\n",
    "  - \"-173_ctrl\"\n",
    "  - \"-172_ctrl\"\n",
    "  - \"-171_ctrl\"\n",
    "  - \"-170_ctrl\"\n",
    "  - \"-169_ctrl\"\n",
    "  - \"-168_ctrl\"\n",
    "  - \"-167_ctrl\"\n",
    "  - \"-166_ctrl\"\n",
    "  - \"-165_ctrl\"\n",
    "  - \"-164_ctrl\"\n",
    "  - \"-163_ctrl\"\n",
    "  - \"-162_ctrl\"\n",
    "  - \"-161_ctrl\"\n",
    "  - \"-160_ctrl\"\n",
    "  - \"-159_ctrl\"\n",
    "  - \"-158_ctrl\"\n",
    "  - \"-157_ctrl\"\n",
    "  - \"-156_ctrl\"\n",
    "  - \"-155_ctrl\"\n",
    "  - \"-154_ctrl\"\n",
    "  - \"-153_ctrl\"\n",
    "  - \"-152_ctrl\"\n",
    "  - \"-151_ctrl\"\n",
    "  - \"-150_ctrl\"\n",
    "  - \"-149_ctrl\"\n",
    "  - \"-148_ctrl\"\n",
    "  - \"-147_ctrl\"\n",
    "  - \"-146_ctrl\"\n",
    "  - \"-145_ctrl\"\n",
    "  - \"-144_ctrl\"\n",
    "  - \"-143_ctrl\"\n",
    "  - \"-142_ctrl\"\n",
    "  - \"-141_ctrl\"\n",
    "  - \"-140_ctrl\"\n",
    "  - \"-139_ctrl\"\n",
    "  - \"-138_ctrl\"\n",
    "  - \"-137_ctrl\"\n",
    "  - \"-136_ctrl\"\n",
    "  - \"-135_ctrl\"\n",
    "  - \"-134_ctrl\"\n",
    "  - \"-133_ctrl\"\n",
    "  - \"-132_ctrl\"\n",
    "  - \"-131_ctrl\"\n",
    "  - \"-130_ctrl\"\n",
    "  - \"-129_ctrl\"\n",
    "  - \"-128_ctrl\"\n",
    "  - \"-127_ctrl\"\n",
    "  - \"-126_ctrl\"\n",
    "  - \"-125_ctrl\"\n",
    "  - \"-124_ctrl\"\n",
    "  - \"-123_ctrl\"\n",
    "  - \"-122_ctrl\"\n",
    "  - \"-121_ctrl\"\n",
    "  - \"-120_ctrl\"\n",
    "  - \"-119_ctrl\"\n",
    "  - \"-118_ctrl\"\n",
    "  - \"-117_ctrl\"\n",
    "  - \"-116_ctrl\"\n",
    "  - \"-115_ctrl\"\n",
    "  - \"-114_ctrl\"\n",
    "  - \"-113_ctrl\"\n",
    "  - \"-112_ctrl\"\n",
    "  - \"-111_ctrl\"\n",
    "  - \"-110_ctrl\"\n",
    "  - \"-109_ctrl\"\n",
    "  - \"-108_ctrl\"\n",
    "  - \"-107_ctrl\"\n",
    "  - \"-106_ctrl\"\n",
    "  - \"-105_ctrl\"\n",
    "  - \"-104_ctrl\"\n",
    "  - \"-103_ctrl\"\n",
    "  - \"-102_ctrl\"\n",
    "  - \"-101_ctrl\"\n",
    "  - \"-100_ctrl\"\n",
    "  - \"-99_ctrl\"\n",
    "  - \"-98_ctrl\"\n",
    "  - \"-97_ctrl\"\n",
    "  - \"-96_ctrl\"\n",
    "  - \"-95_ctrl\"\n",
    "  - \"-94_ctrl\"\n",
    "  - \"-93_ctrl\"\n",
    "  - \"-92_ctrl\"\n",
    "  - \"-91_ctrl\"\n",
    "  - \"-90_ctrl\"\n",
    "  - \"-89_ctrl\"\n",
    "  - \"-88_ctrl\"\n",
    "  - \"-87_ctrl\"\n",
    "  - \"-86_ctrl\"\n",
    "  - \"-85_ctrl\"\n",
    "  - \"-84_ctrl\"\n",
    "  - \"-83_ctrl\"\n",
    "  - \"-82_ctrl\"\n",
    "  - \"-81_ctrl\"\n",
    "  - \"-80_ctrl\"\n",
    "  - \"-79_ctrl\"\n",
    "  - \"-78_ctrl\"\n",
    "  - \"-77_ctrl\"\n",
    "  - \"-76_ctrl\"\n",
    "  - \"-75_ctrl\"\n",
    "  - \"-74_ctrl\"\n",
    "  - \"-73_ctrl\"\n",
    "  - \"-72_ctrl\"\n",
    "  - \"-71_ctrl\"\n",
    "  - \"-70_ctrl\"\n",
    "  - \"-69_ctrl\"\n",
    "  - \"-68_ctrl\"\n",
    "  - \"-67_ctrl\"\n",
    "  - \"-66_ctrl\"\n",
    "  - \"-65_ctrl\"\n",
    "  - \"-64_ctrl\"\n",
    "  - \"-63_ctrl\"\n",
    "  - \"-62_ctrl\"\n",
    "  - \"-61_ctrl\"\n",
    "  - \"-60_ctrl\"\n",
    "  - \"-59_ctrl\"\n",
    "  - \"-58_ctrl\"\n",
    "  - \"-57_ctrl\"\n",
    "  - \"-56_ctrl\"\n",
    "  - \"-55_ctrl\"\n",
    "  - \"-54_ctrl\"\n",
    "  - \"-53_ctrl\"\n",
    "  - \"-52_ctrl\"\n",
    "  - \"-51_ctrl\"\n",
    "  - \"-50_ctrl\"\n",
    "  - \"-49_ctrl\"\n",
    "  - \"-48_ctrl\"\n",
    "  - \"-47_ctrl\"\n",
    "  - \"-46_ctrl\"\n",
    "  - \"-45_ctrl\"\n",
    "  - \"-44_ctrl\"\n",
    "  - \"-43_ctrl\"\n",
    "  - \"-42_ctrl\"\n",
    "  - \"-41_ctrl\"\n",
    "  - \"-40_ctrl\"\n",
    "  - \"-39_ctrl\"\n",
    "  - \"-38_ctrl\"\n",
    "  - \"-37_ctrl\"\n",
    "  - \"-36_ctrl\"\n",
    "  - \"-35_ctrl\"\n",
    "  - \"-34_ctrl\"\n",
    "  - \"-33_ctrl\"\n",
    "  - \"-32_ctrl\"\n",
    "  - \"-31_ctrl\"\n",
    "  - \"-30_ctrl\"\n",
    "  - \"-29_ctrl\"\n",
    "  - \"-28_ctrl\"\n",
    "  - \"-27_ctrl\"\n",
    "  - \"-26_ctrl\"\n",
    "  - \"-25_ctrl\"\n",
    "  - \"-24_ctrl\"\n",
    "  - \"-23_ctrl\"\n",
    "  - \"-22_ctrl\"\n",
    "  - \"-21_ctrl\"\n",
    "  - \"-20_ctrl\"\n",
    "  - \"-19_ctrl\"\n",
    "  - \"-18_ctrl\"\n",
    "  - \"-17_ctrl\"\n",
    "  - \"-16_ctrl\"\n",
    "  - \"-15_ctrl\"\n",
    "  - \"-14_ctrl\"\n",
    "  - \"-13_ctrl\"\n",
    "  - \"-12_ctrl\"\n",
    "  - \"-11_ctrl\"\n",
    "  - \"-10_ctrl\"\n",
    "  - \"-9_ctrl\"\n",
    "  - \"-8_ctrl\"\n",
    "  - \"-7_ctrl\"\n",
    "  - \"-6_ctrl\"\n",
    "  - \"-5_ctrl\"\n",
    "  - \"-4_ctrl\"\n",
    "  - \"-3_ctrl\"\n",
    "  - \"-2_ctrl\"\n",
    "  - \"-1_ctrl\"\n",
    "  - \"0_ctrl\"\n",
    "  - \"1_ctrl\"\n",
    "  - \"2_ctrl\"\n",
    "  - \"3_ctrl\"\n",
    "  - \"4_ctrl\"\n",
    "  - \"5_ctrl\"\n",
    "  - \"6_ctrl\"\n",
    "  - \"7_ctrl\"\n",
    "  - \"8_ctrl\"\n",
    "  - \"9_ctrl\"\n",
    "  - \"10_ctrl\"\n",
    "  - \"11_ctrl\"\n",
    "  - \"12_ctrl\"\n",
    "  - \"13_ctrl\"\n",
    "  - \"14_ctrl\"\n",
    "  - \"15_ctrl\"\n",
    "  - \"16_ctrl\"\n",
    "  - \"17_ctrl\"\n",
    "  - \"18_ctrl\"\n",
    "  - \"19_ctrl\"\n",
    "  - \"20_ctrl\"\n",
    "  - \"21_ctrl\"\n",
    "  - \"22_ctrl\"\n",
    "  - \"23_ctrl\"\n",
    "  - \"24_ctrl\"\n",
    "  - \"25_ctrl\"\n",
    "  - \"26_ctrl\"\n",
    "  - \"27_ctrl\"\n",
    "  - \"28_ctrl\"\n",
    "  - \"29_ctrl\"\n",
    "  - \"30_ctrl\"\n",
    "  - \"31_ctrl\"\n",
    "  - \"32_ctrl\"\n",
    "  - \"33_ctrl\"\n",
    "  - \"34_ctrl\"\n",
    "  - \"35_ctrl\"\n",
    "  - \"36_ctrl\"\n",
    "  - \"37_ctrl\"\n",
    "  - \"38_ctrl\"\n",
    "  - \"39_ctrl\"\n",
    "  - \"40_ctrl\"\n",
    "  - \"41_ctrl\"\n",
    "  - \"42_ctrl\"\n",
    "  - \"43_ctrl\"\n",
    "  - \"44_ctrl\"\n",
    "  - \"45_ctrl\"\n",
    "  - \"46_ctrl\"\n",
    "  - \"47_ctrl\"\n",
    "  - \"48_ctrl\"\n",
    "  - \"49_ctrl\"\n",
    "  - \"50_ctrl\"\n",
    "  - \"51_ctrl\"\n",
    "  - \"52_ctrl\"\n",
    "  - \"53_ctrl\"\n",
    "  - \"54_ctrl\"\n",
    "  - \"55_ctrl\"\n",
    "  - \"56_ctrl\"\n",
    "  - \"57_ctrl\"\n",
    "  - \"58_ctrl\"\n",
    "  - \"59_ctrl\"\n",
    "  - \"60_ctrl\"\n",
    "  - \"61_ctrl\"\n",
    "  - \"62_ctrl\"\n",
    "  - \"63_ctrl\"\n",
    "  - \"64_ctrl\"\n",
    "  - \"65_ctrl\"\n",
    "  - \"66_ctrl\"\n",
    "  - \"67_ctrl\"\n",
    "  - \"68_ctrl\"\n",
    "  - \"69_ctrl\"\n",
    "  - \"70_ctrl\"\n",
    "  - \"71_ctrl\"\n",
    "  - \"72_ctrl\"\n",
    "  - \"73_ctrl\"\n",
    "  - \"74_ctrl\"\n",
    "  - \"75_ctrl\"\n",
    "  - \"76_ctrl\"\n",
    "  - \"77_ctrl\"\n",
    "  - \"78_ctrl\"\n",
    "  - \"79_ctrl\"\n",
    "  - \"80_ctrl\"\n",
    "  - \"81_ctrl\"\n",
    "  - \"82_ctrl\"\n",
    "  - \"83_ctrl\"\n",
    "  - \"84_ctrl\"\n",
    "  - \"85_ctrl\"\n",
    "  - \"86_ctrl\"\n",
    "  - \"87_ctrl\"\n",
    "  - \"88_ctrl\"\n",
    "  - \"89_ctrl\"\n",
    "  - \"90_ctrl\"\n",
    "  - \"91_ctrl\"\n",
    "  - \"92_ctrl\"\n",
    "  - \"93_ctrl\"\n",
    "  - \"94_ctrl\"\n",
    "  - \"95_ctrl\"\n",
    "  - \"96_ctrl\"\n",
    "  - \"97_ctrl\"\n",
    "  - \"98_ctrl\"\n",
    "  - \"99_ctrl\"\n",
    "  - \"100_ctrl\"\n",
    "  - \"101_ctrl\"\n",
    "  - \"102_ctrl\"\n",
    "  - \"103_ctrl\"\n",
    "  - \"104_ctrl\"\n",
    "  - \"105_ctrl\"\n",
    "  - \"106_ctrl\"\n",
    "  - \"107_ctrl\"\n",
    "  - \"108_ctrl\"\n",
    "  - \"109_ctrl\"\n",
    "  - \"110_ctrl\"\n",
    "  - \"111_ctrl\"\n",
    "  - \"112_ctrl\"\n",
    "  - \"113_ctrl\"\n",
    "  - \"114_ctrl\"\n",
    "  - \"115_ctrl\"\n",
    "  - \"116_ctrl\"\n",
    "  - \"117_ctrl\"\n",
    "  - \"118_ctrl\"\n",
    "  - \"119_ctrl\"\n",
    "  - \"120_ctrl\"\n",
    "  - \"121_ctrl\"\n",
    "  - \"122_ctrl\"\n",
    "  - \"123_ctrl\"\n",
    "  - \"124_ctrl\"\n",
    "  - \"125_ctrl\"\n",
    "  - \"126_ctrl\"\n",
    "  - \"127_ctrl\"\n",
    "  - \"128_ctrl\"\n",
    "  - \"129_ctrl\"\n",
    "  - \"130_ctrl\"\n",
    "  - \"131_ctrl\"\n",
    "  - \"132_ctrl\"\n",
    "  - \"133_ctrl\"\n",
    "  - \"134_ctrl\"\n",
    "  - \"135_ctrl\"\n",
    "  - \"136_ctrl\"\n",
    "  - \"137_ctrl\"\n",
    "  - \"138_ctrl\"\n",
    "  - \"139_ctrl\"\n",
    "  - \"140_ctrl\"\n",
    "  - \"141_ctrl\"\n",
    "  - \"142_ctrl\"\n",
    "  - \"143_ctrl\"\n",
    "  - \"144_ctrl\"\n",
    "  - \"145_ctrl\"\n",
    "  - \"146_ctrl\"\n",
    "  - \"147_ctrl\"\n",
    "  - \"148_ctrl\"\n",
    "  - \"149_ctrl\"\n",
    "  - \"150_ctrl\"\n",
    "  - \"151_ctrl\"\n",
    "  - \"152_ctrl\"\n",
    "  - \"153_ctrl\"\n",
    "  - \"154_ctrl\"\n",
    "  - \"155_ctrl\"\n",
    "  - \"156_ctrl\"\n",
    "  - \"157_ctrl\"\n",
    "  - \"158_ctrl\"\n",
    "  - \"159_ctrl\"\n",
    "  - \"160_ctrl\"\n",
    "  - \"161_ctrl\"\n",
    "  - \"162_ctrl\"\n",
    "  - \"163_ctrl\"\n",
    "  - \"164_ctrl\"\n",
    "  - \"165_ctrl\"\n",
    "  - \"166_ctrl\"\n",
    "  - \"167_ctrl\"\n",
    "  - \"168_ctrl\"\n",
    "  - \"169_ctrl\"\n",
    "  - \"170_ctrl\"\n",
    "  - \"171_ctrl\"\n",
    "  - \"172_ctrl\"\n",
    "  - \"173_ctrl\"\n",
    "  - \"174_ctrl\"\n",
    "  - \"175_ctrl\"\n",
    "  - \"176_ctrl\"\n",
    "  - \"177_ctrl\"\n",
    "  - \"178_ctrl\"\n",
    "  - \"179_ctrl\"\n",
    "  - \"180_ctrl\"\n",
    "  - \"181_ctrl\"\n",
    "  - \"182_ctrl\"\n",
    "  - \"183_ctrl\"\n",
    "  - \"184_ctrl\"\n",
    "  - \"185_ctrl\"\n",
    "  - \"186_ctrl\"\n",
    "  - \"187_ctrl\"\n",
    "  - \"188_ctrl\"\n",
    "  - \"189_ctrl\"\n",
    "  - \"190_ctrl\"\n",
    "  - \"191_ctrl\"\n",
    "  - \"192_ctrl\"\n",
    "  - \"193_ctrl\"\n",
    "  - \"194_ctrl\"\n",
    "  - \"195_ctrl\"\n",
    "  - \"196_ctrl\"\n",
    "  - \"197_ctrl\"\n",
    "  - \"198_ctrl\"\n",
    "  - \"199_ctrl\"\n",
    "  - \"200_ctrl\"\n",
    "  \"\"\"\n",
    "}\n",
    "\n",
    "training_softplus_poisson_contents = {\"globals.yaml\":\"\"\"\n",
    "output_folder: ./runs/gene_expression_only_chrom_pure_conv_softplus_poisson_no_bias_log_input_false/ \n",
    "manual_valid_ids_file: ./annotations/manual_validation_ids_chr17.txt  #manual_validation_ids_chr17_uncoupled.txt\n",
    "checkpoint_interval: 30300 # 100 epochs #60000\n",
    "sample_interval: 30300 #60000\n",
    "n_epochs: 120\n",
    "batch_size: 64\n",
    "optimizer: \"adamw\"\n",
    "lr: 0.0001 #0.0001\n",
    "device: \"cuda\"\n",
    "compute_attributions: false\n",
    "# latent_sampling: \n",
    "#   layers_to_sample:\n",
    "#     - \"input_modules.contact_maps.feature_extractor.conv.0.conv_1\"\n",
    "#     - \"input_modules.contact_maps.feature_extractor.conv.1.conv_2\"\n",
    "#     - \"input_modules.contact_maps.feature_extractor.conv.2.conv_2\"\n",
    "#     - \"input_modules.contact_maps.feature_extractor.conv.3.conv_2\"\n",
    "# attribution_background_samples: 512\n",
    "# attributions_every_sample_factor: 1\n",
    "#pretrained_checkpoint: best_models/gene_expression_exformer_unlimited_chrom_and_micro_with_attention_model_117600_perf-average=0.8435.pt \n",
    "\n",
    "\"\"\",\n",
    "\"input_cnn.yaml\": \"\"\" \n",
    "input_info:\n",
    "  input_source: ./inputs/landscape_arrays/training/ \n",
    "  input_name: gene_expression\n",
    "  input_type: array\n",
    "\n",
    "model_config:\n",
    "  model_type: cnn\n",
    "  #pre_normalization: \"instancenorm\"\n",
    "  model_init_config:\n",
    "    num_output_features: 512 # before fc_repr_dim\n",
    "    layers: [4,4]\n",
    "    kernel_height: 1\n",
    "    down_stride_width: 2\n",
    "    first_stride_expansion_width: 1\n",
    "    first_kernel_expansion_height: 4 #5\n",
    "    kernel_width: 10  #10\n",
    "    dilation_factor_width: 2\n",
    "    dilation_factor_height: 1\n",
    "    channel_exp_base: 5  #3 #-1\n",
    "    first_channel_expansion: 2\n",
    "    rb_do: .3\n",
    "    stochastic_depth_p: .1\n",
    "    attention_inclusion_cutoff: 1 #50 #256\n",
    "\n",
    "\"\"\",\n",
    "\"fusion.yaml\": \"\"\"\n",
    "model_config:\n",
    "  fc_do: 0.4\n",
    "  fc_task_dim: 256\n",
    "  layers:\n",
    "  - 2\n",
    "  rb_do: 0.4\n",
    "  stochastic_depth_p: 0.5\n",
    "model_type: \"mlp-residual\"\n",
    "\"\"\",\n",
    "\"outputs_2_cond.yaml\":\"\"\"\n",
    "output_info:\n",
    "  output_name: expression_output\n",
    "  output_source: ./targets/training_targets.csv \n",
    "  output_type: tabular\n",
    "\n",
    "model_config: # <- new\n",
    "  model_type: linear # <- new\n",
    "\n",
    "output_type_info:\n",
    "  con_loss_name: \"PoissonNLLLoss\"\n",
    "  target_con_columns:\n",
    "  - \"-200_ctrl\"\n",
    "  - \"-199_ctrl\"\n",
    "  - \"-198_ctrl\"\n",
    "  - \"-197_ctrl\"\n",
    "  - \"-196_ctrl\"\n",
    "  - \"-195_ctrl\"\n",
    "  - \"-194_ctrl\"\n",
    "  - \"-193_ctrl\"\n",
    "  - \"-192_ctrl\"\n",
    "  - \"-191_ctrl\"\n",
    "  - \"-190_ctrl\"\n",
    "  - \"-189_ctrl\"\n",
    "  - \"-188_ctrl\"\n",
    "  - \"-187_ctrl\"\n",
    "  - \"-186_ctrl\"\n",
    "  - \"-185_ctrl\"\n",
    "  - \"-184_ctrl\"\n",
    "  - \"-183_ctrl\"\n",
    "  - \"-182_ctrl\"\n",
    "  - \"-181_ctrl\"\n",
    "  - \"-180_ctrl\"\n",
    "  - \"-179_ctrl\"\n",
    "  - \"-178_ctrl\"\n",
    "  - \"-177_ctrl\"\n",
    "  - \"-176_ctrl\"\n",
    "  - \"-175_ctrl\"\n",
    "  - \"-174_ctrl\"\n",
    "  - \"-173_ctrl\"\n",
    "  - \"-172_ctrl\"\n",
    "  - \"-171_ctrl\"\n",
    "  - \"-170_ctrl\"\n",
    "  - \"-169_ctrl\"\n",
    "  - \"-168_ctrl\"\n",
    "  - \"-167_ctrl\"\n",
    "  - \"-166_ctrl\"\n",
    "  - \"-165_ctrl\"\n",
    "  - \"-164_ctrl\"\n",
    "  - \"-163_ctrl\"\n",
    "  - \"-162_ctrl\"\n",
    "  - \"-161_ctrl\"\n",
    "  - \"-160_ctrl\"\n",
    "  - \"-159_ctrl\"\n",
    "  - \"-158_ctrl\"\n",
    "  - \"-157_ctrl\"\n",
    "  - \"-156_ctrl\"\n",
    "  - \"-155_ctrl\"\n",
    "  - \"-154_ctrl\"\n",
    "  - \"-153_ctrl\"\n",
    "  - \"-152_ctrl\"\n",
    "  - \"-151_ctrl\"\n",
    "  - \"-150_ctrl\"\n",
    "  - \"-149_ctrl\"\n",
    "  - \"-148_ctrl\"\n",
    "  - \"-147_ctrl\"\n",
    "  - \"-146_ctrl\"\n",
    "  - \"-145_ctrl\"\n",
    "  - \"-144_ctrl\"\n",
    "  - \"-143_ctrl\"\n",
    "  - \"-142_ctrl\"\n",
    "  - \"-141_ctrl\"\n",
    "  - \"-140_ctrl\"\n",
    "  - \"-139_ctrl\"\n",
    "  - \"-138_ctrl\"\n",
    "  - \"-137_ctrl\"\n",
    "  - \"-136_ctrl\"\n",
    "  - \"-135_ctrl\"\n",
    "  - \"-134_ctrl\"\n",
    "  - \"-133_ctrl\"\n",
    "  - \"-132_ctrl\"\n",
    "  - \"-131_ctrl\"\n",
    "  - \"-130_ctrl\"\n",
    "  - \"-129_ctrl\"\n",
    "  - \"-128_ctrl\"\n",
    "  - \"-127_ctrl\"\n",
    "  - \"-126_ctrl\"\n",
    "  - \"-125_ctrl\"\n",
    "  - \"-124_ctrl\"\n",
    "  - \"-123_ctrl\"\n",
    "  - \"-122_ctrl\"\n",
    "  - \"-121_ctrl\"\n",
    "  - \"-120_ctrl\"\n",
    "  - \"-119_ctrl\"\n",
    "  - \"-118_ctrl\"\n",
    "  - \"-117_ctrl\"\n",
    "  - \"-116_ctrl\"\n",
    "  - \"-115_ctrl\"\n",
    "  - \"-114_ctrl\"\n",
    "  - \"-113_ctrl\"\n",
    "  - \"-112_ctrl\"\n",
    "  - \"-111_ctrl\"\n",
    "  - \"-110_ctrl\"\n",
    "  - \"-109_ctrl\"\n",
    "  - \"-108_ctrl\"\n",
    "  - \"-107_ctrl\"\n",
    "  - \"-106_ctrl\"\n",
    "  - \"-105_ctrl\"\n",
    "  - \"-104_ctrl\"\n",
    "  - \"-103_ctrl\"\n",
    "  - \"-102_ctrl\"\n",
    "  - \"-101_ctrl\"\n",
    "  - \"-100_ctrl\"\n",
    "  - \"-99_ctrl\"\n",
    "  - \"-98_ctrl\"\n",
    "  - \"-97_ctrl\"\n",
    "  - \"-96_ctrl\"\n",
    "  - \"-95_ctrl\"\n",
    "  - \"-94_ctrl\"\n",
    "  - \"-93_ctrl\"\n",
    "  - \"-92_ctrl\"\n",
    "  - \"-91_ctrl\"\n",
    "  - \"-90_ctrl\"\n",
    "  - \"-89_ctrl\"\n",
    "  - \"-88_ctrl\"\n",
    "  - \"-87_ctrl\"\n",
    "  - \"-86_ctrl\"\n",
    "  - \"-85_ctrl\"\n",
    "  - \"-84_ctrl\"\n",
    "  - \"-83_ctrl\"\n",
    "  - \"-82_ctrl\"\n",
    "  - \"-81_ctrl\"\n",
    "  - \"-80_ctrl\"\n",
    "  - \"-79_ctrl\"\n",
    "  - \"-78_ctrl\"\n",
    "  - \"-77_ctrl\"\n",
    "  - \"-76_ctrl\"\n",
    "  - \"-75_ctrl\"\n",
    "  - \"-74_ctrl\"\n",
    "  - \"-73_ctrl\"\n",
    "  - \"-72_ctrl\"\n",
    "  - \"-71_ctrl\"\n",
    "  - \"-70_ctrl\"\n",
    "  - \"-69_ctrl\"\n",
    "  - \"-68_ctrl\"\n",
    "  - \"-67_ctrl\"\n",
    "  - \"-66_ctrl\"\n",
    "  - \"-65_ctrl\"\n",
    "  - \"-64_ctrl\"\n",
    "  - \"-63_ctrl\"\n",
    "  - \"-62_ctrl\"\n",
    "  - \"-61_ctrl\"\n",
    "  - \"-60_ctrl\"\n",
    "  - \"-59_ctrl\"\n",
    "  - \"-58_ctrl\"\n",
    "  - \"-57_ctrl\"\n",
    "  - \"-56_ctrl\"\n",
    "  - \"-55_ctrl\"\n",
    "  - \"-54_ctrl\"\n",
    "  - \"-53_ctrl\"\n",
    "  - \"-52_ctrl\"\n",
    "  - \"-51_ctrl\"\n",
    "  - \"-50_ctrl\"\n",
    "  - \"-49_ctrl\"\n",
    "  - \"-48_ctrl\"\n",
    "  - \"-47_ctrl\"\n",
    "  - \"-46_ctrl\"\n",
    "  - \"-45_ctrl\"\n",
    "  - \"-44_ctrl\"\n",
    "  - \"-43_ctrl\"\n",
    "  - \"-42_ctrl\"\n",
    "  - \"-41_ctrl\"\n",
    "  - \"-40_ctrl\"\n",
    "  - \"-39_ctrl\"\n",
    "  - \"-38_ctrl\"\n",
    "  - \"-37_ctrl\"\n",
    "  - \"-36_ctrl\"\n",
    "  - \"-35_ctrl\"\n",
    "  - \"-34_ctrl\"\n",
    "  - \"-33_ctrl\"\n",
    "  - \"-32_ctrl\"\n",
    "  - \"-31_ctrl\"\n",
    "  - \"-30_ctrl\"\n",
    "  - \"-29_ctrl\"\n",
    "  - \"-28_ctrl\"\n",
    "  - \"-27_ctrl\"\n",
    "  - \"-26_ctrl\"\n",
    "  - \"-25_ctrl\"\n",
    "  - \"-24_ctrl\"\n",
    "  - \"-23_ctrl\"\n",
    "  - \"-22_ctrl\"\n",
    "  - \"-21_ctrl\"\n",
    "  - \"-20_ctrl\"\n",
    "  - \"-19_ctrl\"\n",
    "  - \"-18_ctrl\"\n",
    "  - \"-17_ctrl\"\n",
    "  - \"-16_ctrl\"\n",
    "  - \"-15_ctrl\"\n",
    "  - \"-14_ctrl\"\n",
    "  - \"-13_ctrl\"\n",
    "  - \"-12_ctrl\"\n",
    "  - \"-11_ctrl\"\n",
    "  - \"-10_ctrl\"\n",
    "  - \"-9_ctrl\"\n",
    "  - \"-8_ctrl\"\n",
    "  - \"-7_ctrl\"\n",
    "  - \"-6_ctrl\"\n",
    "  - \"-5_ctrl\"\n",
    "  - \"-4_ctrl\"\n",
    "  - \"-3_ctrl\"\n",
    "  - \"-2_ctrl\"\n",
    "  - \"-1_ctrl\"\n",
    "  - \"0_ctrl\"\n",
    "  - \"1_ctrl\"\n",
    "  - \"2_ctrl\"\n",
    "  - \"3_ctrl\"\n",
    "  - \"4_ctrl\"\n",
    "  - \"5_ctrl\"\n",
    "  - \"6_ctrl\"\n",
    "  - \"7_ctrl\"\n",
    "  - \"8_ctrl\"\n",
    "  - \"9_ctrl\"\n",
    "  - \"10_ctrl\"\n",
    "  - \"11_ctrl\"\n",
    "  - \"12_ctrl\"\n",
    "  - \"13_ctrl\"\n",
    "  - \"14_ctrl\"\n",
    "  - \"15_ctrl\"\n",
    "  - \"16_ctrl\"\n",
    "  - \"17_ctrl\"\n",
    "  - \"18_ctrl\"\n",
    "  - \"19_ctrl\"\n",
    "  - \"20_ctrl\"\n",
    "  - \"21_ctrl\"\n",
    "  - \"22_ctrl\"\n",
    "  - \"23_ctrl\"\n",
    "  - \"24_ctrl\"\n",
    "  - \"25_ctrl\"\n",
    "  - \"26_ctrl\"\n",
    "  - \"27_ctrl\"\n",
    "  - \"28_ctrl\"\n",
    "  - \"29_ctrl\"\n",
    "  - \"30_ctrl\"\n",
    "  - \"31_ctrl\"\n",
    "  - \"32_ctrl\"\n",
    "  - \"33_ctrl\"\n",
    "  - \"34_ctrl\"\n",
    "  - \"35_ctrl\"\n",
    "  - \"36_ctrl\"\n",
    "  - \"37_ctrl\"\n",
    "  - \"38_ctrl\"\n",
    "  - \"39_ctrl\"\n",
    "  - \"40_ctrl\"\n",
    "  - \"41_ctrl\"\n",
    "  - \"42_ctrl\"\n",
    "  - \"43_ctrl\"\n",
    "  - \"44_ctrl\"\n",
    "  - \"45_ctrl\"\n",
    "  - \"46_ctrl\"\n",
    "  - \"47_ctrl\"\n",
    "  - \"48_ctrl\"\n",
    "  - \"49_ctrl\"\n",
    "  - \"50_ctrl\"\n",
    "  - \"51_ctrl\"\n",
    "  - \"52_ctrl\"\n",
    "  - \"53_ctrl\"\n",
    "  - \"54_ctrl\"\n",
    "  - \"55_ctrl\"\n",
    "  - \"56_ctrl\"\n",
    "  - \"57_ctrl\"\n",
    "  - \"58_ctrl\"\n",
    "  - \"59_ctrl\"\n",
    "  - \"60_ctrl\"\n",
    "  - \"61_ctrl\"\n",
    "  - \"62_ctrl\"\n",
    "  - \"63_ctrl\"\n",
    "  - \"64_ctrl\"\n",
    "  - \"65_ctrl\"\n",
    "  - \"66_ctrl\"\n",
    "  - \"67_ctrl\"\n",
    "  - \"68_ctrl\"\n",
    "  - \"69_ctrl\"\n",
    "  - \"70_ctrl\"\n",
    "  - \"71_ctrl\"\n",
    "  - \"72_ctrl\"\n",
    "  - \"73_ctrl\"\n",
    "  - \"74_ctrl\"\n",
    "  - \"75_ctrl\"\n",
    "  - \"76_ctrl\"\n",
    "  - \"77_ctrl\"\n",
    "  - \"78_ctrl\"\n",
    "  - \"79_ctrl\"\n",
    "  - \"80_ctrl\"\n",
    "  - \"81_ctrl\"\n",
    "  - \"82_ctrl\"\n",
    "  - \"83_ctrl\"\n",
    "  - \"84_ctrl\"\n",
    "  - \"85_ctrl\"\n",
    "  - \"86_ctrl\"\n",
    "  - \"87_ctrl\"\n",
    "  - \"88_ctrl\"\n",
    "  - \"89_ctrl\"\n",
    "  - \"90_ctrl\"\n",
    "  - \"91_ctrl\"\n",
    "  - \"92_ctrl\"\n",
    "  - \"93_ctrl\"\n",
    "  - \"94_ctrl\"\n",
    "  - \"95_ctrl\"\n",
    "  - \"96_ctrl\"\n",
    "  - \"97_ctrl\"\n",
    "  - \"98_ctrl\"\n",
    "  - \"99_ctrl\"\n",
    "  - \"100_ctrl\"\n",
    "  - \"101_ctrl\"\n",
    "  - \"102_ctrl\"\n",
    "  - \"103_ctrl\"\n",
    "  - \"104_ctrl\"\n",
    "  - \"105_ctrl\"\n",
    "  - \"106_ctrl\"\n",
    "  - \"107_ctrl\"\n",
    "  - \"108_ctrl\"\n",
    "  - \"109_ctrl\"\n",
    "  - \"110_ctrl\"\n",
    "  - \"111_ctrl\"\n",
    "  - \"112_ctrl\"\n",
    "  - \"113_ctrl\"\n",
    "  - \"114_ctrl\"\n",
    "  - \"115_ctrl\"\n",
    "  - \"116_ctrl\"\n",
    "  - \"117_ctrl\"\n",
    "  - \"118_ctrl\"\n",
    "  - \"119_ctrl\"\n",
    "  - \"120_ctrl\"\n",
    "  - \"121_ctrl\"\n",
    "  - \"122_ctrl\"\n",
    "  - \"123_ctrl\"\n",
    "  - \"124_ctrl\"\n",
    "  - \"125_ctrl\"\n",
    "  - \"126_ctrl\"\n",
    "  - \"127_ctrl\"\n",
    "  - \"128_ctrl\"\n",
    "  - \"129_ctrl\"\n",
    "  - \"130_ctrl\"\n",
    "  - \"131_ctrl\"\n",
    "  - \"132_ctrl\"\n",
    "  - \"133_ctrl\"\n",
    "  - \"134_ctrl\"\n",
    "  - \"135_ctrl\"\n",
    "  - \"136_ctrl\"\n",
    "  - \"137_ctrl\"\n",
    "  - \"138_ctrl\"\n",
    "  - \"139_ctrl\"\n",
    "  - \"140_ctrl\"\n",
    "  - \"141_ctrl\"\n",
    "  - \"142_ctrl\"\n",
    "  - \"143_ctrl\"\n",
    "  - \"144_ctrl\"\n",
    "  - \"145_ctrl\"\n",
    "  - \"146_ctrl\"\n",
    "  - \"147_ctrl\"\n",
    "  - \"148_ctrl\"\n",
    "  - \"149_ctrl\"\n",
    "  - \"150_ctrl\"\n",
    "  - \"151_ctrl\"\n",
    "  - \"152_ctrl\"\n",
    "  - \"153_ctrl\"\n",
    "  - \"154_ctrl\"\n",
    "  - \"155_ctrl\"\n",
    "  - \"156_ctrl\"\n",
    "  - \"157_ctrl\"\n",
    "  - \"158_ctrl\"\n",
    "  - \"159_ctrl\"\n",
    "  - \"160_ctrl\"\n",
    "  - \"161_ctrl\"\n",
    "  - \"162_ctrl\"\n",
    "  - \"163_ctrl\"\n",
    "  - \"164_ctrl\"\n",
    "  - \"165_ctrl\"\n",
    "  - \"166_ctrl\"\n",
    "  - \"167_ctrl\"\n",
    "  - \"168_ctrl\"\n",
    "  - \"169_ctrl\"\n",
    "  - \"170_ctrl\"\n",
    "  - \"171_ctrl\"\n",
    "  - \"172_ctrl\"\n",
    "  - \"173_ctrl\"\n",
    "  - \"174_ctrl\"\n",
    "  - \"175_ctrl\"\n",
    "  - \"176_ctrl\"\n",
    "  - \"177_ctrl\"\n",
    "  - \"178_ctrl\"\n",
    "  - \"179_ctrl\"\n",
    "  - \"180_ctrl\"\n",
    "  - \"181_ctrl\"\n",
    "  - \"182_ctrl\"\n",
    "  - \"183_ctrl\"\n",
    "  - \"184_ctrl\"\n",
    "  - \"185_ctrl\"\n",
    "  - \"186_ctrl\"\n",
    "  - \"187_ctrl\"\n",
    "  - \"188_ctrl\"\n",
    "  - \"189_ctrl\"\n",
    "  - \"190_ctrl\"\n",
    "  - \"191_ctrl\"\n",
    "  - \"192_ctrl\"\n",
    "  - \"193_ctrl\"\n",
    "  - \"194_ctrl\"\n",
    "  - \"195_ctrl\"\n",
    "  - \"196_ctrl\"\n",
    "  - \"197_ctrl\"\n",
    "  - \"198_ctrl\"\n",
    "  - \"199_ctrl\"\n",
    "  - \"200_ctrl\"\n",
    "  \"\"\"\n",
    "}\n",
    "\n",
    "test_20kbp_yaml_contents = {\"globals.yaml\":\"\"\"\n",
    "#output_folder: ./runs/test_runs/gene_expression_only_chrom_pure_conv_20kbp_context/  #gene_expression_prediction_no_H3K27ac_uncoupled/     #gene_expression_cnn_1kbp_401_bins_2_cond_reloaded/ \n",
    "#manual_valid_ids_file: ./annotations/manual_validation_ids_chr17.txt  #manual_validation_ids_chr17_uncoupled.txt\n",
    "checkpoint_interval: 30300 # 100 epochs is 30300 #60000\n",
    "sample_interval: 30300 #60000\n",
    "n_epochs: 120\n",
    "batch_size: 64\n",
    "optimizer: \"adamw\"\n",
    "lr: 0.0001 #0.0001\n",
    "device: \"cuda\"\n",
    "compute_attributions: false\n",
    "\"\"\",\n",
    "\"input_cnn.yaml\": \"\"\" \n",
    "input_info:\n",
    "  input_source: ./inputs/landscape_arrays/test_20kbp_context/ \n",
    "  input_name: gene_expression\n",
    "  input_type: array\n",
    "\n",
    "model_config:\n",
    "  model_type: cnn\n",
    "  #pre_normalization: \"instancenorm\"\n",
    "  model_init_config:\n",
    "    num_output_features: 512 # before fc_repr_dim\n",
    "    layers: [1,1]\n",
    "    kernel_height: 1\n",
    "    down_stride_width: 2\n",
    "    first_stride_expansion_width: 1\n",
    "    first_kernel_expansion_height: 4 #5\n",
    "    kernel_width: 10  #10\n",
    "    dilation_factor_width: 2\n",
    "    dilation_factor_height: 1\n",
    "    channel_exp_base: 5  #3 #-1\n",
    "    first_channel_expansion: 2\n",
    "    rb_do: .3\n",
    "    stochastic_depth_p: .1\n",
    "    attention_inclusion_cutoff: 1 #50 #256\n",
    "\n",
    "\"\"\",\n",
    "\"fusion.yaml\": \"\"\"\n",
    "model_config:\n",
    "  fc_do: 0.4\n",
    "  fc_task_dim: 256\n",
    "  layers:\n",
    "  - 1\n",
    "  rb_do: 0.4\n",
    "  stochastic_depth_p: 0.5\n",
    "model_type: \"mlp-residual\"\n",
    "\"\"\",\n",
    "\"outputs_2_cond.yaml\":\"\"\"\n",
    "output_info:\n",
    "  output_name: expression_output\n",
    "  output_source: ./targets/test_targets.csv \n",
    "  output_type: tabular\n",
    "\n",
    "model_config: # <- new\n",
    "  model_type: linear # <- new\n",
    "\n",
    "output_type_info:\n",
    "  con_loss_name: \"SmoothL1Loss\"\n",
    "  target_con_columns:\n",
    "  - \"-9_ctrl\"\n",
    "  - \"-8_ctrl\"\n",
    "  - \"-7_ctrl\"\n",
    "  - \"-6_ctrl\"\n",
    "  - \"-5_ctrl\"\n",
    "  - \"-4_ctrl\"\n",
    "  - \"-3_ctrl\"\n",
    "  - \"-2_ctrl\"\n",
    "  - \"-1_ctrl\"\n",
    "  - \"0_ctrl\"\n",
    "  - \"1_ctrl\"\n",
    "  - \"2_ctrl\"\n",
    "  - \"3_ctrl\"\n",
    "  - \"4_ctrl\"\n",
    "  - \"5_ctrl\"\n",
    "  - \"6_ctrl\"\n",
    "  - \"7_ctrl\"\n",
    "  - \"8_ctrl\"\n",
    "  - \"9_ctrl\"\n",
    "  \"\"\"\n",
    "}\n",
    "\n",
    "training_chr19_holdout_yaml_contents = {\"globals.yaml\":\"\"\"\n",
    "output_folder: ./runs/gene_expression_only_chrom_pure_conv_chr19_holdout/  #gene_expression_prediction_no_H3K27ac_uncoupled/     #gene_expression_cnn_1kbp_401_bins_2_cond_reloaded/ \n",
    "manual_valid_ids_file: ./annotations/manual_validation_ids_chr19.txt  #manual_validation_ids_chr17_uncoupled.txt\n",
    "checkpoint_interval: 30300 # 100 epochs #60000\n",
    "sample_interval: 30300 #60000\n",
    "n_epochs: 120\n",
    "batch_size: 64\n",
    "optimizer: \"adamw\"\n",
    "lr: 0.0001 #0.0001\n",
    "device: \"cuda\"\n",
    "compute_attributions: false\n",
    "# latent_sampling: \n",
    "#   layers_to_sample:\n",
    "#     - \"input_modules.contact_maps.feature_extractor.conv.0.conv_1\"\n",
    "#     - \"input_modules.contact_maps.feature_extractor.conv.1.conv_2\"\n",
    "#     - \"input_modules.contact_maps.feature_extractor.conv.2.conv_2\"\n",
    "#     - \"input_modules.contact_maps.feature_extractor.conv.3.conv_2\"\n",
    "# attribution_background_samples: 512\n",
    "# attributions_every_sample_factor: 1\n",
    "#pretrained_checkpoint: best_models/gene_expression_exformer_unlimited_chrom_and_micro_with_attention_model_117600_perf-average=0.8435.pt \n",
    "\n",
    "\"\"\",\n",
    "\"input_cnn.yaml\": \"\"\" \n",
    "input_info:\n",
    "  input_source: ./inputs/landscape_arrays/training/ \n",
    "  input_name: gene_expression\n",
    "  input_type: array\n",
    "\n",
    "model_config:\n",
    "  model_type: cnn\n",
    "  #pre_normalization: \"instancenorm\"\n",
    "  model_init_config:\n",
    "    num_output_features: 512 # before fc_repr_dim\n",
    "    layers: [4,4]\n",
    "    kernel_height: 1\n",
    "    down_stride_width: 2\n",
    "    first_stride_expansion_width: 1\n",
    "    first_kernel_expansion_height: 4 #5\n",
    "    kernel_width: 10  #10\n",
    "    dilation_factor_width: 2\n",
    "    dilation_factor_height: 1\n",
    "    channel_exp_base: 5  #3 #-1\n",
    "    first_channel_expansion: 2\n",
    "    rb_do: .3\n",
    "    stochastic_depth_p: .1\n",
    "    attention_inclusion_cutoff: 1 #50 #256\n",
    "\n",
    "\"\"\",\n",
    "\"fusion.yaml\": \"\"\"\n",
    "model_config:\n",
    "  fc_do: 0.4\n",
    "  fc_task_dim: 256\n",
    "  layers:\n",
    "  - 2\n",
    "  rb_do: 0.4\n",
    "  stochastic_depth_p: 0.5\n",
    "model_type: \"mlp-residual\"\n",
    "\"\"\",\n",
    "\"outputs_2_cond.yaml\":\"\"\"\n",
    "output_info:\n",
    "  output_name: expression_output\n",
    "  output_source: ./targets/training_targets.csv \n",
    "  output_type: tabular\n",
    "\n",
    "model_config: # <- new\n",
    "  model_type: linear # <- new\n",
    "\n",
    "output_type_info:\n",
    "  con_loss_name: \"SmoothL1Loss\"\n",
    "  target_con_columns:\n",
    "  - \"-200_ctrl\"\n",
    "  - \"-199_ctrl\"\n",
    "  - \"-198_ctrl\"\n",
    "  - \"-197_ctrl\"\n",
    "  - \"-196_ctrl\"\n",
    "  - \"-195_ctrl\"\n",
    "  - \"-194_ctrl\"\n",
    "  - \"-193_ctrl\"\n",
    "  - \"-192_ctrl\"\n",
    "  - \"-191_ctrl\"\n",
    "  - \"-190_ctrl\"\n",
    "  - \"-189_ctrl\"\n",
    "  - \"-188_ctrl\"\n",
    "  - \"-187_ctrl\"\n",
    "  - \"-186_ctrl\"\n",
    "  - \"-185_ctrl\"\n",
    "  - \"-184_ctrl\"\n",
    "  - \"-183_ctrl\"\n",
    "  - \"-182_ctrl\"\n",
    "  - \"-181_ctrl\"\n",
    "  - \"-180_ctrl\"\n",
    "  - \"-179_ctrl\"\n",
    "  - \"-178_ctrl\"\n",
    "  - \"-177_ctrl\"\n",
    "  - \"-176_ctrl\"\n",
    "  - \"-175_ctrl\"\n",
    "  - \"-174_ctrl\"\n",
    "  - \"-173_ctrl\"\n",
    "  - \"-172_ctrl\"\n",
    "  - \"-171_ctrl\"\n",
    "  - \"-170_ctrl\"\n",
    "  - \"-169_ctrl\"\n",
    "  - \"-168_ctrl\"\n",
    "  - \"-167_ctrl\"\n",
    "  - \"-166_ctrl\"\n",
    "  - \"-165_ctrl\"\n",
    "  - \"-164_ctrl\"\n",
    "  - \"-163_ctrl\"\n",
    "  - \"-162_ctrl\"\n",
    "  - \"-161_ctrl\"\n",
    "  - \"-160_ctrl\"\n",
    "  - \"-159_ctrl\"\n",
    "  - \"-158_ctrl\"\n",
    "  - \"-157_ctrl\"\n",
    "  - \"-156_ctrl\"\n",
    "  - \"-155_ctrl\"\n",
    "  - \"-154_ctrl\"\n",
    "  - \"-153_ctrl\"\n",
    "  - \"-152_ctrl\"\n",
    "  - \"-151_ctrl\"\n",
    "  - \"-150_ctrl\"\n",
    "  - \"-149_ctrl\"\n",
    "  - \"-148_ctrl\"\n",
    "  - \"-147_ctrl\"\n",
    "  - \"-146_ctrl\"\n",
    "  - \"-145_ctrl\"\n",
    "  - \"-144_ctrl\"\n",
    "  - \"-143_ctrl\"\n",
    "  - \"-142_ctrl\"\n",
    "  - \"-141_ctrl\"\n",
    "  - \"-140_ctrl\"\n",
    "  - \"-139_ctrl\"\n",
    "  - \"-138_ctrl\"\n",
    "  - \"-137_ctrl\"\n",
    "  - \"-136_ctrl\"\n",
    "  - \"-135_ctrl\"\n",
    "  - \"-134_ctrl\"\n",
    "  - \"-133_ctrl\"\n",
    "  - \"-132_ctrl\"\n",
    "  - \"-131_ctrl\"\n",
    "  - \"-130_ctrl\"\n",
    "  - \"-129_ctrl\"\n",
    "  - \"-128_ctrl\"\n",
    "  - \"-127_ctrl\"\n",
    "  - \"-126_ctrl\"\n",
    "  - \"-125_ctrl\"\n",
    "  - \"-124_ctrl\"\n",
    "  - \"-123_ctrl\"\n",
    "  - \"-122_ctrl\"\n",
    "  - \"-121_ctrl\"\n",
    "  - \"-120_ctrl\"\n",
    "  - \"-119_ctrl\"\n",
    "  - \"-118_ctrl\"\n",
    "  - \"-117_ctrl\"\n",
    "  - \"-116_ctrl\"\n",
    "  - \"-115_ctrl\"\n",
    "  - \"-114_ctrl\"\n",
    "  - \"-113_ctrl\"\n",
    "  - \"-112_ctrl\"\n",
    "  - \"-111_ctrl\"\n",
    "  - \"-110_ctrl\"\n",
    "  - \"-109_ctrl\"\n",
    "  - \"-108_ctrl\"\n",
    "  - \"-107_ctrl\"\n",
    "  - \"-106_ctrl\"\n",
    "  - \"-105_ctrl\"\n",
    "  - \"-104_ctrl\"\n",
    "  - \"-103_ctrl\"\n",
    "  - \"-102_ctrl\"\n",
    "  - \"-101_ctrl\"\n",
    "  - \"-100_ctrl\"\n",
    "  - \"-99_ctrl\"\n",
    "  - \"-98_ctrl\"\n",
    "  - \"-97_ctrl\"\n",
    "  - \"-96_ctrl\"\n",
    "  - \"-95_ctrl\"\n",
    "  - \"-94_ctrl\"\n",
    "  - \"-93_ctrl\"\n",
    "  - \"-92_ctrl\"\n",
    "  - \"-91_ctrl\"\n",
    "  - \"-90_ctrl\"\n",
    "  - \"-89_ctrl\"\n",
    "  - \"-88_ctrl\"\n",
    "  - \"-87_ctrl\"\n",
    "  - \"-86_ctrl\"\n",
    "  - \"-85_ctrl\"\n",
    "  - \"-84_ctrl\"\n",
    "  - \"-83_ctrl\"\n",
    "  - \"-82_ctrl\"\n",
    "  - \"-81_ctrl\"\n",
    "  - \"-80_ctrl\"\n",
    "  - \"-79_ctrl\"\n",
    "  - \"-78_ctrl\"\n",
    "  - \"-77_ctrl\"\n",
    "  - \"-76_ctrl\"\n",
    "  - \"-75_ctrl\"\n",
    "  - \"-74_ctrl\"\n",
    "  - \"-73_ctrl\"\n",
    "  - \"-72_ctrl\"\n",
    "  - \"-71_ctrl\"\n",
    "  - \"-70_ctrl\"\n",
    "  - \"-69_ctrl\"\n",
    "  - \"-68_ctrl\"\n",
    "  - \"-67_ctrl\"\n",
    "  - \"-66_ctrl\"\n",
    "  - \"-65_ctrl\"\n",
    "  - \"-64_ctrl\"\n",
    "  - \"-63_ctrl\"\n",
    "  - \"-62_ctrl\"\n",
    "  - \"-61_ctrl\"\n",
    "  - \"-60_ctrl\"\n",
    "  - \"-59_ctrl\"\n",
    "  - \"-58_ctrl\"\n",
    "  - \"-57_ctrl\"\n",
    "  - \"-56_ctrl\"\n",
    "  - \"-55_ctrl\"\n",
    "  - \"-54_ctrl\"\n",
    "  - \"-53_ctrl\"\n",
    "  - \"-52_ctrl\"\n",
    "  - \"-51_ctrl\"\n",
    "  - \"-50_ctrl\"\n",
    "  - \"-49_ctrl\"\n",
    "  - \"-48_ctrl\"\n",
    "  - \"-47_ctrl\"\n",
    "  - \"-46_ctrl\"\n",
    "  - \"-45_ctrl\"\n",
    "  - \"-44_ctrl\"\n",
    "  - \"-43_ctrl\"\n",
    "  - \"-42_ctrl\"\n",
    "  - \"-41_ctrl\"\n",
    "  - \"-40_ctrl\"\n",
    "  - \"-39_ctrl\"\n",
    "  - \"-38_ctrl\"\n",
    "  - \"-37_ctrl\"\n",
    "  - \"-36_ctrl\"\n",
    "  - \"-35_ctrl\"\n",
    "  - \"-34_ctrl\"\n",
    "  - \"-33_ctrl\"\n",
    "  - \"-32_ctrl\"\n",
    "  - \"-31_ctrl\"\n",
    "  - \"-30_ctrl\"\n",
    "  - \"-29_ctrl\"\n",
    "  - \"-28_ctrl\"\n",
    "  - \"-27_ctrl\"\n",
    "  - \"-26_ctrl\"\n",
    "  - \"-25_ctrl\"\n",
    "  - \"-24_ctrl\"\n",
    "  - \"-23_ctrl\"\n",
    "  - \"-22_ctrl\"\n",
    "  - \"-21_ctrl\"\n",
    "  - \"-20_ctrl\"\n",
    "  - \"-19_ctrl\"\n",
    "  - \"-18_ctrl\"\n",
    "  - \"-17_ctrl\"\n",
    "  - \"-16_ctrl\"\n",
    "  - \"-15_ctrl\"\n",
    "  - \"-14_ctrl\"\n",
    "  - \"-13_ctrl\"\n",
    "  - \"-12_ctrl\"\n",
    "  - \"-11_ctrl\"\n",
    "  - \"-10_ctrl\"\n",
    "  - \"-9_ctrl\"\n",
    "  - \"-8_ctrl\"\n",
    "  - \"-7_ctrl\"\n",
    "  - \"-6_ctrl\"\n",
    "  - \"-5_ctrl\"\n",
    "  - \"-4_ctrl\"\n",
    "  - \"-3_ctrl\"\n",
    "  - \"-2_ctrl\"\n",
    "  - \"-1_ctrl\"\n",
    "  - \"0_ctrl\"\n",
    "  - \"1_ctrl\"\n",
    "  - \"2_ctrl\"\n",
    "  - \"3_ctrl\"\n",
    "  - \"4_ctrl\"\n",
    "  - \"5_ctrl\"\n",
    "  - \"6_ctrl\"\n",
    "  - \"7_ctrl\"\n",
    "  - \"8_ctrl\"\n",
    "  - \"9_ctrl\"\n",
    "  - \"10_ctrl\"\n",
    "  - \"11_ctrl\"\n",
    "  - \"12_ctrl\"\n",
    "  - \"13_ctrl\"\n",
    "  - \"14_ctrl\"\n",
    "  - \"15_ctrl\"\n",
    "  - \"16_ctrl\"\n",
    "  - \"17_ctrl\"\n",
    "  - \"18_ctrl\"\n",
    "  - \"19_ctrl\"\n",
    "  - \"20_ctrl\"\n",
    "  - \"21_ctrl\"\n",
    "  - \"22_ctrl\"\n",
    "  - \"23_ctrl\"\n",
    "  - \"24_ctrl\"\n",
    "  - \"25_ctrl\"\n",
    "  - \"26_ctrl\"\n",
    "  - \"27_ctrl\"\n",
    "  - \"28_ctrl\"\n",
    "  - \"29_ctrl\"\n",
    "  - \"30_ctrl\"\n",
    "  - \"31_ctrl\"\n",
    "  - \"32_ctrl\"\n",
    "  - \"33_ctrl\"\n",
    "  - \"34_ctrl\"\n",
    "  - \"35_ctrl\"\n",
    "  - \"36_ctrl\"\n",
    "  - \"37_ctrl\"\n",
    "  - \"38_ctrl\"\n",
    "  - \"39_ctrl\"\n",
    "  - \"40_ctrl\"\n",
    "  - \"41_ctrl\"\n",
    "  - \"42_ctrl\"\n",
    "  - \"43_ctrl\"\n",
    "  - \"44_ctrl\"\n",
    "  - \"45_ctrl\"\n",
    "  - \"46_ctrl\"\n",
    "  - \"47_ctrl\"\n",
    "  - \"48_ctrl\"\n",
    "  - \"49_ctrl\"\n",
    "  - \"50_ctrl\"\n",
    "  - \"51_ctrl\"\n",
    "  - \"52_ctrl\"\n",
    "  - \"53_ctrl\"\n",
    "  - \"54_ctrl\"\n",
    "  - \"55_ctrl\"\n",
    "  - \"56_ctrl\"\n",
    "  - \"57_ctrl\"\n",
    "  - \"58_ctrl\"\n",
    "  - \"59_ctrl\"\n",
    "  - \"60_ctrl\"\n",
    "  - \"61_ctrl\"\n",
    "  - \"62_ctrl\"\n",
    "  - \"63_ctrl\"\n",
    "  - \"64_ctrl\"\n",
    "  - \"65_ctrl\"\n",
    "  - \"66_ctrl\"\n",
    "  - \"67_ctrl\"\n",
    "  - \"68_ctrl\"\n",
    "  - \"69_ctrl\"\n",
    "  - \"70_ctrl\"\n",
    "  - \"71_ctrl\"\n",
    "  - \"72_ctrl\"\n",
    "  - \"73_ctrl\"\n",
    "  - \"74_ctrl\"\n",
    "  - \"75_ctrl\"\n",
    "  - \"76_ctrl\"\n",
    "  - \"77_ctrl\"\n",
    "  - \"78_ctrl\"\n",
    "  - \"79_ctrl\"\n",
    "  - \"80_ctrl\"\n",
    "  - \"81_ctrl\"\n",
    "  - \"82_ctrl\"\n",
    "  - \"83_ctrl\"\n",
    "  - \"84_ctrl\"\n",
    "  - \"85_ctrl\"\n",
    "  - \"86_ctrl\"\n",
    "  - \"87_ctrl\"\n",
    "  - \"88_ctrl\"\n",
    "  - \"89_ctrl\"\n",
    "  - \"90_ctrl\"\n",
    "  - \"91_ctrl\"\n",
    "  - \"92_ctrl\"\n",
    "  - \"93_ctrl\"\n",
    "  - \"94_ctrl\"\n",
    "  - \"95_ctrl\"\n",
    "  - \"96_ctrl\"\n",
    "  - \"97_ctrl\"\n",
    "  - \"98_ctrl\"\n",
    "  - \"99_ctrl\"\n",
    "  - \"100_ctrl\"\n",
    "  - \"101_ctrl\"\n",
    "  - \"102_ctrl\"\n",
    "  - \"103_ctrl\"\n",
    "  - \"104_ctrl\"\n",
    "  - \"105_ctrl\"\n",
    "  - \"106_ctrl\"\n",
    "  - \"107_ctrl\"\n",
    "  - \"108_ctrl\"\n",
    "  - \"109_ctrl\"\n",
    "  - \"110_ctrl\"\n",
    "  - \"111_ctrl\"\n",
    "  - \"112_ctrl\"\n",
    "  - \"113_ctrl\"\n",
    "  - \"114_ctrl\"\n",
    "  - \"115_ctrl\"\n",
    "  - \"116_ctrl\"\n",
    "  - \"117_ctrl\"\n",
    "  - \"118_ctrl\"\n",
    "  - \"119_ctrl\"\n",
    "  - \"120_ctrl\"\n",
    "  - \"121_ctrl\"\n",
    "  - \"122_ctrl\"\n",
    "  - \"123_ctrl\"\n",
    "  - \"124_ctrl\"\n",
    "  - \"125_ctrl\"\n",
    "  - \"126_ctrl\"\n",
    "  - \"127_ctrl\"\n",
    "  - \"128_ctrl\"\n",
    "  - \"129_ctrl\"\n",
    "  - \"130_ctrl\"\n",
    "  - \"131_ctrl\"\n",
    "  - \"132_ctrl\"\n",
    "  - \"133_ctrl\"\n",
    "  - \"134_ctrl\"\n",
    "  - \"135_ctrl\"\n",
    "  - \"136_ctrl\"\n",
    "  - \"137_ctrl\"\n",
    "  - \"138_ctrl\"\n",
    "  - \"139_ctrl\"\n",
    "  - \"140_ctrl\"\n",
    "  - \"141_ctrl\"\n",
    "  - \"142_ctrl\"\n",
    "  - \"143_ctrl\"\n",
    "  - \"144_ctrl\"\n",
    "  - \"145_ctrl\"\n",
    "  - \"146_ctrl\"\n",
    "  - \"147_ctrl\"\n",
    "  - \"148_ctrl\"\n",
    "  - \"149_ctrl\"\n",
    "  - \"150_ctrl\"\n",
    "  - \"151_ctrl\"\n",
    "  - \"152_ctrl\"\n",
    "  - \"153_ctrl\"\n",
    "  - \"154_ctrl\"\n",
    "  - \"155_ctrl\"\n",
    "  - \"156_ctrl\"\n",
    "  - \"157_ctrl\"\n",
    "  - \"158_ctrl\"\n",
    "  - \"159_ctrl\"\n",
    "  - \"160_ctrl\"\n",
    "  - \"161_ctrl\"\n",
    "  - \"162_ctrl\"\n",
    "  - \"163_ctrl\"\n",
    "  - \"164_ctrl\"\n",
    "  - \"165_ctrl\"\n",
    "  - \"166_ctrl\"\n",
    "  - \"167_ctrl\"\n",
    "  - \"168_ctrl\"\n",
    "  - \"169_ctrl\"\n",
    "  - \"170_ctrl\"\n",
    "  - \"171_ctrl\"\n",
    "  - \"172_ctrl\"\n",
    "  - \"173_ctrl\"\n",
    "  - \"174_ctrl\"\n",
    "  - \"175_ctrl\"\n",
    "  - \"176_ctrl\"\n",
    "  - \"177_ctrl\"\n",
    "  - \"178_ctrl\"\n",
    "  - \"179_ctrl\"\n",
    "  - \"180_ctrl\"\n",
    "  - \"181_ctrl\"\n",
    "  - \"182_ctrl\"\n",
    "  - \"183_ctrl\"\n",
    "  - \"184_ctrl\"\n",
    "  - \"185_ctrl\"\n",
    "  - \"186_ctrl\"\n",
    "  - \"187_ctrl\"\n",
    "  - \"188_ctrl\"\n",
    "  - \"189_ctrl\"\n",
    "  - \"190_ctrl\"\n",
    "  - \"191_ctrl\"\n",
    "  - \"192_ctrl\"\n",
    "  - \"193_ctrl\"\n",
    "  - \"194_ctrl\"\n",
    "  - \"195_ctrl\"\n",
    "  - \"196_ctrl\"\n",
    "  - \"197_ctrl\"\n",
    "  - \"198_ctrl\"\n",
    "  - \"199_ctrl\"\n",
    "  - \"200_ctrl\"\n",
    "  \"\"\"\n",
    "}\n",
    "\n",
    "test_chr19_holdout_yaml_contents = {\"fusion.yaml\":\"\"\"  \n",
    "model_config:\n",
    "  fc_do: 0.4\n",
    "  fc_task_dim: 256\n",
    "  layers:\n",
    "  - 2\n",
    "  rb_do: 0.4\n",
    "  stochastic_depth_p: 0.5\n",
    "model_type: \"mlp-residual\"\n",
    "\"\"\",\n",
    "\"globals.yaml\": \"\"\"\n",
    "\n",
    "checkpoint_interval: 30300 # 100 epochs #60000\n",
    "sample_interval: 30300 #60000\n",
    "n_epochs: 120\n",
    "batch_size: 64\n",
    "optimizer: \"adamw\"\n",
    "lr: 0.0001 #0.0001\n",
    "device: \"cuda\"\n",
    "compute_attributions: true\n",
    "\n",
    "\"\"\",\n",
    "\"input_cnn.yaml\": \"\"\"\n",
    "input_info:\n",
    "  input_source: ./inputs/landscape_arrays/test_chr19/  #inserted_enhancer_test #inputs/silenced_arrays/silenced_arrays_H2B_S.D \n",
    "  #./data/parsed_data/inputs/arrays_train_100bp_no_H3K27ac/ #arrays_train_100bp_no_H3K27ac_uncoupled/\n",
    "  input_name: gene_expression\n",
    "  input_type: array\n",
    "\n",
    "model_config:\n",
    "  model_type: cnn\n",
    "  #pre_normalization: \"instancenorm\"\n",
    "  model_init_config:\n",
    "    num_output_features: 512 # before fc_repr_dim\n",
    "    layers: [4,4]\n",
    "    kernel_height: 1\n",
    "    down_stride_width: 2\n",
    "    first_stride_expansion_width: 1\n",
    "    first_kernel_expansion_height: 4 #5\n",
    "    kernel_width: 10  #10\n",
    "    dilation_factor_width: 2\n",
    "    dilation_factor_height: 1\n",
    "    channel_exp_base: 5  #3 #-1\n",
    "    first_channel_expansion: 2\n",
    "    rb_do: .3\n",
    "    stochastic_depth_p: .1\n",
    "    attention_inclusion_cutoff: 1 #50 #256\n",
    "    \"\"\",\n",
    "\"outputs_2_cond.yaml\": \"\"\"\n",
    "output_info:\n",
    "  output_name: expression_output\n",
    "  output_source: ./targets/test_targets_chr19.csv \n",
    "  #./data/parsed_data/targets/target_arrays_perturbational_inserted_enhancer.csv #target_arrays_perturbational_S.D.csv #target_arrays_1kbp_401_bins_2_conditions_decareads_abs.csv\n",
    "  output_type: tabular\n",
    "\n",
    "model_config: # <- new\n",
    "  model_type: linear # <- new\n",
    "\n",
    "output_type_info:\n",
    "  con_loss_name: \"SmoothL1Loss\"\n",
    "  target_con_columns:\n",
    "  - \"-200_ctrl\"\n",
    "  - \"-199_ctrl\"\n",
    "  - \"-198_ctrl\"\n",
    "  - \"-197_ctrl\"\n",
    "  - \"-196_ctrl\"\n",
    "  - \"-195_ctrl\"\n",
    "  - \"-194_ctrl\"\n",
    "  - \"-193_ctrl\"\n",
    "  - \"-192_ctrl\"\n",
    "  - \"-191_ctrl\"\n",
    "  - \"-190_ctrl\"\n",
    "  - \"-189_ctrl\"\n",
    "  - \"-188_ctrl\"\n",
    "  - \"-187_ctrl\"\n",
    "  - \"-186_ctrl\"\n",
    "  - \"-185_ctrl\"\n",
    "  - \"-184_ctrl\"\n",
    "  - \"-183_ctrl\"\n",
    "  - \"-182_ctrl\"\n",
    "  - \"-181_ctrl\"\n",
    "  - \"-180_ctrl\"\n",
    "  - \"-179_ctrl\"\n",
    "  - \"-178_ctrl\"\n",
    "  - \"-177_ctrl\"\n",
    "  - \"-176_ctrl\"\n",
    "  - \"-175_ctrl\"\n",
    "  - \"-174_ctrl\"\n",
    "  - \"-173_ctrl\"\n",
    "  - \"-172_ctrl\"\n",
    "  - \"-171_ctrl\"\n",
    "  - \"-170_ctrl\"\n",
    "  - \"-169_ctrl\"\n",
    "  - \"-168_ctrl\"\n",
    "  - \"-167_ctrl\"\n",
    "  - \"-166_ctrl\"\n",
    "  - \"-165_ctrl\"\n",
    "  - \"-164_ctrl\"\n",
    "  - \"-163_ctrl\"\n",
    "  - \"-162_ctrl\"\n",
    "  - \"-161_ctrl\"\n",
    "  - \"-160_ctrl\"\n",
    "  - \"-159_ctrl\"\n",
    "  - \"-158_ctrl\"\n",
    "  - \"-157_ctrl\"\n",
    "  - \"-156_ctrl\"\n",
    "  - \"-155_ctrl\"\n",
    "  - \"-154_ctrl\"\n",
    "  - \"-153_ctrl\"\n",
    "  - \"-152_ctrl\"\n",
    "  - \"-151_ctrl\"\n",
    "  - \"-150_ctrl\"\n",
    "  - \"-149_ctrl\"\n",
    "  - \"-148_ctrl\"\n",
    "  - \"-147_ctrl\"\n",
    "  - \"-146_ctrl\"\n",
    "  - \"-145_ctrl\"\n",
    "  - \"-144_ctrl\"\n",
    "  - \"-143_ctrl\"\n",
    "  - \"-142_ctrl\"\n",
    "  - \"-141_ctrl\"\n",
    "  - \"-140_ctrl\"\n",
    "  - \"-139_ctrl\"\n",
    "  - \"-138_ctrl\"\n",
    "  - \"-137_ctrl\"\n",
    "  - \"-136_ctrl\"\n",
    "  - \"-135_ctrl\"\n",
    "  - \"-134_ctrl\"\n",
    "  - \"-133_ctrl\"\n",
    "  - \"-132_ctrl\"\n",
    "  - \"-131_ctrl\"\n",
    "  - \"-130_ctrl\"\n",
    "  - \"-129_ctrl\"\n",
    "  - \"-128_ctrl\"\n",
    "  - \"-127_ctrl\"\n",
    "  - \"-126_ctrl\"\n",
    "  - \"-125_ctrl\"\n",
    "  - \"-124_ctrl\"\n",
    "  - \"-123_ctrl\"\n",
    "  - \"-122_ctrl\"\n",
    "  - \"-121_ctrl\"\n",
    "  - \"-120_ctrl\"\n",
    "  - \"-119_ctrl\"\n",
    "  - \"-118_ctrl\"\n",
    "  - \"-117_ctrl\"\n",
    "  - \"-116_ctrl\"\n",
    "  - \"-115_ctrl\"\n",
    "  - \"-114_ctrl\"\n",
    "  - \"-113_ctrl\"\n",
    "  - \"-112_ctrl\"\n",
    "  - \"-111_ctrl\"\n",
    "  - \"-110_ctrl\"\n",
    "  - \"-109_ctrl\"\n",
    "  - \"-108_ctrl\"\n",
    "  - \"-107_ctrl\"\n",
    "  - \"-106_ctrl\"\n",
    "  - \"-105_ctrl\"\n",
    "  - \"-104_ctrl\"\n",
    "  - \"-103_ctrl\"\n",
    "  - \"-102_ctrl\"\n",
    "  - \"-101_ctrl\"\n",
    "  - \"-100_ctrl\"\n",
    "  - \"-99_ctrl\"\n",
    "  - \"-98_ctrl\"\n",
    "  - \"-97_ctrl\"\n",
    "  - \"-96_ctrl\"\n",
    "  - \"-95_ctrl\"\n",
    "  - \"-94_ctrl\"\n",
    "  - \"-93_ctrl\"\n",
    "  - \"-92_ctrl\"\n",
    "  - \"-91_ctrl\"\n",
    "  - \"-90_ctrl\"\n",
    "  - \"-89_ctrl\"\n",
    "  - \"-88_ctrl\"\n",
    "  - \"-87_ctrl\"\n",
    "  - \"-86_ctrl\"\n",
    "  - \"-85_ctrl\"\n",
    "  - \"-84_ctrl\"\n",
    "  - \"-83_ctrl\"\n",
    "  - \"-82_ctrl\"\n",
    "  - \"-81_ctrl\"\n",
    "  - \"-80_ctrl\"\n",
    "  - \"-79_ctrl\"\n",
    "  - \"-78_ctrl\"\n",
    "  - \"-77_ctrl\"\n",
    "  - \"-76_ctrl\"\n",
    "  - \"-75_ctrl\"\n",
    "  - \"-74_ctrl\"\n",
    "  - \"-73_ctrl\"\n",
    "  - \"-72_ctrl\"\n",
    "  - \"-71_ctrl\"\n",
    "  - \"-70_ctrl\"\n",
    "  - \"-69_ctrl\"\n",
    "  - \"-68_ctrl\"\n",
    "  - \"-67_ctrl\"\n",
    "  - \"-66_ctrl\"\n",
    "  - \"-65_ctrl\"\n",
    "  - \"-64_ctrl\"\n",
    "  - \"-63_ctrl\"\n",
    "  - \"-62_ctrl\"\n",
    "  - \"-61_ctrl\"\n",
    "  - \"-60_ctrl\"\n",
    "  - \"-59_ctrl\"\n",
    "  - \"-58_ctrl\"\n",
    "  - \"-57_ctrl\"\n",
    "  - \"-56_ctrl\"\n",
    "  - \"-55_ctrl\"\n",
    "  - \"-54_ctrl\"\n",
    "  - \"-53_ctrl\"\n",
    "  - \"-52_ctrl\"\n",
    "  - \"-51_ctrl\"\n",
    "  - \"-50_ctrl\"\n",
    "  - \"-49_ctrl\"\n",
    "  - \"-48_ctrl\"\n",
    "  - \"-47_ctrl\"\n",
    "  - \"-46_ctrl\"\n",
    "  - \"-45_ctrl\"\n",
    "  - \"-44_ctrl\"\n",
    "  - \"-43_ctrl\"\n",
    "  - \"-42_ctrl\"\n",
    "  - \"-41_ctrl\"\n",
    "  - \"-40_ctrl\"\n",
    "  - \"-39_ctrl\"\n",
    "  - \"-38_ctrl\"\n",
    "  - \"-37_ctrl\"\n",
    "  - \"-36_ctrl\"\n",
    "  - \"-35_ctrl\"\n",
    "  - \"-34_ctrl\"\n",
    "  - \"-33_ctrl\"\n",
    "  - \"-32_ctrl\"\n",
    "  - \"-31_ctrl\"\n",
    "  - \"-30_ctrl\"\n",
    "  - \"-29_ctrl\"\n",
    "  - \"-28_ctrl\"\n",
    "  - \"-27_ctrl\"\n",
    "  - \"-26_ctrl\"\n",
    "  - \"-25_ctrl\"\n",
    "  - \"-24_ctrl\"\n",
    "  - \"-23_ctrl\"\n",
    "  - \"-22_ctrl\"\n",
    "  - \"-21_ctrl\"\n",
    "  - \"-20_ctrl\"\n",
    "  - \"-19_ctrl\"\n",
    "  - \"-18_ctrl\"\n",
    "  - \"-17_ctrl\"\n",
    "  - \"-16_ctrl\"\n",
    "  - \"-15_ctrl\"\n",
    "  - \"-14_ctrl\"\n",
    "  - \"-13_ctrl\"\n",
    "  - \"-12_ctrl\"\n",
    "  - \"-11_ctrl\"\n",
    "  - \"-10_ctrl\"\n",
    "  - \"-9_ctrl\"\n",
    "  - \"-8_ctrl\"\n",
    "  - \"-7_ctrl\"\n",
    "  - \"-6_ctrl\"\n",
    "  - \"-5_ctrl\"\n",
    "  - \"-4_ctrl\"\n",
    "  - \"-3_ctrl\"\n",
    "  - \"-2_ctrl\"\n",
    "  - \"-1_ctrl\"\n",
    "  - \"0_ctrl\"\n",
    "  - \"1_ctrl\"\n",
    "  - \"2_ctrl\"\n",
    "  - \"3_ctrl\"\n",
    "  - \"4_ctrl\"\n",
    "  - \"5_ctrl\"\n",
    "  - \"6_ctrl\"\n",
    "  - \"7_ctrl\"\n",
    "  - \"8_ctrl\"\n",
    "  - \"9_ctrl\"\n",
    "  - \"10_ctrl\"\n",
    "  - \"11_ctrl\"\n",
    "  - \"12_ctrl\"\n",
    "  - \"13_ctrl\"\n",
    "  - \"14_ctrl\"\n",
    "  - \"15_ctrl\"\n",
    "  - \"16_ctrl\"\n",
    "  - \"17_ctrl\"\n",
    "  - \"18_ctrl\"\n",
    "  - \"19_ctrl\"\n",
    "  - \"20_ctrl\"\n",
    "  - \"21_ctrl\"\n",
    "  - \"22_ctrl\"\n",
    "  - \"23_ctrl\"\n",
    "  - \"24_ctrl\"\n",
    "  - \"25_ctrl\"\n",
    "  - \"26_ctrl\"\n",
    "  - \"27_ctrl\"\n",
    "  - \"28_ctrl\"\n",
    "  - \"29_ctrl\"\n",
    "  - \"30_ctrl\"\n",
    "  - \"31_ctrl\"\n",
    "  - \"32_ctrl\"\n",
    "  - \"33_ctrl\"\n",
    "  - \"34_ctrl\"\n",
    "  - \"35_ctrl\"\n",
    "  - \"36_ctrl\"\n",
    "  - \"37_ctrl\"\n",
    "  - \"38_ctrl\"\n",
    "  - \"39_ctrl\"\n",
    "  - \"40_ctrl\"\n",
    "  - \"41_ctrl\"\n",
    "  - \"42_ctrl\"\n",
    "  - \"43_ctrl\"\n",
    "  - \"44_ctrl\"\n",
    "  - \"45_ctrl\"\n",
    "  - \"46_ctrl\"\n",
    "  - \"47_ctrl\"\n",
    "  - \"48_ctrl\"\n",
    "  - \"49_ctrl\"\n",
    "  - \"50_ctrl\"\n",
    "  - \"51_ctrl\"\n",
    "  - \"52_ctrl\"\n",
    "  - \"53_ctrl\"\n",
    "  - \"54_ctrl\"\n",
    "  - \"55_ctrl\"\n",
    "  - \"56_ctrl\"\n",
    "  - \"57_ctrl\"\n",
    "  - \"58_ctrl\"\n",
    "  - \"59_ctrl\"\n",
    "  - \"60_ctrl\"\n",
    "  - \"61_ctrl\"\n",
    "  - \"62_ctrl\"\n",
    "  - \"63_ctrl\"\n",
    "  - \"64_ctrl\"\n",
    "  - \"65_ctrl\"\n",
    "  - \"66_ctrl\"\n",
    "  - \"67_ctrl\"\n",
    "  - \"68_ctrl\"\n",
    "  - \"69_ctrl\"\n",
    "  - \"70_ctrl\"\n",
    "  - \"71_ctrl\"\n",
    "  - \"72_ctrl\"\n",
    "  - \"73_ctrl\"\n",
    "  - \"74_ctrl\"\n",
    "  - \"75_ctrl\"\n",
    "  - \"76_ctrl\"\n",
    "  - \"77_ctrl\"\n",
    "  - \"78_ctrl\"\n",
    "  - \"79_ctrl\"\n",
    "  - \"80_ctrl\"\n",
    "  - \"81_ctrl\"\n",
    "  - \"82_ctrl\"\n",
    "  - \"83_ctrl\"\n",
    "  - \"84_ctrl\"\n",
    "  - \"85_ctrl\"\n",
    "  - \"86_ctrl\"\n",
    "  - \"87_ctrl\"\n",
    "  - \"88_ctrl\"\n",
    "  - \"89_ctrl\"\n",
    "  - \"90_ctrl\"\n",
    "  - \"91_ctrl\"\n",
    "  - \"92_ctrl\"\n",
    "  - \"93_ctrl\"\n",
    "  - \"94_ctrl\"\n",
    "  - \"95_ctrl\"\n",
    "  - \"96_ctrl\"\n",
    "  - \"97_ctrl\"\n",
    "  - \"98_ctrl\"\n",
    "  - \"99_ctrl\"\n",
    "  - \"100_ctrl\"\n",
    "  - \"101_ctrl\"\n",
    "  - \"102_ctrl\"\n",
    "  - \"103_ctrl\"\n",
    "  - \"104_ctrl\"\n",
    "  - \"105_ctrl\"\n",
    "  - \"106_ctrl\"\n",
    "  - \"107_ctrl\"\n",
    "  - \"108_ctrl\"\n",
    "  - \"109_ctrl\"\n",
    "  - \"110_ctrl\"\n",
    "  - \"111_ctrl\"\n",
    "  - \"112_ctrl\"\n",
    "  - \"113_ctrl\"\n",
    "  - \"114_ctrl\"\n",
    "  - \"115_ctrl\"\n",
    "  - \"116_ctrl\"\n",
    "  - \"117_ctrl\"\n",
    "  - \"118_ctrl\"\n",
    "  - \"119_ctrl\"\n",
    "  - \"120_ctrl\"\n",
    "  - \"121_ctrl\"\n",
    "  - \"122_ctrl\"\n",
    "  - \"123_ctrl\"\n",
    "  - \"124_ctrl\"\n",
    "  - \"125_ctrl\"\n",
    "  - \"126_ctrl\"\n",
    "  - \"127_ctrl\"\n",
    "  - \"128_ctrl\"\n",
    "  - \"129_ctrl\"\n",
    "  - \"130_ctrl\"\n",
    "  - \"131_ctrl\"\n",
    "  - \"132_ctrl\"\n",
    "  - \"133_ctrl\"\n",
    "  - \"134_ctrl\"\n",
    "  - \"135_ctrl\"\n",
    "  - \"136_ctrl\"\n",
    "  - \"137_ctrl\"\n",
    "  - \"138_ctrl\"\n",
    "  - \"139_ctrl\"\n",
    "  - \"140_ctrl\"\n",
    "  - \"141_ctrl\"\n",
    "  - \"142_ctrl\"\n",
    "  - \"143_ctrl\"\n",
    "  - \"144_ctrl\"\n",
    "  - \"145_ctrl\"\n",
    "  - \"146_ctrl\"\n",
    "  - \"147_ctrl\"\n",
    "  - \"148_ctrl\"\n",
    "  - \"149_ctrl\"\n",
    "  - \"150_ctrl\"\n",
    "  - \"151_ctrl\"\n",
    "  - \"152_ctrl\"\n",
    "  - \"153_ctrl\"\n",
    "  - \"154_ctrl\"\n",
    "  - \"155_ctrl\"\n",
    "  - \"156_ctrl\"\n",
    "  - \"157_ctrl\"\n",
    "  - \"158_ctrl\"\n",
    "  - \"159_ctrl\"\n",
    "  - \"160_ctrl\"\n",
    "  - \"161_ctrl\"\n",
    "  - \"162_ctrl\"\n",
    "  - \"163_ctrl\"\n",
    "  - \"164_ctrl\"\n",
    "  - \"165_ctrl\"\n",
    "  - \"166_ctrl\"\n",
    "  - \"167_ctrl\"\n",
    "  - \"168_ctrl\"\n",
    "  - \"169_ctrl\"\n",
    "  - \"170_ctrl\"\n",
    "  - \"171_ctrl\"\n",
    "  - \"172_ctrl\"\n",
    "  - \"173_ctrl\"\n",
    "  - \"174_ctrl\"\n",
    "  - \"175_ctrl\"\n",
    "  - \"176_ctrl\"\n",
    "  - \"177_ctrl\"\n",
    "  - \"178_ctrl\"\n",
    "  - \"179_ctrl\"\n",
    "  - \"180_ctrl\"\n",
    "  - \"181_ctrl\"\n",
    "  - \"182_ctrl\"\n",
    "  - \"183_ctrl\"\n",
    "  - \"184_ctrl\"\n",
    "  - \"185_ctrl\"\n",
    "  - \"186_ctrl\"\n",
    "  - \"187_ctrl\"\n",
    "  - \"188_ctrl\"\n",
    "  - \"189_ctrl\"\n",
    "  - \"190_ctrl\"\n",
    "  - \"191_ctrl\"\n",
    "  - \"192_ctrl\"\n",
    "  - \"193_ctrl\"\n",
    "  - \"194_ctrl\"\n",
    "  - \"195_ctrl\"\n",
    "  - \"196_ctrl\"\n",
    "  - \"197_ctrl\"\n",
    "  - \"198_ctrl\"\n",
    "  - \"199_ctrl\"\n",
    "  - \"200_ctrl\"\n",
    "  \"\"\" }\n",
    "\n",
    "\n",
    "training_no_H3K27ac_contents = {\"globals.yaml\":\"\"\"\n",
    "output_folder: ./runs/gene_expression_only_chrom_pure_conv_no_H3K27ac/ \n",
    "manual_valid_ids_file: ./annotations/manual_validation_ids_chr17.txt  #manual_validation_ids_chr17_uncoupled.txt\n",
    "checkpoint_interval: 30300 # 100 epochs #60000\n",
    "sample_interval: 30300 #60000\n",
    "n_epochs: 120\n",
    "batch_size: 64\n",
    "optimizer: \"adamw\"\n",
    "lr: 0.0001 #0.0001\n",
    "device: \"cuda\"\n",
    "compute_attributions: false\n",
    "# latent_sampling: \n",
    "#   layers_to_sample:\n",
    "#     - \"input_modules.contact_maps.feature_extractor.conv.0.conv_1\"\n",
    "#     - \"input_modules.contact_maps.feature_extractor.conv.1.conv_2\"\n",
    "#     - \"input_modules.contact_maps.feature_extractor.conv.2.conv_2\"\n",
    "#     - \"input_modules.contact_maps.feature_extractor.conv.3.conv_2\"\n",
    "# attribution_background_samples: 512\n",
    "# attributions_every_sample_factor: 1\n",
    "#pretrained_checkpoint: best_models/gene_expression_exformer_unlimited_chrom_and_micro_with_attention_model_117600_perf-average=0.8435.pt \n",
    "\n",
    "\"\"\",\n",
    "\"input_cnn.yaml\": \"\"\" \n",
    "input_info:\n",
    "  input_source: ./inputs/landscape_arrays/training_no_H3K27ac/ \n",
    "  input_name: gene_expression\n",
    "  input_type: array\n",
    "\n",
    "model_config:\n",
    "  model_type: cnn\n",
    "  #pre_normalization: \"instancenorm\"\n",
    "  model_init_config:\n",
    "    num_output_features: 512 # before fc_repr_dim\n",
    "    layers: [4,4]\n",
    "    kernel_height: 1\n",
    "    down_stride_width: 2\n",
    "    first_stride_expansion_width: 1\n",
    "    first_kernel_expansion_height: 3 #5\n",
    "    kernel_width: 10  #10\n",
    "    dilation_factor_width: 2\n",
    "    dilation_factor_height: 1\n",
    "    channel_exp_base: 5  #3 #-1\n",
    "    first_channel_expansion: 2\n",
    "    rb_do: .3\n",
    "    stochastic_depth_p: .1\n",
    "    attention_inclusion_cutoff: 1 #50 #256\n",
    "\n",
    "\"\"\",\n",
    "\"fusion.yaml\": \"\"\"\n",
    "model_config:\n",
    "  fc_do: 0.4\n",
    "  fc_task_dim: 256\n",
    "  layers:\n",
    "  - 2\n",
    "  rb_do: 0.4\n",
    "  stochastic_depth_p: 0.5\n",
    "model_type: \"mlp-residual\"\n",
    "\"\"\",\n",
    "\"outputs_2_cond.yaml\":\"\"\"\n",
    "output_info:\n",
    "  output_name: expression_output\n",
    "  output_source: ./targets/training_targets.csv \n",
    "  output_type: tabular\n",
    "\n",
    "model_config: # <- new\n",
    "  model_type: linear # <- new\n",
    "\n",
    "output_type_info:\n",
    "  con_loss_name: \"SmoothL1Loss\"\n",
    "  target_con_columns:\n",
    "  - \"-200_ctrl\"\n",
    "  - \"-199_ctrl\"\n",
    "  - \"-198_ctrl\"\n",
    "  - \"-197_ctrl\"\n",
    "  - \"-196_ctrl\"\n",
    "  - \"-195_ctrl\"\n",
    "  - \"-194_ctrl\"\n",
    "  - \"-193_ctrl\"\n",
    "  - \"-192_ctrl\"\n",
    "  - \"-191_ctrl\"\n",
    "  - \"-190_ctrl\"\n",
    "  - \"-189_ctrl\"\n",
    "  - \"-188_ctrl\"\n",
    "  - \"-187_ctrl\"\n",
    "  - \"-186_ctrl\"\n",
    "  - \"-185_ctrl\"\n",
    "  - \"-184_ctrl\"\n",
    "  - \"-183_ctrl\"\n",
    "  - \"-182_ctrl\"\n",
    "  - \"-181_ctrl\"\n",
    "  - \"-180_ctrl\"\n",
    "  - \"-179_ctrl\"\n",
    "  - \"-178_ctrl\"\n",
    "  - \"-177_ctrl\"\n",
    "  - \"-176_ctrl\"\n",
    "  - \"-175_ctrl\"\n",
    "  - \"-174_ctrl\"\n",
    "  - \"-173_ctrl\"\n",
    "  - \"-172_ctrl\"\n",
    "  - \"-171_ctrl\"\n",
    "  - \"-170_ctrl\"\n",
    "  - \"-169_ctrl\"\n",
    "  - \"-168_ctrl\"\n",
    "  - \"-167_ctrl\"\n",
    "  - \"-166_ctrl\"\n",
    "  - \"-165_ctrl\"\n",
    "  - \"-164_ctrl\"\n",
    "  - \"-163_ctrl\"\n",
    "  - \"-162_ctrl\"\n",
    "  - \"-161_ctrl\"\n",
    "  - \"-160_ctrl\"\n",
    "  - \"-159_ctrl\"\n",
    "  - \"-158_ctrl\"\n",
    "  - \"-157_ctrl\"\n",
    "  - \"-156_ctrl\"\n",
    "  - \"-155_ctrl\"\n",
    "  - \"-154_ctrl\"\n",
    "  - \"-153_ctrl\"\n",
    "  - \"-152_ctrl\"\n",
    "  - \"-151_ctrl\"\n",
    "  - \"-150_ctrl\"\n",
    "  - \"-149_ctrl\"\n",
    "  - \"-148_ctrl\"\n",
    "  - \"-147_ctrl\"\n",
    "  - \"-146_ctrl\"\n",
    "  - \"-145_ctrl\"\n",
    "  - \"-144_ctrl\"\n",
    "  - \"-143_ctrl\"\n",
    "  - \"-142_ctrl\"\n",
    "  - \"-141_ctrl\"\n",
    "  - \"-140_ctrl\"\n",
    "  - \"-139_ctrl\"\n",
    "  - \"-138_ctrl\"\n",
    "  - \"-137_ctrl\"\n",
    "  - \"-136_ctrl\"\n",
    "  - \"-135_ctrl\"\n",
    "  - \"-134_ctrl\"\n",
    "  - \"-133_ctrl\"\n",
    "  - \"-132_ctrl\"\n",
    "  - \"-131_ctrl\"\n",
    "  - \"-130_ctrl\"\n",
    "  - \"-129_ctrl\"\n",
    "  - \"-128_ctrl\"\n",
    "  - \"-127_ctrl\"\n",
    "  - \"-126_ctrl\"\n",
    "  - \"-125_ctrl\"\n",
    "  - \"-124_ctrl\"\n",
    "  - \"-123_ctrl\"\n",
    "  - \"-122_ctrl\"\n",
    "  - \"-121_ctrl\"\n",
    "  - \"-120_ctrl\"\n",
    "  - \"-119_ctrl\"\n",
    "  - \"-118_ctrl\"\n",
    "  - \"-117_ctrl\"\n",
    "  - \"-116_ctrl\"\n",
    "  - \"-115_ctrl\"\n",
    "  - \"-114_ctrl\"\n",
    "  - \"-113_ctrl\"\n",
    "  - \"-112_ctrl\"\n",
    "  - \"-111_ctrl\"\n",
    "  - \"-110_ctrl\"\n",
    "  - \"-109_ctrl\"\n",
    "  - \"-108_ctrl\"\n",
    "  - \"-107_ctrl\"\n",
    "  - \"-106_ctrl\"\n",
    "  - \"-105_ctrl\"\n",
    "  - \"-104_ctrl\"\n",
    "  - \"-103_ctrl\"\n",
    "  - \"-102_ctrl\"\n",
    "  - \"-101_ctrl\"\n",
    "  - \"-100_ctrl\"\n",
    "  - \"-99_ctrl\"\n",
    "  - \"-98_ctrl\"\n",
    "  - \"-97_ctrl\"\n",
    "  - \"-96_ctrl\"\n",
    "  - \"-95_ctrl\"\n",
    "  - \"-94_ctrl\"\n",
    "  - \"-93_ctrl\"\n",
    "  - \"-92_ctrl\"\n",
    "  - \"-91_ctrl\"\n",
    "  - \"-90_ctrl\"\n",
    "  - \"-89_ctrl\"\n",
    "  - \"-88_ctrl\"\n",
    "  - \"-87_ctrl\"\n",
    "  - \"-86_ctrl\"\n",
    "  - \"-85_ctrl\"\n",
    "  - \"-84_ctrl\"\n",
    "  - \"-83_ctrl\"\n",
    "  - \"-82_ctrl\"\n",
    "  - \"-81_ctrl\"\n",
    "  - \"-80_ctrl\"\n",
    "  - \"-79_ctrl\"\n",
    "  - \"-78_ctrl\"\n",
    "  - \"-77_ctrl\"\n",
    "  - \"-76_ctrl\"\n",
    "  - \"-75_ctrl\"\n",
    "  - \"-74_ctrl\"\n",
    "  - \"-73_ctrl\"\n",
    "  - \"-72_ctrl\"\n",
    "  - \"-71_ctrl\"\n",
    "  - \"-70_ctrl\"\n",
    "  - \"-69_ctrl\"\n",
    "  - \"-68_ctrl\"\n",
    "  - \"-67_ctrl\"\n",
    "  - \"-66_ctrl\"\n",
    "  - \"-65_ctrl\"\n",
    "  - \"-64_ctrl\"\n",
    "  - \"-63_ctrl\"\n",
    "  - \"-62_ctrl\"\n",
    "  - \"-61_ctrl\"\n",
    "  - \"-60_ctrl\"\n",
    "  - \"-59_ctrl\"\n",
    "  - \"-58_ctrl\"\n",
    "  - \"-57_ctrl\"\n",
    "  - \"-56_ctrl\"\n",
    "  - \"-55_ctrl\"\n",
    "  - \"-54_ctrl\"\n",
    "  - \"-53_ctrl\"\n",
    "  - \"-52_ctrl\"\n",
    "  - \"-51_ctrl\"\n",
    "  - \"-50_ctrl\"\n",
    "  - \"-49_ctrl\"\n",
    "  - \"-48_ctrl\"\n",
    "  - \"-47_ctrl\"\n",
    "  - \"-46_ctrl\"\n",
    "  - \"-45_ctrl\"\n",
    "  - \"-44_ctrl\"\n",
    "  - \"-43_ctrl\"\n",
    "  - \"-42_ctrl\"\n",
    "  - \"-41_ctrl\"\n",
    "  - \"-40_ctrl\"\n",
    "  - \"-39_ctrl\"\n",
    "  - \"-38_ctrl\"\n",
    "  - \"-37_ctrl\"\n",
    "  - \"-36_ctrl\"\n",
    "  - \"-35_ctrl\"\n",
    "  - \"-34_ctrl\"\n",
    "  - \"-33_ctrl\"\n",
    "  - \"-32_ctrl\"\n",
    "  - \"-31_ctrl\"\n",
    "  - \"-30_ctrl\"\n",
    "  - \"-29_ctrl\"\n",
    "  - \"-28_ctrl\"\n",
    "  - \"-27_ctrl\"\n",
    "  - \"-26_ctrl\"\n",
    "  - \"-25_ctrl\"\n",
    "  - \"-24_ctrl\"\n",
    "  - \"-23_ctrl\"\n",
    "  - \"-22_ctrl\"\n",
    "  - \"-21_ctrl\"\n",
    "  - \"-20_ctrl\"\n",
    "  - \"-19_ctrl\"\n",
    "  - \"-18_ctrl\"\n",
    "  - \"-17_ctrl\"\n",
    "  - \"-16_ctrl\"\n",
    "  - \"-15_ctrl\"\n",
    "  - \"-14_ctrl\"\n",
    "  - \"-13_ctrl\"\n",
    "  - \"-12_ctrl\"\n",
    "  - \"-11_ctrl\"\n",
    "  - \"-10_ctrl\"\n",
    "  - \"-9_ctrl\"\n",
    "  - \"-8_ctrl\"\n",
    "  - \"-7_ctrl\"\n",
    "  - \"-6_ctrl\"\n",
    "  - \"-5_ctrl\"\n",
    "  - \"-4_ctrl\"\n",
    "  - \"-3_ctrl\"\n",
    "  - \"-2_ctrl\"\n",
    "  - \"-1_ctrl\"\n",
    "  - \"0_ctrl\"\n",
    "  - \"1_ctrl\"\n",
    "  - \"2_ctrl\"\n",
    "  - \"3_ctrl\"\n",
    "  - \"4_ctrl\"\n",
    "  - \"5_ctrl\"\n",
    "  - \"6_ctrl\"\n",
    "  - \"7_ctrl\"\n",
    "  - \"8_ctrl\"\n",
    "  - \"9_ctrl\"\n",
    "  - \"10_ctrl\"\n",
    "  - \"11_ctrl\"\n",
    "  - \"12_ctrl\"\n",
    "  - \"13_ctrl\"\n",
    "  - \"14_ctrl\"\n",
    "  - \"15_ctrl\"\n",
    "  - \"16_ctrl\"\n",
    "  - \"17_ctrl\"\n",
    "  - \"18_ctrl\"\n",
    "  - \"19_ctrl\"\n",
    "  - \"20_ctrl\"\n",
    "  - \"21_ctrl\"\n",
    "  - \"22_ctrl\"\n",
    "  - \"23_ctrl\"\n",
    "  - \"24_ctrl\"\n",
    "  - \"25_ctrl\"\n",
    "  - \"26_ctrl\"\n",
    "  - \"27_ctrl\"\n",
    "  - \"28_ctrl\"\n",
    "  - \"29_ctrl\"\n",
    "  - \"30_ctrl\"\n",
    "  - \"31_ctrl\"\n",
    "  - \"32_ctrl\"\n",
    "  - \"33_ctrl\"\n",
    "  - \"34_ctrl\"\n",
    "  - \"35_ctrl\"\n",
    "  - \"36_ctrl\"\n",
    "  - \"37_ctrl\"\n",
    "  - \"38_ctrl\"\n",
    "  - \"39_ctrl\"\n",
    "  - \"40_ctrl\"\n",
    "  - \"41_ctrl\"\n",
    "  - \"42_ctrl\"\n",
    "  - \"43_ctrl\"\n",
    "  - \"44_ctrl\"\n",
    "  - \"45_ctrl\"\n",
    "  - \"46_ctrl\"\n",
    "  - \"47_ctrl\"\n",
    "  - \"48_ctrl\"\n",
    "  - \"49_ctrl\"\n",
    "  - \"50_ctrl\"\n",
    "  - \"51_ctrl\"\n",
    "  - \"52_ctrl\"\n",
    "  - \"53_ctrl\"\n",
    "  - \"54_ctrl\"\n",
    "  - \"55_ctrl\"\n",
    "  - \"56_ctrl\"\n",
    "  - \"57_ctrl\"\n",
    "  - \"58_ctrl\"\n",
    "  - \"59_ctrl\"\n",
    "  - \"60_ctrl\"\n",
    "  - \"61_ctrl\"\n",
    "  - \"62_ctrl\"\n",
    "  - \"63_ctrl\"\n",
    "  - \"64_ctrl\"\n",
    "  - \"65_ctrl\"\n",
    "  - \"66_ctrl\"\n",
    "  - \"67_ctrl\"\n",
    "  - \"68_ctrl\"\n",
    "  - \"69_ctrl\"\n",
    "  - \"70_ctrl\"\n",
    "  - \"71_ctrl\"\n",
    "  - \"72_ctrl\"\n",
    "  - \"73_ctrl\"\n",
    "  - \"74_ctrl\"\n",
    "  - \"75_ctrl\"\n",
    "  - \"76_ctrl\"\n",
    "  - \"77_ctrl\"\n",
    "  - \"78_ctrl\"\n",
    "  - \"79_ctrl\"\n",
    "  - \"80_ctrl\"\n",
    "  - \"81_ctrl\"\n",
    "  - \"82_ctrl\"\n",
    "  - \"83_ctrl\"\n",
    "  - \"84_ctrl\"\n",
    "  - \"85_ctrl\"\n",
    "  - \"86_ctrl\"\n",
    "  - \"87_ctrl\"\n",
    "  - \"88_ctrl\"\n",
    "  - \"89_ctrl\"\n",
    "  - \"90_ctrl\"\n",
    "  - \"91_ctrl\"\n",
    "  - \"92_ctrl\"\n",
    "  - \"93_ctrl\"\n",
    "  - \"94_ctrl\"\n",
    "  - \"95_ctrl\"\n",
    "  - \"96_ctrl\"\n",
    "  - \"97_ctrl\"\n",
    "  - \"98_ctrl\"\n",
    "  - \"99_ctrl\"\n",
    "  - \"100_ctrl\"\n",
    "  - \"101_ctrl\"\n",
    "  - \"102_ctrl\"\n",
    "  - \"103_ctrl\"\n",
    "  - \"104_ctrl\"\n",
    "  - \"105_ctrl\"\n",
    "  - \"106_ctrl\"\n",
    "  - \"107_ctrl\"\n",
    "  - \"108_ctrl\"\n",
    "  - \"109_ctrl\"\n",
    "  - \"110_ctrl\"\n",
    "  - \"111_ctrl\"\n",
    "  - \"112_ctrl\"\n",
    "  - \"113_ctrl\"\n",
    "  - \"114_ctrl\"\n",
    "  - \"115_ctrl\"\n",
    "  - \"116_ctrl\"\n",
    "  - \"117_ctrl\"\n",
    "  - \"118_ctrl\"\n",
    "  - \"119_ctrl\"\n",
    "  - \"120_ctrl\"\n",
    "  - \"121_ctrl\"\n",
    "  - \"122_ctrl\"\n",
    "  - \"123_ctrl\"\n",
    "  - \"124_ctrl\"\n",
    "  - \"125_ctrl\"\n",
    "  - \"126_ctrl\"\n",
    "  - \"127_ctrl\"\n",
    "  - \"128_ctrl\"\n",
    "  - \"129_ctrl\"\n",
    "  - \"130_ctrl\"\n",
    "  - \"131_ctrl\"\n",
    "  - \"132_ctrl\"\n",
    "  - \"133_ctrl\"\n",
    "  - \"134_ctrl\"\n",
    "  - \"135_ctrl\"\n",
    "  - \"136_ctrl\"\n",
    "  - \"137_ctrl\"\n",
    "  - \"138_ctrl\"\n",
    "  - \"139_ctrl\"\n",
    "  - \"140_ctrl\"\n",
    "  - \"141_ctrl\"\n",
    "  - \"142_ctrl\"\n",
    "  - \"143_ctrl\"\n",
    "  - \"144_ctrl\"\n",
    "  - \"145_ctrl\"\n",
    "  - \"146_ctrl\"\n",
    "  - \"147_ctrl\"\n",
    "  - \"148_ctrl\"\n",
    "  - \"149_ctrl\"\n",
    "  - \"150_ctrl\"\n",
    "  - \"151_ctrl\"\n",
    "  - \"152_ctrl\"\n",
    "  - \"153_ctrl\"\n",
    "  - \"154_ctrl\"\n",
    "  - \"155_ctrl\"\n",
    "  - \"156_ctrl\"\n",
    "  - \"157_ctrl\"\n",
    "  - \"158_ctrl\"\n",
    "  - \"159_ctrl\"\n",
    "  - \"160_ctrl\"\n",
    "  - \"161_ctrl\"\n",
    "  - \"162_ctrl\"\n",
    "  - \"163_ctrl\"\n",
    "  - \"164_ctrl\"\n",
    "  - \"165_ctrl\"\n",
    "  - \"166_ctrl\"\n",
    "  - \"167_ctrl\"\n",
    "  - \"168_ctrl\"\n",
    "  - \"169_ctrl\"\n",
    "  - \"170_ctrl\"\n",
    "  - \"171_ctrl\"\n",
    "  - \"172_ctrl\"\n",
    "  - \"173_ctrl\"\n",
    "  - \"174_ctrl\"\n",
    "  - \"175_ctrl\"\n",
    "  - \"176_ctrl\"\n",
    "  - \"177_ctrl\"\n",
    "  - \"178_ctrl\"\n",
    "  - \"179_ctrl\"\n",
    "  - \"180_ctrl\"\n",
    "  - \"181_ctrl\"\n",
    "  - \"182_ctrl\"\n",
    "  - \"183_ctrl\"\n",
    "  - \"184_ctrl\"\n",
    "  - \"185_ctrl\"\n",
    "  - \"186_ctrl\"\n",
    "  - \"187_ctrl\"\n",
    "  - \"188_ctrl\"\n",
    "  - \"189_ctrl\"\n",
    "  - \"190_ctrl\"\n",
    "  - \"191_ctrl\"\n",
    "  - \"192_ctrl\"\n",
    "  - \"193_ctrl\"\n",
    "  - \"194_ctrl\"\n",
    "  - \"195_ctrl\"\n",
    "  - \"196_ctrl\"\n",
    "  - \"197_ctrl\"\n",
    "  - \"198_ctrl\"\n",
    "  - \"199_ctrl\"\n",
    "  - \"200_ctrl\"\n",
    "  \"\"\"\n",
    "}\n",
    "\n",
    "test_no_H3K27ac_yaml_contents = {\"fusion.yaml\":\"\"\"  \n",
    "model_config:\n",
    "  fc_do: 0.4\n",
    "  fc_task_dim: 256\n",
    "  layers:\n",
    "  - 2\n",
    "  rb_do: 0.4\n",
    "  stochastic_depth_p: 0.5\n",
    "model_type: \"mlp-residual\"\n",
    "\"\"\",\n",
    "\"globals.yaml\": \"\"\"\n",
    "\n",
    "checkpoint_interval: 30300 # 100 epochs #60000\n",
    "sample_interval: 30300 #60000\n",
    "n_epochs: 120\n",
    "batch_size: 64\n",
    "optimizer: \"adamw\"\n",
    "lr: 0.0001 #0.0001\n",
    "device: \"cuda\"\n",
    "compute_attributions: true\n",
    "\n",
    "\"\"\",\n",
    "\"input_cnn.yaml\": \"\"\"\n",
    "input_info:\n",
    "  input_source: ./inputs/landscape_arrays/test_no_H3K27ac/  #inserted_enhancer_test #inputs/silenced_arrays/silenced_arrays_H2B_S.D \n",
    "  #./data/parsed_data/inputs/arrays_train_100bp_no_H3K27ac/ #arrays_train_100bp_no_H3K27ac_uncoupled/\n",
    "  input_name: gene_expression\n",
    "  input_type: array\n",
    "\n",
    "model_config:\n",
    "  model_type: cnn\n",
    "  #pre_normalization: \"instancenorm\"\n",
    "  model_init_config:\n",
    "    num_output_features: 512 # before fc_repr_dim\n",
    "    layers: [4,4]\n",
    "    kernel_height: 1\n",
    "    down_stride_width: 2\n",
    "    first_stride_expansion_width: 1\n",
    "    first_kernel_expansion_height: 3 #5\n",
    "    kernel_width: 10  #10\n",
    "    dilation_factor_width: 2\n",
    "    dilation_factor_height: 1\n",
    "    channel_exp_base: 5  #3 #-1\n",
    "    first_channel_expansion: 2\n",
    "    rb_do: .3\n",
    "    stochastic_depth_p: .1\n",
    "    attention_inclusion_cutoff: 1 #50 #256\n",
    "    \"\"\",\n",
    "\"outputs_2_cond.yaml\": \"\"\"\n",
    "output_info:\n",
    "  output_name: expression_output\n",
    "  output_source: ./targets/test_targets.csv \n",
    "  #./data/parsed_data/targets/target_arrays_perturbational_inserted_enhancer.csv #target_arrays_perturbational_S.D.csv #target_arrays_1kbp_401_bins_2_conditions_decareads_abs.csv\n",
    "  output_type: tabular\n",
    "\n",
    "model_config: # <- new\n",
    "  model_type: linear # <- new\n",
    "\n",
    "output_type_info:\n",
    "  con_loss_name: \"SmoothL1Loss\"\n",
    "  target_con_columns:\n",
    "  - \"-200_ctrl\"\n",
    "  - \"-199_ctrl\"\n",
    "  - \"-198_ctrl\"\n",
    "  - \"-197_ctrl\"\n",
    "  - \"-196_ctrl\"\n",
    "  - \"-195_ctrl\"\n",
    "  - \"-194_ctrl\"\n",
    "  - \"-193_ctrl\"\n",
    "  - \"-192_ctrl\"\n",
    "  - \"-191_ctrl\"\n",
    "  - \"-190_ctrl\"\n",
    "  - \"-189_ctrl\"\n",
    "  - \"-188_ctrl\"\n",
    "  - \"-187_ctrl\"\n",
    "  - \"-186_ctrl\"\n",
    "  - \"-185_ctrl\"\n",
    "  - \"-184_ctrl\"\n",
    "  - \"-183_ctrl\"\n",
    "  - \"-182_ctrl\"\n",
    "  - \"-181_ctrl\"\n",
    "  - \"-180_ctrl\"\n",
    "  - \"-179_ctrl\"\n",
    "  - \"-178_ctrl\"\n",
    "  - \"-177_ctrl\"\n",
    "  - \"-176_ctrl\"\n",
    "  - \"-175_ctrl\"\n",
    "  - \"-174_ctrl\"\n",
    "  - \"-173_ctrl\"\n",
    "  - \"-172_ctrl\"\n",
    "  - \"-171_ctrl\"\n",
    "  - \"-170_ctrl\"\n",
    "  - \"-169_ctrl\"\n",
    "  - \"-168_ctrl\"\n",
    "  - \"-167_ctrl\"\n",
    "  - \"-166_ctrl\"\n",
    "  - \"-165_ctrl\"\n",
    "  - \"-164_ctrl\"\n",
    "  - \"-163_ctrl\"\n",
    "  - \"-162_ctrl\"\n",
    "  - \"-161_ctrl\"\n",
    "  - \"-160_ctrl\"\n",
    "  - \"-159_ctrl\"\n",
    "  - \"-158_ctrl\"\n",
    "  - \"-157_ctrl\"\n",
    "  - \"-156_ctrl\"\n",
    "  - \"-155_ctrl\"\n",
    "  - \"-154_ctrl\"\n",
    "  - \"-153_ctrl\"\n",
    "  - \"-152_ctrl\"\n",
    "  - \"-151_ctrl\"\n",
    "  - \"-150_ctrl\"\n",
    "  - \"-149_ctrl\"\n",
    "  - \"-148_ctrl\"\n",
    "  - \"-147_ctrl\"\n",
    "  - \"-146_ctrl\"\n",
    "  - \"-145_ctrl\"\n",
    "  - \"-144_ctrl\"\n",
    "  - \"-143_ctrl\"\n",
    "  - \"-142_ctrl\"\n",
    "  - \"-141_ctrl\"\n",
    "  - \"-140_ctrl\"\n",
    "  - \"-139_ctrl\"\n",
    "  - \"-138_ctrl\"\n",
    "  - \"-137_ctrl\"\n",
    "  - \"-136_ctrl\"\n",
    "  - \"-135_ctrl\"\n",
    "  - \"-134_ctrl\"\n",
    "  - \"-133_ctrl\"\n",
    "  - \"-132_ctrl\"\n",
    "  - \"-131_ctrl\"\n",
    "  - \"-130_ctrl\"\n",
    "  - \"-129_ctrl\"\n",
    "  - \"-128_ctrl\"\n",
    "  - \"-127_ctrl\"\n",
    "  - \"-126_ctrl\"\n",
    "  - \"-125_ctrl\"\n",
    "  - \"-124_ctrl\"\n",
    "  - \"-123_ctrl\"\n",
    "  - \"-122_ctrl\"\n",
    "  - \"-121_ctrl\"\n",
    "  - \"-120_ctrl\"\n",
    "  - \"-119_ctrl\"\n",
    "  - \"-118_ctrl\"\n",
    "  - \"-117_ctrl\"\n",
    "  - \"-116_ctrl\"\n",
    "  - \"-115_ctrl\"\n",
    "  - \"-114_ctrl\"\n",
    "  - \"-113_ctrl\"\n",
    "  - \"-112_ctrl\"\n",
    "  - \"-111_ctrl\"\n",
    "  - \"-110_ctrl\"\n",
    "  - \"-109_ctrl\"\n",
    "  - \"-108_ctrl\"\n",
    "  - \"-107_ctrl\"\n",
    "  - \"-106_ctrl\"\n",
    "  - \"-105_ctrl\"\n",
    "  - \"-104_ctrl\"\n",
    "  - \"-103_ctrl\"\n",
    "  - \"-102_ctrl\"\n",
    "  - \"-101_ctrl\"\n",
    "  - \"-100_ctrl\"\n",
    "  - \"-99_ctrl\"\n",
    "  - \"-98_ctrl\"\n",
    "  - \"-97_ctrl\"\n",
    "  - \"-96_ctrl\"\n",
    "  - \"-95_ctrl\"\n",
    "  - \"-94_ctrl\"\n",
    "  - \"-93_ctrl\"\n",
    "  - \"-92_ctrl\"\n",
    "  - \"-91_ctrl\"\n",
    "  - \"-90_ctrl\"\n",
    "  - \"-89_ctrl\"\n",
    "  - \"-88_ctrl\"\n",
    "  - \"-87_ctrl\"\n",
    "  - \"-86_ctrl\"\n",
    "  - \"-85_ctrl\"\n",
    "  - \"-84_ctrl\"\n",
    "  - \"-83_ctrl\"\n",
    "  - \"-82_ctrl\"\n",
    "  - \"-81_ctrl\"\n",
    "  - \"-80_ctrl\"\n",
    "  - \"-79_ctrl\"\n",
    "  - \"-78_ctrl\"\n",
    "  - \"-77_ctrl\"\n",
    "  - \"-76_ctrl\"\n",
    "  - \"-75_ctrl\"\n",
    "  - \"-74_ctrl\"\n",
    "  - \"-73_ctrl\"\n",
    "  - \"-72_ctrl\"\n",
    "  - \"-71_ctrl\"\n",
    "  - \"-70_ctrl\"\n",
    "  - \"-69_ctrl\"\n",
    "  - \"-68_ctrl\"\n",
    "  - \"-67_ctrl\"\n",
    "  - \"-66_ctrl\"\n",
    "  - \"-65_ctrl\"\n",
    "  - \"-64_ctrl\"\n",
    "  - \"-63_ctrl\"\n",
    "  - \"-62_ctrl\"\n",
    "  - \"-61_ctrl\"\n",
    "  - \"-60_ctrl\"\n",
    "  - \"-59_ctrl\"\n",
    "  - \"-58_ctrl\"\n",
    "  - \"-57_ctrl\"\n",
    "  - \"-56_ctrl\"\n",
    "  - \"-55_ctrl\"\n",
    "  - \"-54_ctrl\"\n",
    "  - \"-53_ctrl\"\n",
    "  - \"-52_ctrl\"\n",
    "  - \"-51_ctrl\"\n",
    "  - \"-50_ctrl\"\n",
    "  - \"-49_ctrl\"\n",
    "  - \"-48_ctrl\"\n",
    "  - \"-47_ctrl\"\n",
    "  - \"-46_ctrl\"\n",
    "  - \"-45_ctrl\"\n",
    "  - \"-44_ctrl\"\n",
    "  - \"-43_ctrl\"\n",
    "  - \"-42_ctrl\"\n",
    "  - \"-41_ctrl\"\n",
    "  - \"-40_ctrl\"\n",
    "  - \"-39_ctrl\"\n",
    "  - \"-38_ctrl\"\n",
    "  - \"-37_ctrl\"\n",
    "  - \"-36_ctrl\"\n",
    "  - \"-35_ctrl\"\n",
    "  - \"-34_ctrl\"\n",
    "  - \"-33_ctrl\"\n",
    "  - \"-32_ctrl\"\n",
    "  - \"-31_ctrl\"\n",
    "  - \"-30_ctrl\"\n",
    "  - \"-29_ctrl\"\n",
    "  - \"-28_ctrl\"\n",
    "  - \"-27_ctrl\"\n",
    "  - \"-26_ctrl\"\n",
    "  - \"-25_ctrl\"\n",
    "  - \"-24_ctrl\"\n",
    "  - \"-23_ctrl\"\n",
    "  - \"-22_ctrl\"\n",
    "  - \"-21_ctrl\"\n",
    "  - \"-20_ctrl\"\n",
    "  - \"-19_ctrl\"\n",
    "  - \"-18_ctrl\"\n",
    "  - \"-17_ctrl\"\n",
    "  - \"-16_ctrl\"\n",
    "  - \"-15_ctrl\"\n",
    "  - \"-14_ctrl\"\n",
    "  - \"-13_ctrl\"\n",
    "  - \"-12_ctrl\"\n",
    "  - \"-11_ctrl\"\n",
    "  - \"-10_ctrl\"\n",
    "  - \"-9_ctrl\"\n",
    "  - \"-8_ctrl\"\n",
    "  - \"-7_ctrl\"\n",
    "  - \"-6_ctrl\"\n",
    "  - \"-5_ctrl\"\n",
    "  - \"-4_ctrl\"\n",
    "  - \"-3_ctrl\"\n",
    "  - \"-2_ctrl\"\n",
    "  - \"-1_ctrl\"\n",
    "  - \"0_ctrl\"\n",
    "  - \"1_ctrl\"\n",
    "  - \"2_ctrl\"\n",
    "  - \"3_ctrl\"\n",
    "  - \"4_ctrl\"\n",
    "  - \"5_ctrl\"\n",
    "  - \"6_ctrl\"\n",
    "  - \"7_ctrl\"\n",
    "  - \"8_ctrl\"\n",
    "  - \"9_ctrl\"\n",
    "  - \"10_ctrl\"\n",
    "  - \"11_ctrl\"\n",
    "  - \"12_ctrl\"\n",
    "  - \"13_ctrl\"\n",
    "  - \"14_ctrl\"\n",
    "  - \"15_ctrl\"\n",
    "  - \"16_ctrl\"\n",
    "  - \"17_ctrl\"\n",
    "  - \"18_ctrl\"\n",
    "  - \"19_ctrl\"\n",
    "  - \"20_ctrl\"\n",
    "  - \"21_ctrl\"\n",
    "  - \"22_ctrl\"\n",
    "  - \"23_ctrl\"\n",
    "  - \"24_ctrl\"\n",
    "  - \"25_ctrl\"\n",
    "  - \"26_ctrl\"\n",
    "  - \"27_ctrl\"\n",
    "  - \"28_ctrl\"\n",
    "  - \"29_ctrl\"\n",
    "  - \"30_ctrl\"\n",
    "  - \"31_ctrl\"\n",
    "  - \"32_ctrl\"\n",
    "  - \"33_ctrl\"\n",
    "  - \"34_ctrl\"\n",
    "  - \"35_ctrl\"\n",
    "  - \"36_ctrl\"\n",
    "  - \"37_ctrl\"\n",
    "  - \"38_ctrl\"\n",
    "  - \"39_ctrl\"\n",
    "  - \"40_ctrl\"\n",
    "  - \"41_ctrl\"\n",
    "  - \"42_ctrl\"\n",
    "  - \"43_ctrl\"\n",
    "  - \"44_ctrl\"\n",
    "  - \"45_ctrl\"\n",
    "  - \"46_ctrl\"\n",
    "  - \"47_ctrl\"\n",
    "  - \"48_ctrl\"\n",
    "  - \"49_ctrl\"\n",
    "  - \"50_ctrl\"\n",
    "  - \"51_ctrl\"\n",
    "  - \"52_ctrl\"\n",
    "  - \"53_ctrl\"\n",
    "  - \"54_ctrl\"\n",
    "  - \"55_ctrl\"\n",
    "  - \"56_ctrl\"\n",
    "  - \"57_ctrl\"\n",
    "  - \"58_ctrl\"\n",
    "  - \"59_ctrl\"\n",
    "  - \"60_ctrl\"\n",
    "  - \"61_ctrl\"\n",
    "  - \"62_ctrl\"\n",
    "  - \"63_ctrl\"\n",
    "  - \"64_ctrl\"\n",
    "  - \"65_ctrl\"\n",
    "  - \"66_ctrl\"\n",
    "  - \"67_ctrl\"\n",
    "  - \"68_ctrl\"\n",
    "  - \"69_ctrl\"\n",
    "  - \"70_ctrl\"\n",
    "  - \"71_ctrl\"\n",
    "  - \"72_ctrl\"\n",
    "  - \"73_ctrl\"\n",
    "  - \"74_ctrl\"\n",
    "  - \"75_ctrl\"\n",
    "  - \"76_ctrl\"\n",
    "  - \"77_ctrl\"\n",
    "  - \"78_ctrl\"\n",
    "  - \"79_ctrl\"\n",
    "  - \"80_ctrl\"\n",
    "  - \"81_ctrl\"\n",
    "  - \"82_ctrl\"\n",
    "  - \"83_ctrl\"\n",
    "  - \"84_ctrl\"\n",
    "  - \"85_ctrl\"\n",
    "  - \"86_ctrl\"\n",
    "  - \"87_ctrl\"\n",
    "  - \"88_ctrl\"\n",
    "  - \"89_ctrl\"\n",
    "  - \"90_ctrl\"\n",
    "  - \"91_ctrl\"\n",
    "  - \"92_ctrl\"\n",
    "  - \"93_ctrl\"\n",
    "  - \"94_ctrl\"\n",
    "  - \"95_ctrl\"\n",
    "  - \"96_ctrl\"\n",
    "  - \"97_ctrl\"\n",
    "  - \"98_ctrl\"\n",
    "  - \"99_ctrl\"\n",
    "  - \"100_ctrl\"\n",
    "  - \"101_ctrl\"\n",
    "  - \"102_ctrl\"\n",
    "  - \"103_ctrl\"\n",
    "  - \"104_ctrl\"\n",
    "  - \"105_ctrl\"\n",
    "  - \"106_ctrl\"\n",
    "  - \"107_ctrl\"\n",
    "  - \"108_ctrl\"\n",
    "  - \"109_ctrl\"\n",
    "  - \"110_ctrl\"\n",
    "  - \"111_ctrl\"\n",
    "  - \"112_ctrl\"\n",
    "  - \"113_ctrl\"\n",
    "  - \"114_ctrl\"\n",
    "  - \"115_ctrl\"\n",
    "  - \"116_ctrl\"\n",
    "  - \"117_ctrl\"\n",
    "  - \"118_ctrl\"\n",
    "  - \"119_ctrl\"\n",
    "  - \"120_ctrl\"\n",
    "  - \"121_ctrl\"\n",
    "  - \"122_ctrl\"\n",
    "  - \"123_ctrl\"\n",
    "  - \"124_ctrl\"\n",
    "  - \"125_ctrl\"\n",
    "  - \"126_ctrl\"\n",
    "  - \"127_ctrl\"\n",
    "  - \"128_ctrl\"\n",
    "  - \"129_ctrl\"\n",
    "  - \"130_ctrl\"\n",
    "  - \"131_ctrl\"\n",
    "  - \"132_ctrl\"\n",
    "  - \"133_ctrl\"\n",
    "  - \"134_ctrl\"\n",
    "  - \"135_ctrl\"\n",
    "  - \"136_ctrl\"\n",
    "  - \"137_ctrl\"\n",
    "  - \"138_ctrl\"\n",
    "  - \"139_ctrl\"\n",
    "  - \"140_ctrl\"\n",
    "  - \"141_ctrl\"\n",
    "  - \"142_ctrl\"\n",
    "  - \"143_ctrl\"\n",
    "  - \"144_ctrl\"\n",
    "  - \"145_ctrl\"\n",
    "  - \"146_ctrl\"\n",
    "  - \"147_ctrl\"\n",
    "  - \"148_ctrl\"\n",
    "  - \"149_ctrl\"\n",
    "  - \"150_ctrl\"\n",
    "  - \"151_ctrl\"\n",
    "  - \"152_ctrl\"\n",
    "  - \"153_ctrl\"\n",
    "  - \"154_ctrl\"\n",
    "  - \"155_ctrl\"\n",
    "  - \"156_ctrl\"\n",
    "  - \"157_ctrl\"\n",
    "  - \"158_ctrl\"\n",
    "  - \"159_ctrl\"\n",
    "  - \"160_ctrl\"\n",
    "  - \"161_ctrl\"\n",
    "  - \"162_ctrl\"\n",
    "  - \"163_ctrl\"\n",
    "  - \"164_ctrl\"\n",
    "  - \"165_ctrl\"\n",
    "  - \"166_ctrl\"\n",
    "  - \"167_ctrl\"\n",
    "  - \"168_ctrl\"\n",
    "  - \"169_ctrl\"\n",
    "  - \"170_ctrl\"\n",
    "  - \"171_ctrl\"\n",
    "  - \"172_ctrl\"\n",
    "  - \"173_ctrl\"\n",
    "  - \"174_ctrl\"\n",
    "  - \"175_ctrl\"\n",
    "  - \"176_ctrl\"\n",
    "  - \"177_ctrl\"\n",
    "  - \"178_ctrl\"\n",
    "  - \"179_ctrl\"\n",
    "  - \"180_ctrl\"\n",
    "  - \"181_ctrl\"\n",
    "  - \"182_ctrl\"\n",
    "  - \"183_ctrl\"\n",
    "  - \"184_ctrl\"\n",
    "  - \"185_ctrl\"\n",
    "  - \"186_ctrl\"\n",
    "  - \"187_ctrl\"\n",
    "  - \"188_ctrl\"\n",
    "  - \"189_ctrl\"\n",
    "  - \"190_ctrl\"\n",
    "  - \"191_ctrl\"\n",
    "  - \"192_ctrl\"\n",
    "  - \"193_ctrl\"\n",
    "  - \"194_ctrl\"\n",
    "  - \"195_ctrl\"\n",
    "  - \"196_ctrl\"\n",
    "  - \"197_ctrl\"\n",
    "  - \"198_ctrl\"\n",
    "  - \"199_ctrl\"\n",
    "  - \"200_ctrl\"\n",
    "  \"\"\" }\n",
    "\n",
    "\n",
    "predict_20kbp_context_yaml_contents = {\"globals.yaml\":\"\"\"\n",
    "checkpoint_interval: 30300 # 100 epochs is 30300 #60000\n",
    "sample_interval: 30300 #60000\n",
    "n_epochs: 120\n",
    "batch_size: 64\n",
    "optimizer: \"adamw\"\n",
    "lr: 0.0001 #0.0001\n",
    "device: \"cuda\"\n",
    "compute_attributions: false\n",
    "\"\"\",\n",
    "\"input_cnn.yaml\": \"\"\" \n",
    "input_info:\n",
    "  input_source: ./inputs/landscape_arrays/test_20kbp_context/ \n",
    "  input_name: gene_expression\n",
    "  input_type: array\n",
    "\n",
    "model_config:\n",
    "  model_type: cnn\n",
    "  #pre_normalization: \"instancenorm\"\n",
    "  model_init_config:\n",
    "    num_output_features: 512 # before fc_repr_dim\n",
    "    layers: [1,1]\n",
    "    kernel_height: 1\n",
    "    down_stride_width: 2\n",
    "    first_stride_expansion_width: 1\n",
    "    first_kernel_expansion_height: 4 #5\n",
    "    kernel_width: 10  #10\n",
    "    dilation_factor_width: 2\n",
    "    dilation_factor_height: 1\n",
    "    channel_exp_base: 5  #3 #-1\n",
    "    first_channel_expansion: 2\n",
    "    rb_do: .3\n",
    "    stochastic_depth_p: .1\n",
    "    attention_inclusion_cutoff: 1 #50 #256\n",
    "\n",
    "\"\"\",\n",
    "\"fusion.yaml\": \"\"\"\n",
    "model_config:\n",
    "  fc_do: 0.4\n",
    "  fc_task_dim: 256\n",
    "  layers:\n",
    "  - 1\n",
    "  rb_do: 0.4\n",
    "  stochastic_depth_p: 0.5\n",
    "model_type: \"mlp-residual\"\n",
    "\"\"\",\n",
    "\"outputs_2_cond.yaml\":\"\"\"\n",
    "output_info:\n",
    "  output_name: expression_output\n",
    "  output_source: ./targets/test_targets.csv \n",
    "  output_type: tabular\n",
    "\n",
    "model_config: # <- new\n",
    "  model_type: linear # <- new\n",
    "\n",
    "output_type_info:\n",
    "  con_loss_name: \"SmoothL1Loss\"\n",
    "  target_con_columns:\n",
    "  - \"-9_ctrl\"\n",
    "  - \"-8_ctrl\"\n",
    "  - \"-7_ctrl\"\n",
    "  - \"-6_ctrl\"\n",
    "  - \"-5_ctrl\"\n",
    "  - \"-4_ctrl\"\n",
    "  - \"-3_ctrl\"\n",
    "  - \"-2_ctrl\"\n",
    "  - \"-1_ctrl\"\n",
    "  - \"0_ctrl\"\n",
    "  - \"1_ctrl\"\n",
    "  - \"2_ctrl\"\n",
    "  - \"3_ctrl\"\n",
    "  - \"4_ctrl\"\n",
    "  - \"5_ctrl\"\n",
    "  - \"6_ctrl\"\n",
    "  - \"7_ctrl\"\n",
    "  - \"8_ctrl\"\n",
    "  - \"9_ctrl\"\n",
    "  \"\"\"}\n",
    "\n",
    "\n",
    "training_1kbp_binning_yaml_contents = {\"globals.yaml\":\"\"\"\n",
    "output_folder: ./runs/gene_expression_only_chrom_pure_conv_1kbp_binning/  #gene_expression_prediction_no_H3K27ac_uncoupled/     #gene_expression_cnn_1kbp_401_bins_2_cond_reloaded/ \n",
    "manual_valid_ids_file: ./annotations/manual_validation_ids_chr17.txt  #manual_validation_ids_chr17_uncoupled.txt\n",
    "checkpoint_interval: 30300 # 100 epochs #60000\n",
    "sample_interval: 30300 #60000\n",
    "n_epochs: 120\n",
    "batch_size: 64\n",
    "optimizer: \"adamw\"\n",
    "lr: 0.0001 #0.0001\n",
    "device: \"cuda\"\n",
    "compute_attributions: false\n",
    "# latent_sampling: \n",
    "#   layers_to_sample:\n",
    "#     - \"input_modules.contact_maps.feature_extractor.conv.0.conv_1\"\n",
    "#     - \"input_modules.contact_maps.feature_extractor.conv.1.conv_2\"\n",
    "#     - \"input_modules.contact_maps.feature_extractor.conv.2.conv_2\"\n",
    "#     - \"input_modules.contact_maps.feature_extractor.conv.3.conv_2\"\n",
    "# attribution_background_samples: 512\n",
    "# attributions_every_sample_factor: 1\n",
    "#pretrained_checkpoint: best_models/gene_expression_exformer_unlimited_chrom_and_micro_with_attention_model_117600_perf-average=0.8435.pt \n",
    "\n",
    "\"\"\",\n",
    "\"input_cnn.yaml\": \"\"\" \n",
    "input_info:\n",
    "  input_source: ./inputs/landscape_arrays/training/ \n",
    "  input_name: gene_expression\n",
    "  input_type: array\n",
    "\n",
    "model_config:\n",
    "  model_type: cnn\n",
    "  #pre_normalization: \"instancenorm\"\n",
    "  model_init_config:\n",
    "    num_output_features: 512 # before fc_repr_dim\n",
    "    layers: [4,4]\n",
    "    kernel_height: 1\n",
    "    down_stride_width: 2\n",
    "    first_stride_expansion_width: 1\n",
    "    first_kernel_expansion_height: 4 #5\n",
    "    kernel_width: 10  #10\n",
    "    dilation_factor_width: 2\n",
    "    dilation_factor_height: 1\n",
    "    channel_exp_base: 5  #3 #-1\n",
    "    first_channel_expansion: 2\n",
    "    rb_do: .3\n",
    "    stochastic_depth_p: .1\n",
    "    attention_inclusion_cutoff: 1 #50 #256\n",
    "\n",
    "\"\"\",\n",
    "\"fusion.yaml\": \"\"\"\n",
    "model_config:\n",
    "  fc_do: 0.4\n",
    "  fc_task_dim: 256\n",
    "  layers:\n",
    "  - 2\n",
    "  rb_do: 0.4\n",
    "  stochastic_depth_p: 0.5\n",
    "model_type: \"mlp-residual\"\n",
    "\"\"\",\n",
    "\"outputs_2_cond.yaml\":\"\"\"\n",
    "output_info:\n",
    "  output_name: expression_output\n",
    "  output_source: ./targets/training_targets_1kbp_binning.csv \n",
    "  output_type: tabular\n",
    "\n",
    "model_config: # <- new\n",
    "  model_type: linear # <- new\n",
    "\n",
    "output_type_info:\n",
    "  con_loss_name: \"SmoothL1Loss\"\n",
    "  target_con_columns:\n",
    "  - \"-200_ctrl\"\n",
    "  - \"-199_ctrl\"\n",
    "  - \"-198_ctrl\"\n",
    "  - \"-197_ctrl\"\n",
    "  - \"-196_ctrl\"\n",
    "  - \"-195_ctrl\"\n",
    "  - \"-194_ctrl\"\n",
    "  - \"-193_ctrl\"\n",
    "  - \"-192_ctrl\"\n",
    "  - \"-191_ctrl\"\n",
    "  - \"-190_ctrl\"\n",
    "  - \"-189_ctrl\"\n",
    "  - \"-188_ctrl\"\n",
    "  - \"-187_ctrl\"\n",
    "  - \"-186_ctrl\"\n",
    "  - \"-185_ctrl\"\n",
    "  - \"-184_ctrl\"\n",
    "  - \"-183_ctrl\"\n",
    "  - \"-182_ctrl\"\n",
    "  - \"-181_ctrl\"\n",
    "  - \"-180_ctrl\"\n",
    "  - \"-179_ctrl\"\n",
    "  - \"-178_ctrl\"\n",
    "  - \"-177_ctrl\"\n",
    "  - \"-176_ctrl\"\n",
    "  - \"-175_ctrl\"\n",
    "  - \"-174_ctrl\"\n",
    "  - \"-173_ctrl\"\n",
    "  - \"-172_ctrl\"\n",
    "  - \"-171_ctrl\"\n",
    "  - \"-170_ctrl\"\n",
    "  - \"-169_ctrl\"\n",
    "  - \"-168_ctrl\"\n",
    "  - \"-167_ctrl\"\n",
    "  - \"-166_ctrl\"\n",
    "  - \"-165_ctrl\"\n",
    "  - \"-164_ctrl\"\n",
    "  - \"-163_ctrl\"\n",
    "  - \"-162_ctrl\"\n",
    "  - \"-161_ctrl\"\n",
    "  - \"-160_ctrl\"\n",
    "  - \"-159_ctrl\"\n",
    "  - \"-158_ctrl\"\n",
    "  - \"-157_ctrl\"\n",
    "  - \"-156_ctrl\"\n",
    "  - \"-155_ctrl\"\n",
    "  - \"-154_ctrl\"\n",
    "  - \"-153_ctrl\"\n",
    "  - \"-152_ctrl\"\n",
    "  - \"-151_ctrl\"\n",
    "  - \"-150_ctrl\"\n",
    "  - \"-149_ctrl\"\n",
    "  - \"-148_ctrl\"\n",
    "  - \"-147_ctrl\"\n",
    "  - \"-146_ctrl\"\n",
    "  - \"-145_ctrl\"\n",
    "  - \"-144_ctrl\"\n",
    "  - \"-143_ctrl\"\n",
    "  - \"-142_ctrl\"\n",
    "  - \"-141_ctrl\"\n",
    "  - \"-140_ctrl\"\n",
    "  - \"-139_ctrl\"\n",
    "  - \"-138_ctrl\"\n",
    "  - \"-137_ctrl\"\n",
    "  - \"-136_ctrl\"\n",
    "  - \"-135_ctrl\"\n",
    "  - \"-134_ctrl\"\n",
    "  - \"-133_ctrl\"\n",
    "  - \"-132_ctrl\"\n",
    "  - \"-131_ctrl\"\n",
    "  - \"-130_ctrl\"\n",
    "  - \"-129_ctrl\"\n",
    "  - \"-128_ctrl\"\n",
    "  - \"-127_ctrl\"\n",
    "  - \"-126_ctrl\"\n",
    "  - \"-125_ctrl\"\n",
    "  - \"-124_ctrl\"\n",
    "  - \"-123_ctrl\"\n",
    "  - \"-122_ctrl\"\n",
    "  - \"-121_ctrl\"\n",
    "  - \"-120_ctrl\"\n",
    "  - \"-119_ctrl\"\n",
    "  - \"-118_ctrl\"\n",
    "  - \"-117_ctrl\"\n",
    "  - \"-116_ctrl\"\n",
    "  - \"-115_ctrl\"\n",
    "  - \"-114_ctrl\"\n",
    "  - \"-113_ctrl\"\n",
    "  - \"-112_ctrl\"\n",
    "  - \"-111_ctrl\"\n",
    "  - \"-110_ctrl\"\n",
    "  - \"-109_ctrl\"\n",
    "  - \"-108_ctrl\"\n",
    "  - \"-107_ctrl\"\n",
    "  - \"-106_ctrl\"\n",
    "  - \"-105_ctrl\"\n",
    "  - \"-104_ctrl\"\n",
    "  - \"-103_ctrl\"\n",
    "  - \"-102_ctrl\"\n",
    "  - \"-101_ctrl\"\n",
    "  - \"-100_ctrl\"\n",
    "  - \"-99_ctrl\"\n",
    "  - \"-98_ctrl\"\n",
    "  - \"-97_ctrl\"\n",
    "  - \"-96_ctrl\"\n",
    "  - \"-95_ctrl\"\n",
    "  - \"-94_ctrl\"\n",
    "  - \"-93_ctrl\"\n",
    "  - \"-92_ctrl\"\n",
    "  - \"-91_ctrl\"\n",
    "  - \"-90_ctrl\"\n",
    "  - \"-89_ctrl\"\n",
    "  - \"-88_ctrl\"\n",
    "  - \"-87_ctrl\"\n",
    "  - \"-86_ctrl\"\n",
    "  - \"-85_ctrl\"\n",
    "  - \"-84_ctrl\"\n",
    "  - \"-83_ctrl\"\n",
    "  - \"-82_ctrl\"\n",
    "  - \"-81_ctrl\"\n",
    "  - \"-80_ctrl\"\n",
    "  - \"-79_ctrl\"\n",
    "  - \"-78_ctrl\"\n",
    "  - \"-77_ctrl\"\n",
    "  - \"-76_ctrl\"\n",
    "  - \"-75_ctrl\"\n",
    "  - \"-74_ctrl\"\n",
    "  - \"-73_ctrl\"\n",
    "  - \"-72_ctrl\"\n",
    "  - \"-71_ctrl\"\n",
    "  - \"-70_ctrl\"\n",
    "  - \"-69_ctrl\"\n",
    "  - \"-68_ctrl\"\n",
    "  - \"-67_ctrl\"\n",
    "  - \"-66_ctrl\"\n",
    "  - \"-65_ctrl\"\n",
    "  - \"-64_ctrl\"\n",
    "  - \"-63_ctrl\"\n",
    "  - \"-62_ctrl\"\n",
    "  - \"-61_ctrl\"\n",
    "  - \"-60_ctrl\"\n",
    "  - \"-59_ctrl\"\n",
    "  - \"-58_ctrl\"\n",
    "  - \"-57_ctrl\"\n",
    "  - \"-56_ctrl\"\n",
    "  - \"-55_ctrl\"\n",
    "  - \"-54_ctrl\"\n",
    "  - \"-53_ctrl\"\n",
    "  - \"-52_ctrl\"\n",
    "  - \"-51_ctrl\"\n",
    "  - \"-50_ctrl\"\n",
    "  - \"-49_ctrl\"\n",
    "  - \"-48_ctrl\"\n",
    "  - \"-47_ctrl\"\n",
    "  - \"-46_ctrl\"\n",
    "  - \"-45_ctrl\"\n",
    "  - \"-44_ctrl\"\n",
    "  - \"-43_ctrl\"\n",
    "  - \"-42_ctrl\"\n",
    "  - \"-41_ctrl\"\n",
    "  - \"-40_ctrl\"\n",
    "  - \"-39_ctrl\"\n",
    "  - \"-38_ctrl\"\n",
    "  - \"-37_ctrl\"\n",
    "  - \"-36_ctrl\"\n",
    "  - \"-35_ctrl\"\n",
    "  - \"-34_ctrl\"\n",
    "  - \"-33_ctrl\"\n",
    "  - \"-32_ctrl\"\n",
    "  - \"-31_ctrl\"\n",
    "  - \"-30_ctrl\"\n",
    "  - \"-29_ctrl\"\n",
    "  - \"-28_ctrl\"\n",
    "  - \"-27_ctrl\"\n",
    "  - \"-26_ctrl\"\n",
    "  - \"-25_ctrl\"\n",
    "  - \"-24_ctrl\"\n",
    "  - \"-23_ctrl\"\n",
    "  - \"-22_ctrl\"\n",
    "  - \"-21_ctrl\"\n",
    "  - \"-20_ctrl\"\n",
    "  - \"-19_ctrl\"\n",
    "  - \"-18_ctrl\"\n",
    "  - \"-17_ctrl\"\n",
    "  - \"-16_ctrl\"\n",
    "  - \"-15_ctrl\"\n",
    "  - \"-14_ctrl\"\n",
    "  - \"-13_ctrl\"\n",
    "  - \"-12_ctrl\"\n",
    "  - \"-11_ctrl\"\n",
    "  - \"-10_ctrl\"\n",
    "  - \"-9_ctrl\"\n",
    "  - \"-8_ctrl\"\n",
    "  - \"-7_ctrl\"\n",
    "  - \"-6_ctrl\"\n",
    "  - \"-5_ctrl\"\n",
    "  - \"-4_ctrl\"\n",
    "  - \"-3_ctrl\"\n",
    "  - \"-2_ctrl\"\n",
    "  - \"-1_ctrl\"\n",
    "  - \"0_ctrl\"\n",
    "  - \"1_ctrl\"\n",
    "  - \"2_ctrl\"\n",
    "  - \"3_ctrl\"\n",
    "  - \"4_ctrl\"\n",
    "  - \"5_ctrl\"\n",
    "  - \"6_ctrl\"\n",
    "  - \"7_ctrl\"\n",
    "  - \"8_ctrl\"\n",
    "  - \"9_ctrl\"\n",
    "  - \"10_ctrl\"\n",
    "  - \"11_ctrl\"\n",
    "  - \"12_ctrl\"\n",
    "  - \"13_ctrl\"\n",
    "  - \"14_ctrl\"\n",
    "  - \"15_ctrl\"\n",
    "  - \"16_ctrl\"\n",
    "  - \"17_ctrl\"\n",
    "  - \"18_ctrl\"\n",
    "  - \"19_ctrl\"\n",
    "  - \"20_ctrl\"\n",
    "  - \"21_ctrl\"\n",
    "  - \"22_ctrl\"\n",
    "  - \"23_ctrl\"\n",
    "  - \"24_ctrl\"\n",
    "  - \"25_ctrl\"\n",
    "  - \"26_ctrl\"\n",
    "  - \"27_ctrl\"\n",
    "  - \"28_ctrl\"\n",
    "  - \"29_ctrl\"\n",
    "  - \"30_ctrl\"\n",
    "  - \"31_ctrl\"\n",
    "  - \"32_ctrl\"\n",
    "  - \"33_ctrl\"\n",
    "  - \"34_ctrl\"\n",
    "  - \"35_ctrl\"\n",
    "  - \"36_ctrl\"\n",
    "  - \"37_ctrl\"\n",
    "  - \"38_ctrl\"\n",
    "  - \"39_ctrl\"\n",
    "  - \"40_ctrl\"\n",
    "  - \"41_ctrl\"\n",
    "  - \"42_ctrl\"\n",
    "  - \"43_ctrl\"\n",
    "  - \"44_ctrl\"\n",
    "  - \"45_ctrl\"\n",
    "  - \"46_ctrl\"\n",
    "  - \"47_ctrl\"\n",
    "  - \"48_ctrl\"\n",
    "  - \"49_ctrl\"\n",
    "  - \"50_ctrl\"\n",
    "  - \"51_ctrl\"\n",
    "  - \"52_ctrl\"\n",
    "  - \"53_ctrl\"\n",
    "  - \"54_ctrl\"\n",
    "  - \"55_ctrl\"\n",
    "  - \"56_ctrl\"\n",
    "  - \"57_ctrl\"\n",
    "  - \"58_ctrl\"\n",
    "  - \"59_ctrl\"\n",
    "  - \"60_ctrl\"\n",
    "  - \"61_ctrl\"\n",
    "  - \"62_ctrl\"\n",
    "  - \"63_ctrl\"\n",
    "  - \"64_ctrl\"\n",
    "  - \"65_ctrl\"\n",
    "  - \"66_ctrl\"\n",
    "  - \"67_ctrl\"\n",
    "  - \"68_ctrl\"\n",
    "  - \"69_ctrl\"\n",
    "  - \"70_ctrl\"\n",
    "  - \"71_ctrl\"\n",
    "  - \"72_ctrl\"\n",
    "  - \"73_ctrl\"\n",
    "  - \"74_ctrl\"\n",
    "  - \"75_ctrl\"\n",
    "  - \"76_ctrl\"\n",
    "  - \"77_ctrl\"\n",
    "  - \"78_ctrl\"\n",
    "  - \"79_ctrl\"\n",
    "  - \"80_ctrl\"\n",
    "  - \"81_ctrl\"\n",
    "  - \"82_ctrl\"\n",
    "  - \"83_ctrl\"\n",
    "  - \"84_ctrl\"\n",
    "  - \"85_ctrl\"\n",
    "  - \"86_ctrl\"\n",
    "  - \"87_ctrl\"\n",
    "  - \"88_ctrl\"\n",
    "  - \"89_ctrl\"\n",
    "  - \"90_ctrl\"\n",
    "  - \"91_ctrl\"\n",
    "  - \"92_ctrl\"\n",
    "  - \"93_ctrl\"\n",
    "  - \"94_ctrl\"\n",
    "  - \"95_ctrl\"\n",
    "  - \"96_ctrl\"\n",
    "  - \"97_ctrl\"\n",
    "  - \"98_ctrl\"\n",
    "  - \"99_ctrl\"\n",
    "  - \"100_ctrl\"\n",
    "  - \"101_ctrl\"\n",
    "  - \"102_ctrl\"\n",
    "  - \"103_ctrl\"\n",
    "  - \"104_ctrl\"\n",
    "  - \"105_ctrl\"\n",
    "  - \"106_ctrl\"\n",
    "  - \"107_ctrl\"\n",
    "  - \"108_ctrl\"\n",
    "  - \"109_ctrl\"\n",
    "  - \"110_ctrl\"\n",
    "  - \"111_ctrl\"\n",
    "  - \"112_ctrl\"\n",
    "  - \"113_ctrl\"\n",
    "  - \"114_ctrl\"\n",
    "  - \"115_ctrl\"\n",
    "  - \"116_ctrl\"\n",
    "  - \"117_ctrl\"\n",
    "  - \"118_ctrl\"\n",
    "  - \"119_ctrl\"\n",
    "  - \"120_ctrl\"\n",
    "  - \"121_ctrl\"\n",
    "  - \"122_ctrl\"\n",
    "  - \"123_ctrl\"\n",
    "  - \"124_ctrl\"\n",
    "  - \"125_ctrl\"\n",
    "  - \"126_ctrl\"\n",
    "  - \"127_ctrl\"\n",
    "  - \"128_ctrl\"\n",
    "  - \"129_ctrl\"\n",
    "  - \"130_ctrl\"\n",
    "  - \"131_ctrl\"\n",
    "  - \"132_ctrl\"\n",
    "  - \"133_ctrl\"\n",
    "  - \"134_ctrl\"\n",
    "  - \"135_ctrl\"\n",
    "  - \"136_ctrl\"\n",
    "  - \"137_ctrl\"\n",
    "  - \"138_ctrl\"\n",
    "  - \"139_ctrl\"\n",
    "  - \"140_ctrl\"\n",
    "  - \"141_ctrl\"\n",
    "  - \"142_ctrl\"\n",
    "  - \"143_ctrl\"\n",
    "  - \"144_ctrl\"\n",
    "  - \"145_ctrl\"\n",
    "  - \"146_ctrl\"\n",
    "  - \"147_ctrl\"\n",
    "  - \"148_ctrl\"\n",
    "  - \"149_ctrl\"\n",
    "  - \"150_ctrl\"\n",
    "  - \"151_ctrl\"\n",
    "  - \"152_ctrl\"\n",
    "  - \"153_ctrl\"\n",
    "  - \"154_ctrl\"\n",
    "  - \"155_ctrl\"\n",
    "  - \"156_ctrl\"\n",
    "  - \"157_ctrl\"\n",
    "  - \"158_ctrl\"\n",
    "  - \"159_ctrl\"\n",
    "  - \"160_ctrl\"\n",
    "  - \"161_ctrl\"\n",
    "  - \"162_ctrl\"\n",
    "  - \"163_ctrl\"\n",
    "  - \"164_ctrl\"\n",
    "  - \"165_ctrl\"\n",
    "  - \"166_ctrl\"\n",
    "  - \"167_ctrl\"\n",
    "  - \"168_ctrl\"\n",
    "  - \"169_ctrl\"\n",
    "  - \"170_ctrl\"\n",
    "  - \"171_ctrl\"\n",
    "  - \"172_ctrl\"\n",
    "  - \"173_ctrl\"\n",
    "  - \"174_ctrl\"\n",
    "  - \"175_ctrl\"\n",
    "  - \"176_ctrl\"\n",
    "  - \"177_ctrl\"\n",
    "  - \"178_ctrl\"\n",
    "  - \"179_ctrl\"\n",
    "  - \"180_ctrl\"\n",
    "  - \"181_ctrl\"\n",
    "  - \"182_ctrl\"\n",
    "  - \"183_ctrl\"\n",
    "  - \"184_ctrl\"\n",
    "  - \"185_ctrl\"\n",
    "  - \"186_ctrl\"\n",
    "  - \"187_ctrl\"\n",
    "  - \"188_ctrl\"\n",
    "  - \"189_ctrl\"\n",
    "  - \"190_ctrl\"\n",
    "  - \"191_ctrl\"\n",
    "  - \"192_ctrl\"\n",
    "  - \"193_ctrl\"\n",
    "  - \"194_ctrl\"\n",
    "  - \"195_ctrl\"\n",
    "  - \"196_ctrl\"\n",
    "  - \"197_ctrl\"\n",
    "  - \"198_ctrl\"\n",
    "  - \"199_ctrl\"\n",
    "  - \"200_ctrl\"\n",
    "  \"\"\"\n",
    "}\n",
    "\n",
    "test_1kbp_binning_yaml_contents =  {\"globals.yaml\":\"\"\"\n",
    "checkpoint_interval: 30300 # 100 epochs #60000\n",
    "sample_interval: 30300 #60000\n",
    "n_epochs: 120\n",
    "batch_size: 64\n",
    "optimizer: \"adamw\"\n",
    "lr: 0.0001 #0.0001\n",
    "device: \"cuda\"\n",
    "compute_attributions: true\n",
    "\"\"\",\n",
    "\"input_cnn.yaml\": \"\"\" \n",
    "input_info:\n",
    "  input_source: ./inputs/landscape_arrays/test/ \n",
    "  input_name: gene_expression\n",
    "  input_type: array\n",
    "\n",
    "model_config:\n",
    "  model_type: cnn\n",
    "  #pre_normalization: \"instancenorm\"\n",
    "  model_init_config:\n",
    "    num_output_features: 512 # before fc_repr_dim\n",
    "    layers: [4,4]\n",
    "    kernel_height: 1\n",
    "    down_stride_width: 2\n",
    "    first_stride_expansion_width: 1\n",
    "    first_kernel_expansion_height: 4 #5\n",
    "    kernel_width: 10  #10\n",
    "    dilation_factor_width: 2\n",
    "    dilation_factor_height: 1\n",
    "    channel_exp_base: 5  #3 #-1\n",
    "    first_channel_expansion: 2\n",
    "    rb_do: .3\n",
    "    stochastic_depth_p: .1\n",
    "    attention_inclusion_cutoff: 1 #50 #256\n",
    "\n",
    "\"\"\",\n",
    "\"fusion.yaml\": \"\"\"\n",
    "model_config:\n",
    "  fc_do: 0.4\n",
    "  fc_task_dim: 256\n",
    "  layers:\n",
    "  - 2\n",
    "  rb_do: 0.4\n",
    "  stochastic_depth_p: 0.5\n",
    "model_type: \"mlp-residual\"\n",
    "\"\"\",\n",
    "\"outputs_2_cond.yaml\":\"\"\"\n",
    "output_info:\n",
    "  output_name: expression_output\n",
    "  output_source: ./targets/test_targets_1kbp_binning.csv \n",
    "  output_type: tabular\n",
    "\n",
    "model_config: # <- new\n",
    "  model_type: linear # <- new\n",
    "\n",
    "output_type_info:\n",
    "  con_loss_name: \"SmoothL1Loss\"\n",
    "  target_con_columns:\n",
    "  - \"-200_ctrl\"\n",
    "  - \"-199_ctrl\"\n",
    "  - \"-198_ctrl\"\n",
    "  - \"-197_ctrl\"\n",
    "  - \"-196_ctrl\"\n",
    "  - \"-195_ctrl\"\n",
    "  - \"-194_ctrl\"\n",
    "  - \"-193_ctrl\"\n",
    "  - \"-192_ctrl\"\n",
    "  - \"-191_ctrl\"\n",
    "  - \"-190_ctrl\"\n",
    "  - \"-189_ctrl\"\n",
    "  - \"-188_ctrl\"\n",
    "  - \"-187_ctrl\"\n",
    "  - \"-186_ctrl\"\n",
    "  - \"-185_ctrl\"\n",
    "  - \"-184_ctrl\"\n",
    "  - \"-183_ctrl\"\n",
    "  - \"-182_ctrl\"\n",
    "  - \"-181_ctrl\"\n",
    "  - \"-180_ctrl\"\n",
    "  - \"-179_ctrl\"\n",
    "  - \"-178_ctrl\"\n",
    "  - \"-177_ctrl\"\n",
    "  - \"-176_ctrl\"\n",
    "  - \"-175_ctrl\"\n",
    "  - \"-174_ctrl\"\n",
    "  - \"-173_ctrl\"\n",
    "  - \"-172_ctrl\"\n",
    "  - \"-171_ctrl\"\n",
    "  - \"-170_ctrl\"\n",
    "  - \"-169_ctrl\"\n",
    "  - \"-168_ctrl\"\n",
    "  - \"-167_ctrl\"\n",
    "  - \"-166_ctrl\"\n",
    "  - \"-165_ctrl\"\n",
    "  - \"-164_ctrl\"\n",
    "  - \"-163_ctrl\"\n",
    "  - \"-162_ctrl\"\n",
    "  - \"-161_ctrl\"\n",
    "  - \"-160_ctrl\"\n",
    "  - \"-159_ctrl\"\n",
    "  - \"-158_ctrl\"\n",
    "  - \"-157_ctrl\"\n",
    "  - \"-156_ctrl\"\n",
    "  - \"-155_ctrl\"\n",
    "  - \"-154_ctrl\"\n",
    "  - \"-153_ctrl\"\n",
    "  - \"-152_ctrl\"\n",
    "  - \"-151_ctrl\"\n",
    "  - \"-150_ctrl\"\n",
    "  - \"-149_ctrl\"\n",
    "  - \"-148_ctrl\"\n",
    "  - \"-147_ctrl\"\n",
    "  - \"-146_ctrl\"\n",
    "  - \"-145_ctrl\"\n",
    "  - \"-144_ctrl\"\n",
    "  - \"-143_ctrl\"\n",
    "  - \"-142_ctrl\"\n",
    "  - \"-141_ctrl\"\n",
    "  - \"-140_ctrl\"\n",
    "  - \"-139_ctrl\"\n",
    "  - \"-138_ctrl\"\n",
    "  - \"-137_ctrl\"\n",
    "  - \"-136_ctrl\"\n",
    "  - \"-135_ctrl\"\n",
    "  - \"-134_ctrl\"\n",
    "  - \"-133_ctrl\"\n",
    "  - \"-132_ctrl\"\n",
    "  - \"-131_ctrl\"\n",
    "  - \"-130_ctrl\"\n",
    "  - \"-129_ctrl\"\n",
    "  - \"-128_ctrl\"\n",
    "  - \"-127_ctrl\"\n",
    "  - \"-126_ctrl\"\n",
    "  - \"-125_ctrl\"\n",
    "  - \"-124_ctrl\"\n",
    "  - \"-123_ctrl\"\n",
    "  - \"-122_ctrl\"\n",
    "  - \"-121_ctrl\"\n",
    "  - \"-120_ctrl\"\n",
    "  - \"-119_ctrl\"\n",
    "  - \"-118_ctrl\"\n",
    "  - \"-117_ctrl\"\n",
    "  - \"-116_ctrl\"\n",
    "  - \"-115_ctrl\"\n",
    "  - \"-114_ctrl\"\n",
    "  - \"-113_ctrl\"\n",
    "  - \"-112_ctrl\"\n",
    "  - \"-111_ctrl\"\n",
    "  - \"-110_ctrl\"\n",
    "  - \"-109_ctrl\"\n",
    "  - \"-108_ctrl\"\n",
    "  - \"-107_ctrl\"\n",
    "  - \"-106_ctrl\"\n",
    "  - \"-105_ctrl\"\n",
    "  - \"-104_ctrl\"\n",
    "  - \"-103_ctrl\"\n",
    "  - \"-102_ctrl\"\n",
    "  - \"-101_ctrl\"\n",
    "  - \"-100_ctrl\"\n",
    "  - \"-99_ctrl\"\n",
    "  - \"-98_ctrl\"\n",
    "  - \"-97_ctrl\"\n",
    "  - \"-96_ctrl\"\n",
    "  - \"-95_ctrl\"\n",
    "  - \"-94_ctrl\"\n",
    "  - \"-93_ctrl\"\n",
    "  - \"-92_ctrl\"\n",
    "  - \"-91_ctrl\"\n",
    "  - \"-90_ctrl\"\n",
    "  - \"-89_ctrl\"\n",
    "  - \"-88_ctrl\"\n",
    "  - \"-87_ctrl\"\n",
    "  - \"-86_ctrl\"\n",
    "  - \"-85_ctrl\"\n",
    "  - \"-84_ctrl\"\n",
    "  - \"-83_ctrl\"\n",
    "  - \"-82_ctrl\"\n",
    "  - \"-81_ctrl\"\n",
    "  - \"-80_ctrl\"\n",
    "  - \"-79_ctrl\"\n",
    "  - \"-78_ctrl\"\n",
    "  - \"-77_ctrl\"\n",
    "  - \"-76_ctrl\"\n",
    "  - \"-75_ctrl\"\n",
    "  - \"-74_ctrl\"\n",
    "  - \"-73_ctrl\"\n",
    "  - \"-72_ctrl\"\n",
    "  - \"-71_ctrl\"\n",
    "  - \"-70_ctrl\"\n",
    "  - \"-69_ctrl\"\n",
    "  - \"-68_ctrl\"\n",
    "  - \"-67_ctrl\"\n",
    "  - \"-66_ctrl\"\n",
    "  - \"-65_ctrl\"\n",
    "  - \"-64_ctrl\"\n",
    "  - \"-63_ctrl\"\n",
    "  - \"-62_ctrl\"\n",
    "  - \"-61_ctrl\"\n",
    "  - \"-60_ctrl\"\n",
    "  - \"-59_ctrl\"\n",
    "  - \"-58_ctrl\"\n",
    "  - \"-57_ctrl\"\n",
    "  - \"-56_ctrl\"\n",
    "  - \"-55_ctrl\"\n",
    "  - \"-54_ctrl\"\n",
    "  - \"-53_ctrl\"\n",
    "  - \"-52_ctrl\"\n",
    "  - \"-51_ctrl\"\n",
    "  - \"-50_ctrl\"\n",
    "  - \"-49_ctrl\"\n",
    "  - \"-48_ctrl\"\n",
    "  - \"-47_ctrl\"\n",
    "  - \"-46_ctrl\"\n",
    "  - \"-45_ctrl\"\n",
    "  - \"-44_ctrl\"\n",
    "  - \"-43_ctrl\"\n",
    "  - \"-42_ctrl\"\n",
    "  - \"-41_ctrl\"\n",
    "  - \"-40_ctrl\"\n",
    "  - \"-39_ctrl\"\n",
    "  - \"-38_ctrl\"\n",
    "  - \"-37_ctrl\"\n",
    "  - \"-36_ctrl\"\n",
    "  - \"-35_ctrl\"\n",
    "  - \"-34_ctrl\"\n",
    "  - \"-33_ctrl\"\n",
    "  - \"-32_ctrl\"\n",
    "  - \"-31_ctrl\"\n",
    "  - \"-30_ctrl\"\n",
    "  - \"-29_ctrl\"\n",
    "  - \"-28_ctrl\"\n",
    "  - \"-27_ctrl\"\n",
    "  - \"-26_ctrl\"\n",
    "  - \"-25_ctrl\"\n",
    "  - \"-24_ctrl\"\n",
    "  - \"-23_ctrl\"\n",
    "  - \"-22_ctrl\"\n",
    "  - \"-21_ctrl\"\n",
    "  - \"-20_ctrl\"\n",
    "  - \"-19_ctrl\"\n",
    "  - \"-18_ctrl\"\n",
    "  - \"-17_ctrl\"\n",
    "  - \"-16_ctrl\"\n",
    "  - \"-15_ctrl\"\n",
    "  - \"-14_ctrl\"\n",
    "  - \"-13_ctrl\"\n",
    "  - \"-12_ctrl\"\n",
    "  - \"-11_ctrl\"\n",
    "  - \"-10_ctrl\"\n",
    "  - \"-9_ctrl\"\n",
    "  - \"-8_ctrl\"\n",
    "  - \"-7_ctrl\"\n",
    "  - \"-6_ctrl\"\n",
    "  - \"-5_ctrl\"\n",
    "  - \"-4_ctrl\"\n",
    "  - \"-3_ctrl\"\n",
    "  - \"-2_ctrl\"\n",
    "  - \"-1_ctrl\"\n",
    "  - \"0_ctrl\"\n",
    "  - \"1_ctrl\"\n",
    "  - \"2_ctrl\"\n",
    "  - \"3_ctrl\"\n",
    "  - \"4_ctrl\"\n",
    "  - \"5_ctrl\"\n",
    "  - \"6_ctrl\"\n",
    "  - \"7_ctrl\"\n",
    "  - \"8_ctrl\"\n",
    "  - \"9_ctrl\"\n",
    "  - \"10_ctrl\"\n",
    "  - \"11_ctrl\"\n",
    "  - \"12_ctrl\"\n",
    "  - \"13_ctrl\"\n",
    "  - \"14_ctrl\"\n",
    "  - \"15_ctrl\"\n",
    "  - \"16_ctrl\"\n",
    "  - \"17_ctrl\"\n",
    "  - \"18_ctrl\"\n",
    "  - \"19_ctrl\"\n",
    "  - \"20_ctrl\"\n",
    "  - \"21_ctrl\"\n",
    "  - \"22_ctrl\"\n",
    "  - \"23_ctrl\"\n",
    "  - \"24_ctrl\"\n",
    "  - \"25_ctrl\"\n",
    "  - \"26_ctrl\"\n",
    "  - \"27_ctrl\"\n",
    "  - \"28_ctrl\"\n",
    "  - \"29_ctrl\"\n",
    "  - \"30_ctrl\"\n",
    "  - \"31_ctrl\"\n",
    "  - \"32_ctrl\"\n",
    "  - \"33_ctrl\"\n",
    "  - \"34_ctrl\"\n",
    "  - \"35_ctrl\"\n",
    "  - \"36_ctrl\"\n",
    "  - \"37_ctrl\"\n",
    "  - \"38_ctrl\"\n",
    "  - \"39_ctrl\"\n",
    "  - \"40_ctrl\"\n",
    "  - \"41_ctrl\"\n",
    "  - \"42_ctrl\"\n",
    "  - \"43_ctrl\"\n",
    "  - \"44_ctrl\"\n",
    "  - \"45_ctrl\"\n",
    "  - \"46_ctrl\"\n",
    "  - \"47_ctrl\"\n",
    "  - \"48_ctrl\"\n",
    "  - \"49_ctrl\"\n",
    "  - \"50_ctrl\"\n",
    "  - \"51_ctrl\"\n",
    "  - \"52_ctrl\"\n",
    "  - \"53_ctrl\"\n",
    "  - \"54_ctrl\"\n",
    "  - \"55_ctrl\"\n",
    "  - \"56_ctrl\"\n",
    "  - \"57_ctrl\"\n",
    "  - \"58_ctrl\"\n",
    "  - \"59_ctrl\"\n",
    "  - \"60_ctrl\"\n",
    "  - \"61_ctrl\"\n",
    "  - \"62_ctrl\"\n",
    "  - \"63_ctrl\"\n",
    "  - \"64_ctrl\"\n",
    "  - \"65_ctrl\"\n",
    "  - \"66_ctrl\"\n",
    "  - \"67_ctrl\"\n",
    "  - \"68_ctrl\"\n",
    "  - \"69_ctrl\"\n",
    "  - \"70_ctrl\"\n",
    "  - \"71_ctrl\"\n",
    "  - \"72_ctrl\"\n",
    "  - \"73_ctrl\"\n",
    "  - \"74_ctrl\"\n",
    "  - \"75_ctrl\"\n",
    "  - \"76_ctrl\"\n",
    "  - \"77_ctrl\"\n",
    "  - \"78_ctrl\"\n",
    "  - \"79_ctrl\"\n",
    "  - \"80_ctrl\"\n",
    "  - \"81_ctrl\"\n",
    "  - \"82_ctrl\"\n",
    "  - \"83_ctrl\"\n",
    "  - \"84_ctrl\"\n",
    "  - \"85_ctrl\"\n",
    "  - \"86_ctrl\"\n",
    "  - \"87_ctrl\"\n",
    "  - \"88_ctrl\"\n",
    "  - \"89_ctrl\"\n",
    "  - \"90_ctrl\"\n",
    "  - \"91_ctrl\"\n",
    "  - \"92_ctrl\"\n",
    "  - \"93_ctrl\"\n",
    "  - \"94_ctrl\"\n",
    "  - \"95_ctrl\"\n",
    "  - \"96_ctrl\"\n",
    "  - \"97_ctrl\"\n",
    "  - \"98_ctrl\"\n",
    "  - \"99_ctrl\"\n",
    "  - \"100_ctrl\"\n",
    "  - \"101_ctrl\"\n",
    "  - \"102_ctrl\"\n",
    "  - \"103_ctrl\"\n",
    "  - \"104_ctrl\"\n",
    "  - \"105_ctrl\"\n",
    "  - \"106_ctrl\"\n",
    "  - \"107_ctrl\"\n",
    "  - \"108_ctrl\"\n",
    "  - \"109_ctrl\"\n",
    "  - \"110_ctrl\"\n",
    "  - \"111_ctrl\"\n",
    "  - \"112_ctrl\"\n",
    "  - \"113_ctrl\"\n",
    "  - \"114_ctrl\"\n",
    "  - \"115_ctrl\"\n",
    "  - \"116_ctrl\"\n",
    "  - \"117_ctrl\"\n",
    "  - \"118_ctrl\"\n",
    "  - \"119_ctrl\"\n",
    "  - \"120_ctrl\"\n",
    "  - \"121_ctrl\"\n",
    "  - \"122_ctrl\"\n",
    "  - \"123_ctrl\"\n",
    "  - \"124_ctrl\"\n",
    "  - \"125_ctrl\"\n",
    "  - \"126_ctrl\"\n",
    "  - \"127_ctrl\"\n",
    "  - \"128_ctrl\"\n",
    "  - \"129_ctrl\"\n",
    "  - \"130_ctrl\"\n",
    "  - \"131_ctrl\"\n",
    "  - \"132_ctrl\"\n",
    "  - \"133_ctrl\"\n",
    "  - \"134_ctrl\"\n",
    "  - \"135_ctrl\"\n",
    "  - \"136_ctrl\"\n",
    "  - \"137_ctrl\"\n",
    "  - \"138_ctrl\"\n",
    "  - \"139_ctrl\"\n",
    "  - \"140_ctrl\"\n",
    "  - \"141_ctrl\"\n",
    "  - \"142_ctrl\"\n",
    "  - \"143_ctrl\"\n",
    "  - \"144_ctrl\"\n",
    "  - \"145_ctrl\"\n",
    "  - \"146_ctrl\"\n",
    "  - \"147_ctrl\"\n",
    "  - \"148_ctrl\"\n",
    "  - \"149_ctrl\"\n",
    "  - \"150_ctrl\"\n",
    "  - \"151_ctrl\"\n",
    "  - \"152_ctrl\"\n",
    "  - \"153_ctrl\"\n",
    "  - \"154_ctrl\"\n",
    "  - \"155_ctrl\"\n",
    "  - \"156_ctrl\"\n",
    "  - \"157_ctrl\"\n",
    "  - \"158_ctrl\"\n",
    "  - \"159_ctrl\"\n",
    "  - \"160_ctrl\"\n",
    "  - \"161_ctrl\"\n",
    "  - \"162_ctrl\"\n",
    "  - \"163_ctrl\"\n",
    "  - \"164_ctrl\"\n",
    "  - \"165_ctrl\"\n",
    "  - \"166_ctrl\"\n",
    "  - \"167_ctrl\"\n",
    "  - \"168_ctrl\"\n",
    "  - \"169_ctrl\"\n",
    "  - \"170_ctrl\"\n",
    "  - \"171_ctrl\"\n",
    "  - \"172_ctrl\"\n",
    "  - \"173_ctrl\"\n",
    "  - \"174_ctrl\"\n",
    "  - \"175_ctrl\"\n",
    "  - \"176_ctrl\"\n",
    "  - \"177_ctrl\"\n",
    "  - \"178_ctrl\"\n",
    "  - \"179_ctrl\"\n",
    "  - \"180_ctrl\"\n",
    "  - \"181_ctrl\"\n",
    "  - \"182_ctrl\"\n",
    "  - \"183_ctrl\"\n",
    "  - \"184_ctrl\"\n",
    "  - \"185_ctrl\"\n",
    "  - \"186_ctrl\"\n",
    "  - \"187_ctrl\"\n",
    "  - \"188_ctrl\"\n",
    "  - \"189_ctrl\"\n",
    "  - \"190_ctrl\"\n",
    "  - \"191_ctrl\"\n",
    "  - \"192_ctrl\"\n",
    "  - \"193_ctrl\"\n",
    "  - \"194_ctrl\"\n",
    "  - \"195_ctrl\"\n",
    "  - \"196_ctrl\"\n",
    "  - \"197_ctrl\"\n",
    "  - \"198_ctrl\"\n",
    "  - \"199_ctrl\"\n",
    "  - \"200_ctrl\"\n",
    "  \"\"\"\n",
    "}\n",
    "\n",
    "enhancer_centric_yaml_contents = {\"fusion.yaml\":\"\"\"  \n",
    "model_config:\n",
    "  fc_do: 0.4\n",
    "  fc_task_dim: 256\n",
    "  layers:\n",
    "  - 2\n",
    "  rb_do: 0.4\n",
    "  stochastic_depth_p: 0.5\n",
    "model_type: \"mlp-residual\"\n",
    "\"\"\",\n",
    "\"globals.yaml\": \"\"\"\n",
    "\n",
    "checkpoint_interval: 30300 # 100 epochs #60000\n",
    "sample_interval: 30300 #60000\n",
    "n_epochs: 120\n",
    "batch_size: 64\n",
    "optimizer: \"adamw\"\n",
    "lr: 0.0001 #0.0001\n",
    "device: \"cuda\"\n",
    "compute_attributions: false\n",
    "\"\"\",\n",
    "\"input_cnn.yaml\": \"\"\"\n",
    "input_info:\n",
    "  input_source: ./inputs/perturbed_landscape_arrays/enhancer_arrays/  #inserted_enhancer_test #inputs/silenced_arrays/silenced_arrays_H2B_S.D \n",
    "  #./data/parsed_data/inputs/arrays_train_100bp_no_H3K27ac/ #arrays_train_100bp_no_H3K27ac_uncoupled/\n",
    "  input_name: gene_expression\n",
    "  input_type: array\n",
    "\n",
    "model_config:\n",
    "  model_type: cnn\n",
    "  #pre_normalization: \"instancenorm\"\n",
    "  model_init_config:\n",
    "    num_output_features: 512 # before fc_repr_dim\n",
    "    layers: [4,4]\n",
    "    kernel_height: 1\n",
    "    down_stride_width: 2\n",
    "    first_stride_expansion_width: 1\n",
    "    first_kernel_expansion_height: 4 #5\n",
    "    kernel_width: 10  #10\n",
    "    dilation_factor_width: 2\n",
    "    dilation_factor_height: 1\n",
    "    channel_exp_base: 5  #3 #-1\n",
    "    first_channel_expansion: 2\n",
    "    rb_do: .3\n",
    "    stochastic_depth_p: .1\n",
    "    attention_inclusion_cutoff: 1 #50 #256\n",
    "    \"\"\",\n",
    "\"outputs_2_cond.yaml\": \"\"\"\n",
    "output_info:\n",
    "  output_name: expression_output\n",
    "  output_source: ./targets/Enhancer_centered_targets.csv \n",
    "  #./data/parsed_data/targets/target_arrays_perturbational_inserted_enhancer.csv #target_arrays_perturbational_S.D.csv #target_arrays_1kbp_401_bins_2_conditions_decareads_abs.csv\n",
    "  output_type: tabular\n",
    "\n",
    "model_config: # <- new\n",
    "  model_type: linear # <- new\n",
    "\n",
    "output_type_info:\n",
    "  con_loss_name: \"SmoothL1Loss\"\n",
    "  target_con_columns:\n",
    "  - \"-200_ctrl\"\n",
    "  - \"-199_ctrl\"\n",
    "  - \"-198_ctrl\"\n",
    "  - \"-197_ctrl\"\n",
    "  - \"-196_ctrl\"\n",
    "  - \"-195_ctrl\"\n",
    "  - \"-194_ctrl\"\n",
    "  - \"-193_ctrl\"\n",
    "  - \"-192_ctrl\"\n",
    "  - \"-191_ctrl\"\n",
    "  - \"-190_ctrl\"\n",
    "  - \"-189_ctrl\"\n",
    "  - \"-188_ctrl\"\n",
    "  - \"-187_ctrl\"\n",
    "  - \"-186_ctrl\"\n",
    "  - \"-185_ctrl\"\n",
    "  - \"-184_ctrl\"\n",
    "  - \"-183_ctrl\"\n",
    "  - \"-182_ctrl\"\n",
    "  - \"-181_ctrl\"\n",
    "  - \"-180_ctrl\"\n",
    "  - \"-179_ctrl\"\n",
    "  - \"-178_ctrl\"\n",
    "  - \"-177_ctrl\"\n",
    "  - \"-176_ctrl\"\n",
    "  - \"-175_ctrl\"\n",
    "  - \"-174_ctrl\"\n",
    "  - \"-173_ctrl\"\n",
    "  - \"-172_ctrl\"\n",
    "  - \"-171_ctrl\"\n",
    "  - \"-170_ctrl\"\n",
    "  - \"-169_ctrl\"\n",
    "  - \"-168_ctrl\"\n",
    "  - \"-167_ctrl\"\n",
    "  - \"-166_ctrl\"\n",
    "  - \"-165_ctrl\"\n",
    "  - \"-164_ctrl\"\n",
    "  - \"-163_ctrl\"\n",
    "  - \"-162_ctrl\"\n",
    "  - \"-161_ctrl\"\n",
    "  - \"-160_ctrl\"\n",
    "  - \"-159_ctrl\"\n",
    "  - \"-158_ctrl\"\n",
    "  - \"-157_ctrl\"\n",
    "  - \"-156_ctrl\"\n",
    "  - \"-155_ctrl\"\n",
    "  - \"-154_ctrl\"\n",
    "  - \"-153_ctrl\"\n",
    "  - \"-152_ctrl\"\n",
    "  - \"-151_ctrl\"\n",
    "  - \"-150_ctrl\"\n",
    "  - \"-149_ctrl\"\n",
    "  - \"-148_ctrl\"\n",
    "  - \"-147_ctrl\"\n",
    "  - \"-146_ctrl\"\n",
    "  - \"-145_ctrl\"\n",
    "  - \"-144_ctrl\"\n",
    "  - \"-143_ctrl\"\n",
    "  - \"-142_ctrl\"\n",
    "  - \"-141_ctrl\"\n",
    "  - \"-140_ctrl\"\n",
    "  - \"-139_ctrl\"\n",
    "  - \"-138_ctrl\"\n",
    "  - \"-137_ctrl\"\n",
    "  - \"-136_ctrl\"\n",
    "  - \"-135_ctrl\"\n",
    "  - \"-134_ctrl\"\n",
    "  - \"-133_ctrl\"\n",
    "  - \"-132_ctrl\"\n",
    "  - \"-131_ctrl\"\n",
    "  - \"-130_ctrl\"\n",
    "  - \"-129_ctrl\"\n",
    "  - \"-128_ctrl\"\n",
    "  - \"-127_ctrl\"\n",
    "  - \"-126_ctrl\"\n",
    "  - \"-125_ctrl\"\n",
    "  - \"-124_ctrl\"\n",
    "  - \"-123_ctrl\"\n",
    "  - \"-122_ctrl\"\n",
    "  - \"-121_ctrl\"\n",
    "  - \"-120_ctrl\"\n",
    "  - \"-119_ctrl\"\n",
    "  - \"-118_ctrl\"\n",
    "  - \"-117_ctrl\"\n",
    "  - \"-116_ctrl\"\n",
    "  - \"-115_ctrl\"\n",
    "  - \"-114_ctrl\"\n",
    "  - \"-113_ctrl\"\n",
    "  - \"-112_ctrl\"\n",
    "  - \"-111_ctrl\"\n",
    "  - \"-110_ctrl\"\n",
    "  - \"-109_ctrl\"\n",
    "  - \"-108_ctrl\"\n",
    "  - \"-107_ctrl\"\n",
    "  - \"-106_ctrl\"\n",
    "  - \"-105_ctrl\"\n",
    "  - \"-104_ctrl\"\n",
    "  - \"-103_ctrl\"\n",
    "  - \"-102_ctrl\"\n",
    "  - \"-101_ctrl\"\n",
    "  - \"-100_ctrl\"\n",
    "  - \"-99_ctrl\"\n",
    "  - \"-98_ctrl\"\n",
    "  - \"-97_ctrl\"\n",
    "  - \"-96_ctrl\"\n",
    "  - \"-95_ctrl\"\n",
    "  - \"-94_ctrl\"\n",
    "  - \"-93_ctrl\"\n",
    "  - \"-92_ctrl\"\n",
    "  - \"-91_ctrl\"\n",
    "  - \"-90_ctrl\"\n",
    "  - \"-89_ctrl\"\n",
    "  - \"-88_ctrl\"\n",
    "  - \"-87_ctrl\"\n",
    "  - \"-86_ctrl\"\n",
    "  - \"-85_ctrl\"\n",
    "  - \"-84_ctrl\"\n",
    "  - \"-83_ctrl\"\n",
    "  - \"-82_ctrl\"\n",
    "  - \"-81_ctrl\"\n",
    "  - \"-80_ctrl\"\n",
    "  - \"-79_ctrl\"\n",
    "  - \"-78_ctrl\"\n",
    "  - \"-77_ctrl\"\n",
    "  - \"-76_ctrl\"\n",
    "  - \"-75_ctrl\"\n",
    "  - \"-74_ctrl\"\n",
    "  - \"-73_ctrl\"\n",
    "  - \"-72_ctrl\"\n",
    "  - \"-71_ctrl\"\n",
    "  - \"-70_ctrl\"\n",
    "  - \"-69_ctrl\"\n",
    "  - \"-68_ctrl\"\n",
    "  - \"-67_ctrl\"\n",
    "  - \"-66_ctrl\"\n",
    "  - \"-65_ctrl\"\n",
    "  - \"-64_ctrl\"\n",
    "  - \"-63_ctrl\"\n",
    "  - \"-62_ctrl\"\n",
    "  - \"-61_ctrl\"\n",
    "  - \"-60_ctrl\"\n",
    "  - \"-59_ctrl\"\n",
    "  - \"-58_ctrl\"\n",
    "  - \"-57_ctrl\"\n",
    "  - \"-56_ctrl\"\n",
    "  - \"-55_ctrl\"\n",
    "  - \"-54_ctrl\"\n",
    "  - \"-53_ctrl\"\n",
    "  - \"-52_ctrl\"\n",
    "  - \"-51_ctrl\"\n",
    "  - \"-50_ctrl\"\n",
    "  - \"-49_ctrl\"\n",
    "  - \"-48_ctrl\"\n",
    "  - \"-47_ctrl\"\n",
    "  - \"-46_ctrl\"\n",
    "  - \"-45_ctrl\"\n",
    "  - \"-44_ctrl\"\n",
    "  - \"-43_ctrl\"\n",
    "  - \"-42_ctrl\"\n",
    "  - \"-41_ctrl\"\n",
    "  - \"-40_ctrl\"\n",
    "  - \"-39_ctrl\"\n",
    "  - \"-38_ctrl\"\n",
    "  - \"-37_ctrl\"\n",
    "  - \"-36_ctrl\"\n",
    "  - \"-35_ctrl\"\n",
    "  - \"-34_ctrl\"\n",
    "  - \"-33_ctrl\"\n",
    "  - \"-32_ctrl\"\n",
    "  - \"-31_ctrl\"\n",
    "  - \"-30_ctrl\"\n",
    "  - \"-29_ctrl\"\n",
    "  - \"-28_ctrl\"\n",
    "  - \"-27_ctrl\"\n",
    "  - \"-26_ctrl\"\n",
    "  - \"-25_ctrl\"\n",
    "  - \"-24_ctrl\"\n",
    "  - \"-23_ctrl\"\n",
    "  - \"-22_ctrl\"\n",
    "  - \"-21_ctrl\"\n",
    "  - \"-20_ctrl\"\n",
    "  - \"-19_ctrl\"\n",
    "  - \"-18_ctrl\"\n",
    "  - \"-17_ctrl\"\n",
    "  - \"-16_ctrl\"\n",
    "  - \"-15_ctrl\"\n",
    "  - \"-14_ctrl\"\n",
    "  - \"-13_ctrl\"\n",
    "  - \"-12_ctrl\"\n",
    "  - \"-11_ctrl\"\n",
    "  - \"-10_ctrl\"\n",
    "  - \"-9_ctrl\"\n",
    "  - \"-8_ctrl\"\n",
    "  - \"-7_ctrl\"\n",
    "  - \"-6_ctrl\"\n",
    "  - \"-5_ctrl\"\n",
    "  - \"-4_ctrl\"\n",
    "  - \"-3_ctrl\"\n",
    "  - \"-2_ctrl\"\n",
    "  - \"-1_ctrl\"\n",
    "  - \"0_ctrl\"\n",
    "  - \"1_ctrl\"\n",
    "  - \"2_ctrl\"\n",
    "  - \"3_ctrl\"\n",
    "  - \"4_ctrl\"\n",
    "  - \"5_ctrl\"\n",
    "  - \"6_ctrl\"\n",
    "  - \"7_ctrl\"\n",
    "  - \"8_ctrl\"\n",
    "  - \"9_ctrl\"\n",
    "  - \"10_ctrl\"\n",
    "  - \"11_ctrl\"\n",
    "  - \"12_ctrl\"\n",
    "  - \"13_ctrl\"\n",
    "  - \"14_ctrl\"\n",
    "  - \"15_ctrl\"\n",
    "  - \"16_ctrl\"\n",
    "  - \"17_ctrl\"\n",
    "  - \"18_ctrl\"\n",
    "  - \"19_ctrl\"\n",
    "  - \"20_ctrl\"\n",
    "  - \"21_ctrl\"\n",
    "  - \"22_ctrl\"\n",
    "  - \"23_ctrl\"\n",
    "  - \"24_ctrl\"\n",
    "  - \"25_ctrl\"\n",
    "  - \"26_ctrl\"\n",
    "  - \"27_ctrl\"\n",
    "  - \"28_ctrl\"\n",
    "  - \"29_ctrl\"\n",
    "  - \"30_ctrl\"\n",
    "  - \"31_ctrl\"\n",
    "  - \"32_ctrl\"\n",
    "  - \"33_ctrl\"\n",
    "  - \"34_ctrl\"\n",
    "  - \"35_ctrl\"\n",
    "  - \"36_ctrl\"\n",
    "  - \"37_ctrl\"\n",
    "  - \"38_ctrl\"\n",
    "  - \"39_ctrl\"\n",
    "  - \"40_ctrl\"\n",
    "  - \"41_ctrl\"\n",
    "  - \"42_ctrl\"\n",
    "  - \"43_ctrl\"\n",
    "  - \"44_ctrl\"\n",
    "  - \"45_ctrl\"\n",
    "  - \"46_ctrl\"\n",
    "  - \"47_ctrl\"\n",
    "  - \"48_ctrl\"\n",
    "  - \"49_ctrl\"\n",
    "  - \"50_ctrl\"\n",
    "  - \"51_ctrl\"\n",
    "  - \"52_ctrl\"\n",
    "  - \"53_ctrl\"\n",
    "  - \"54_ctrl\"\n",
    "  - \"55_ctrl\"\n",
    "  - \"56_ctrl\"\n",
    "  - \"57_ctrl\"\n",
    "  - \"58_ctrl\"\n",
    "  - \"59_ctrl\"\n",
    "  - \"60_ctrl\"\n",
    "  - \"61_ctrl\"\n",
    "  - \"62_ctrl\"\n",
    "  - \"63_ctrl\"\n",
    "  - \"64_ctrl\"\n",
    "  - \"65_ctrl\"\n",
    "  - \"66_ctrl\"\n",
    "  - \"67_ctrl\"\n",
    "  - \"68_ctrl\"\n",
    "  - \"69_ctrl\"\n",
    "  - \"70_ctrl\"\n",
    "  - \"71_ctrl\"\n",
    "  - \"72_ctrl\"\n",
    "  - \"73_ctrl\"\n",
    "  - \"74_ctrl\"\n",
    "  - \"75_ctrl\"\n",
    "  - \"76_ctrl\"\n",
    "  - \"77_ctrl\"\n",
    "  - \"78_ctrl\"\n",
    "  - \"79_ctrl\"\n",
    "  - \"80_ctrl\"\n",
    "  - \"81_ctrl\"\n",
    "  - \"82_ctrl\"\n",
    "  - \"83_ctrl\"\n",
    "  - \"84_ctrl\"\n",
    "  - \"85_ctrl\"\n",
    "  - \"86_ctrl\"\n",
    "  - \"87_ctrl\"\n",
    "  - \"88_ctrl\"\n",
    "  - \"89_ctrl\"\n",
    "  - \"90_ctrl\"\n",
    "  - \"91_ctrl\"\n",
    "  - \"92_ctrl\"\n",
    "  - \"93_ctrl\"\n",
    "  - \"94_ctrl\"\n",
    "  - \"95_ctrl\"\n",
    "  - \"96_ctrl\"\n",
    "  - \"97_ctrl\"\n",
    "  - \"98_ctrl\"\n",
    "  - \"99_ctrl\"\n",
    "  - \"100_ctrl\"\n",
    "  - \"101_ctrl\"\n",
    "  - \"102_ctrl\"\n",
    "  - \"103_ctrl\"\n",
    "  - \"104_ctrl\"\n",
    "  - \"105_ctrl\"\n",
    "  - \"106_ctrl\"\n",
    "  - \"107_ctrl\"\n",
    "  - \"108_ctrl\"\n",
    "  - \"109_ctrl\"\n",
    "  - \"110_ctrl\"\n",
    "  - \"111_ctrl\"\n",
    "  - \"112_ctrl\"\n",
    "  - \"113_ctrl\"\n",
    "  - \"114_ctrl\"\n",
    "  - \"115_ctrl\"\n",
    "  - \"116_ctrl\"\n",
    "  - \"117_ctrl\"\n",
    "  - \"118_ctrl\"\n",
    "  - \"119_ctrl\"\n",
    "  - \"120_ctrl\"\n",
    "  - \"121_ctrl\"\n",
    "  - \"122_ctrl\"\n",
    "  - \"123_ctrl\"\n",
    "  - \"124_ctrl\"\n",
    "  - \"125_ctrl\"\n",
    "  - \"126_ctrl\"\n",
    "  - \"127_ctrl\"\n",
    "  - \"128_ctrl\"\n",
    "  - \"129_ctrl\"\n",
    "  - \"130_ctrl\"\n",
    "  - \"131_ctrl\"\n",
    "  - \"132_ctrl\"\n",
    "  - \"133_ctrl\"\n",
    "  - \"134_ctrl\"\n",
    "  - \"135_ctrl\"\n",
    "  - \"136_ctrl\"\n",
    "  - \"137_ctrl\"\n",
    "  - \"138_ctrl\"\n",
    "  - \"139_ctrl\"\n",
    "  - \"140_ctrl\"\n",
    "  - \"141_ctrl\"\n",
    "  - \"142_ctrl\"\n",
    "  - \"143_ctrl\"\n",
    "  - \"144_ctrl\"\n",
    "  - \"145_ctrl\"\n",
    "  - \"146_ctrl\"\n",
    "  - \"147_ctrl\"\n",
    "  - \"148_ctrl\"\n",
    "  - \"149_ctrl\"\n",
    "  - \"150_ctrl\"\n",
    "  - \"151_ctrl\"\n",
    "  - \"152_ctrl\"\n",
    "  - \"153_ctrl\"\n",
    "  - \"154_ctrl\"\n",
    "  - \"155_ctrl\"\n",
    "  - \"156_ctrl\"\n",
    "  - \"157_ctrl\"\n",
    "  - \"158_ctrl\"\n",
    "  - \"159_ctrl\"\n",
    "  - \"160_ctrl\"\n",
    "  - \"161_ctrl\"\n",
    "  - \"162_ctrl\"\n",
    "  - \"163_ctrl\"\n",
    "  - \"164_ctrl\"\n",
    "  - \"165_ctrl\"\n",
    "  - \"166_ctrl\"\n",
    "  - \"167_ctrl\"\n",
    "  - \"168_ctrl\"\n",
    "  - \"169_ctrl\"\n",
    "  - \"170_ctrl\"\n",
    "  - \"171_ctrl\"\n",
    "  - \"172_ctrl\"\n",
    "  - \"173_ctrl\"\n",
    "  - \"174_ctrl\"\n",
    "  - \"175_ctrl\"\n",
    "  - \"176_ctrl\"\n",
    "  - \"177_ctrl\"\n",
    "  - \"178_ctrl\"\n",
    "  - \"179_ctrl\"\n",
    "  - \"180_ctrl\"\n",
    "  - \"181_ctrl\"\n",
    "  - \"182_ctrl\"\n",
    "  - \"183_ctrl\"\n",
    "  - \"184_ctrl\"\n",
    "  - \"185_ctrl\"\n",
    "  - \"186_ctrl\"\n",
    "  - \"187_ctrl\"\n",
    "  - \"188_ctrl\"\n",
    "  - \"189_ctrl\"\n",
    "  - \"190_ctrl\"\n",
    "  - \"191_ctrl\"\n",
    "  - \"192_ctrl\"\n",
    "  - \"193_ctrl\"\n",
    "  - \"194_ctrl\"\n",
    "  - \"195_ctrl\"\n",
    "  - \"196_ctrl\"\n",
    "  - \"197_ctrl\"\n",
    "  - \"198_ctrl\"\n",
    "  - \"199_ctrl\"\n",
    "  - \"200_ctrl\"\n",
    "  \"\"\" }\n",
    "\n",
    "training_K562_yaml_contents = {\"globals.yaml\":\"\"\"\n",
    "output_folder: ./runs/gene_expression_only_chrom_K562_train/ \n",
    "manual_valid_ids_file: ./annotations/human/manual_validation_ids_chr17_human.txt  #manual_validation_ids_chr17_uncoupled.txt\n",
    "checkpoint_interval: 30300 # 100 epochs #60000\n",
    "sample_interval: 30300 #60000\n",
    "n_epochs: 120\n",
    "batch_size: 64\n",
    "optimizer: \"adamw\"\n",
    "lr: 0.0001 #0.0001\n",
    "device: \"cuda\"\n",
    "compute_attributions: false\n",
    "\n",
    "\"\"\",\n",
    "\"input_cnn.yaml\": \"\"\" \n",
    "input_info:\n",
    "  input_source: ./inputs/landscape_arrays/K562/training/ \n",
    "  input_name: gene_expression\n",
    "  input_type: array\n",
    "\n",
    "model_config:\n",
    "  model_type: cnn\n",
    "  #pre_normalization: \"instancenorm\"\n",
    "  model_init_config:\n",
    "    num_output_features: 512 # before fc_repr_dim\n",
    "    layers: [4,4]\n",
    "    kernel_height: 1\n",
    "    down_stride_width: 2\n",
    "    first_stride_expansion_width: 1\n",
    "    first_kernel_expansion_height: 4 #5\n",
    "    kernel_width: 10  #10\n",
    "    dilation_factor_width: 2\n",
    "    dilation_factor_height: 1\n",
    "    channel_exp_base: 5  #3 #-1\n",
    "    first_channel_expansion: 2\n",
    "    rb_do: .3\n",
    "    stochastic_depth_p: .1\n",
    "    attention_inclusion_cutoff: 1 #50 #256\n",
    "\n",
    "\"\"\",\n",
    "\"fusion.yaml\": \"\"\"\n",
    "model_config:\n",
    "  fc_do: 0.4\n",
    "  fc_task_dim: 256\n",
    "  layers:\n",
    "  - 2\n",
    "  rb_do: 0.4\n",
    "  stochastic_depth_p: 0.5\n",
    "model_type: \"mlp-residual\"\n",
    "\"\"\",\n",
    "\"outputs_2_cond.yaml\":\"\"\"\n",
    "output_info:\n",
    "  output_name: expression_output\n",
    "  output_source: ./targets/K562/training_targets.csv \n",
    "  output_type: tabular\n",
    "\n",
    "model_config: # <- new\n",
    "  model_type: linear # <- new\n",
    "\n",
    "output_type_info:\n",
    "  con_loss_name: \"SmoothL1Loss\"\n",
    "  target_con_columns:\n",
    "  - \"-200_ctrl\"\n",
    "  - \"-199_ctrl\"\n",
    "  - \"-198_ctrl\"\n",
    "  - \"-197_ctrl\"\n",
    "  - \"-196_ctrl\"\n",
    "  - \"-195_ctrl\"\n",
    "  - \"-194_ctrl\"\n",
    "  - \"-193_ctrl\"\n",
    "  - \"-192_ctrl\"\n",
    "  - \"-191_ctrl\"\n",
    "  - \"-190_ctrl\"\n",
    "  - \"-189_ctrl\"\n",
    "  - \"-188_ctrl\"\n",
    "  - \"-187_ctrl\"\n",
    "  - \"-186_ctrl\"\n",
    "  - \"-185_ctrl\"\n",
    "  - \"-184_ctrl\"\n",
    "  - \"-183_ctrl\"\n",
    "  - \"-182_ctrl\"\n",
    "  - \"-181_ctrl\"\n",
    "  - \"-180_ctrl\"\n",
    "  - \"-179_ctrl\"\n",
    "  - \"-178_ctrl\"\n",
    "  - \"-177_ctrl\"\n",
    "  - \"-176_ctrl\"\n",
    "  - \"-175_ctrl\"\n",
    "  - \"-174_ctrl\"\n",
    "  - \"-173_ctrl\"\n",
    "  - \"-172_ctrl\"\n",
    "  - \"-171_ctrl\"\n",
    "  - \"-170_ctrl\"\n",
    "  - \"-169_ctrl\"\n",
    "  - \"-168_ctrl\"\n",
    "  - \"-167_ctrl\"\n",
    "  - \"-166_ctrl\"\n",
    "  - \"-165_ctrl\"\n",
    "  - \"-164_ctrl\"\n",
    "  - \"-163_ctrl\"\n",
    "  - \"-162_ctrl\"\n",
    "  - \"-161_ctrl\"\n",
    "  - \"-160_ctrl\"\n",
    "  - \"-159_ctrl\"\n",
    "  - \"-158_ctrl\"\n",
    "  - \"-157_ctrl\"\n",
    "  - \"-156_ctrl\"\n",
    "  - \"-155_ctrl\"\n",
    "  - \"-154_ctrl\"\n",
    "  - \"-153_ctrl\"\n",
    "  - \"-152_ctrl\"\n",
    "  - \"-151_ctrl\"\n",
    "  - \"-150_ctrl\"\n",
    "  - \"-149_ctrl\"\n",
    "  - \"-148_ctrl\"\n",
    "  - \"-147_ctrl\"\n",
    "  - \"-146_ctrl\"\n",
    "  - \"-145_ctrl\"\n",
    "  - \"-144_ctrl\"\n",
    "  - \"-143_ctrl\"\n",
    "  - \"-142_ctrl\"\n",
    "  - \"-141_ctrl\"\n",
    "  - \"-140_ctrl\"\n",
    "  - \"-139_ctrl\"\n",
    "  - \"-138_ctrl\"\n",
    "  - \"-137_ctrl\"\n",
    "  - \"-136_ctrl\"\n",
    "  - \"-135_ctrl\"\n",
    "  - \"-134_ctrl\"\n",
    "  - \"-133_ctrl\"\n",
    "  - \"-132_ctrl\"\n",
    "  - \"-131_ctrl\"\n",
    "  - \"-130_ctrl\"\n",
    "  - \"-129_ctrl\"\n",
    "  - \"-128_ctrl\"\n",
    "  - \"-127_ctrl\"\n",
    "  - \"-126_ctrl\"\n",
    "  - \"-125_ctrl\"\n",
    "  - \"-124_ctrl\"\n",
    "  - \"-123_ctrl\"\n",
    "  - \"-122_ctrl\"\n",
    "  - \"-121_ctrl\"\n",
    "  - \"-120_ctrl\"\n",
    "  - \"-119_ctrl\"\n",
    "  - \"-118_ctrl\"\n",
    "  - \"-117_ctrl\"\n",
    "  - \"-116_ctrl\"\n",
    "  - \"-115_ctrl\"\n",
    "  - \"-114_ctrl\"\n",
    "  - \"-113_ctrl\"\n",
    "  - \"-112_ctrl\"\n",
    "  - \"-111_ctrl\"\n",
    "  - \"-110_ctrl\"\n",
    "  - \"-109_ctrl\"\n",
    "  - \"-108_ctrl\"\n",
    "  - \"-107_ctrl\"\n",
    "  - \"-106_ctrl\"\n",
    "  - \"-105_ctrl\"\n",
    "  - \"-104_ctrl\"\n",
    "  - \"-103_ctrl\"\n",
    "  - \"-102_ctrl\"\n",
    "  - \"-101_ctrl\"\n",
    "  - \"-100_ctrl\"\n",
    "  - \"-99_ctrl\"\n",
    "  - \"-98_ctrl\"\n",
    "  - \"-97_ctrl\"\n",
    "  - \"-96_ctrl\"\n",
    "  - \"-95_ctrl\"\n",
    "  - \"-94_ctrl\"\n",
    "  - \"-93_ctrl\"\n",
    "  - \"-92_ctrl\"\n",
    "  - \"-91_ctrl\"\n",
    "  - \"-90_ctrl\"\n",
    "  - \"-89_ctrl\"\n",
    "  - \"-88_ctrl\"\n",
    "  - \"-87_ctrl\"\n",
    "  - \"-86_ctrl\"\n",
    "  - \"-85_ctrl\"\n",
    "  - \"-84_ctrl\"\n",
    "  - \"-83_ctrl\"\n",
    "  - \"-82_ctrl\"\n",
    "  - \"-81_ctrl\"\n",
    "  - \"-80_ctrl\"\n",
    "  - \"-79_ctrl\"\n",
    "  - \"-78_ctrl\"\n",
    "  - \"-77_ctrl\"\n",
    "  - \"-76_ctrl\"\n",
    "  - \"-75_ctrl\"\n",
    "  - \"-74_ctrl\"\n",
    "  - \"-73_ctrl\"\n",
    "  - \"-72_ctrl\"\n",
    "  - \"-71_ctrl\"\n",
    "  - \"-70_ctrl\"\n",
    "  - \"-69_ctrl\"\n",
    "  - \"-68_ctrl\"\n",
    "  - \"-67_ctrl\"\n",
    "  - \"-66_ctrl\"\n",
    "  - \"-65_ctrl\"\n",
    "  - \"-64_ctrl\"\n",
    "  - \"-63_ctrl\"\n",
    "  - \"-62_ctrl\"\n",
    "  - \"-61_ctrl\"\n",
    "  - \"-60_ctrl\"\n",
    "  - \"-59_ctrl\"\n",
    "  - \"-58_ctrl\"\n",
    "  - \"-57_ctrl\"\n",
    "  - \"-56_ctrl\"\n",
    "  - \"-55_ctrl\"\n",
    "  - \"-54_ctrl\"\n",
    "  - \"-53_ctrl\"\n",
    "  - \"-52_ctrl\"\n",
    "  - \"-51_ctrl\"\n",
    "  - \"-50_ctrl\"\n",
    "  - \"-49_ctrl\"\n",
    "  - \"-48_ctrl\"\n",
    "  - \"-47_ctrl\"\n",
    "  - \"-46_ctrl\"\n",
    "  - \"-45_ctrl\"\n",
    "  - \"-44_ctrl\"\n",
    "  - \"-43_ctrl\"\n",
    "  - \"-42_ctrl\"\n",
    "  - \"-41_ctrl\"\n",
    "  - \"-40_ctrl\"\n",
    "  - \"-39_ctrl\"\n",
    "  - \"-38_ctrl\"\n",
    "  - \"-37_ctrl\"\n",
    "  - \"-36_ctrl\"\n",
    "  - \"-35_ctrl\"\n",
    "  - \"-34_ctrl\"\n",
    "  - \"-33_ctrl\"\n",
    "  - \"-32_ctrl\"\n",
    "  - \"-31_ctrl\"\n",
    "  - \"-30_ctrl\"\n",
    "  - \"-29_ctrl\"\n",
    "  - \"-28_ctrl\"\n",
    "  - \"-27_ctrl\"\n",
    "  - \"-26_ctrl\"\n",
    "  - \"-25_ctrl\"\n",
    "  - \"-24_ctrl\"\n",
    "  - \"-23_ctrl\"\n",
    "  - \"-22_ctrl\"\n",
    "  - \"-21_ctrl\"\n",
    "  - \"-20_ctrl\"\n",
    "  - \"-19_ctrl\"\n",
    "  - \"-18_ctrl\"\n",
    "  - \"-17_ctrl\"\n",
    "  - \"-16_ctrl\"\n",
    "  - \"-15_ctrl\"\n",
    "  - \"-14_ctrl\"\n",
    "  - \"-13_ctrl\"\n",
    "  - \"-12_ctrl\"\n",
    "  - \"-11_ctrl\"\n",
    "  - \"-10_ctrl\"\n",
    "  - \"-9_ctrl\"\n",
    "  - \"-8_ctrl\"\n",
    "  - \"-7_ctrl\"\n",
    "  - \"-6_ctrl\"\n",
    "  - \"-5_ctrl\"\n",
    "  - \"-4_ctrl\"\n",
    "  - \"-3_ctrl\"\n",
    "  - \"-2_ctrl\"\n",
    "  - \"-1_ctrl\"\n",
    "  - \"0_ctrl\"\n",
    "  - \"1_ctrl\"\n",
    "  - \"2_ctrl\"\n",
    "  - \"3_ctrl\"\n",
    "  - \"4_ctrl\"\n",
    "  - \"5_ctrl\"\n",
    "  - \"6_ctrl\"\n",
    "  - \"7_ctrl\"\n",
    "  - \"8_ctrl\"\n",
    "  - \"9_ctrl\"\n",
    "  - \"10_ctrl\"\n",
    "  - \"11_ctrl\"\n",
    "  - \"12_ctrl\"\n",
    "  - \"13_ctrl\"\n",
    "  - \"14_ctrl\"\n",
    "  - \"15_ctrl\"\n",
    "  - \"16_ctrl\"\n",
    "  - \"17_ctrl\"\n",
    "  - \"18_ctrl\"\n",
    "  - \"19_ctrl\"\n",
    "  - \"20_ctrl\"\n",
    "  - \"21_ctrl\"\n",
    "  - \"22_ctrl\"\n",
    "  - \"23_ctrl\"\n",
    "  - \"24_ctrl\"\n",
    "  - \"25_ctrl\"\n",
    "  - \"26_ctrl\"\n",
    "  - \"27_ctrl\"\n",
    "  - \"28_ctrl\"\n",
    "  - \"29_ctrl\"\n",
    "  - \"30_ctrl\"\n",
    "  - \"31_ctrl\"\n",
    "  - \"32_ctrl\"\n",
    "  - \"33_ctrl\"\n",
    "  - \"34_ctrl\"\n",
    "  - \"35_ctrl\"\n",
    "  - \"36_ctrl\"\n",
    "  - \"37_ctrl\"\n",
    "  - \"38_ctrl\"\n",
    "  - \"39_ctrl\"\n",
    "  - \"40_ctrl\"\n",
    "  - \"41_ctrl\"\n",
    "  - \"42_ctrl\"\n",
    "  - \"43_ctrl\"\n",
    "  - \"44_ctrl\"\n",
    "  - \"45_ctrl\"\n",
    "  - \"46_ctrl\"\n",
    "  - \"47_ctrl\"\n",
    "  - \"48_ctrl\"\n",
    "  - \"49_ctrl\"\n",
    "  - \"50_ctrl\"\n",
    "  - \"51_ctrl\"\n",
    "  - \"52_ctrl\"\n",
    "  - \"53_ctrl\"\n",
    "  - \"54_ctrl\"\n",
    "  - \"55_ctrl\"\n",
    "  - \"56_ctrl\"\n",
    "  - \"57_ctrl\"\n",
    "  - \"58_ctrl\"\n",
    "  - \"59_ctrl\"\n",
    "  - \"60_ctrl\"\n",
    "  - \"61_ctrl\"\n",
    "  - \"62_ctrl\"\n",
    "  - \"63_ctrl\"\n",
    "  - \"64_ctrl\"\n",
    "  - \"65_ctrl\"\n",
    "  - \"66_ctrl\"\n",
    "  - \"67_ctrl\"\n",
    "  - \"68_ctrl\"\n",
    "  - \"69_ctrl\"\n",
    "  - \"70_ctrl\"\n",
    "  - \"71_ctrl\"\n",
    "  - \"72_ctrl\"\n",
    "  - \"73_ctrl\"\n",
    "  - \"74_ctrl\"\n",
    "  - \"75_ctrl\"\n",
    "  - \"76_ctrl\"\n",
    "  - \"77_ctrl\"\n",
    "  - \"78_ctrl\"\n",
    "  - \"79_ctrl\"\n",
    "  - \"80_ctrl\"\n",
    "  - \"81_ctrl\"\n",
    "  - \"82_ctrl\"\n",
    "  - \"83_ctrl\"\n",
    "  - \"84_ctrl\"\n",
    "  - \"85_ctrl\"\n",
    "  - \"86_ctrl\"\n",
    "  - \"87_ctrl\"\n",
    "  - \"88_ctrl\"\n",
    "  - \"89_ctrl\"\n",
    "  - \"90_ctrl\"\n",
    "  - \"91_ctrl\"\n",
    "  - \"92_ctrl\"\n",
    "  - \"93_ctrl\"\n",
    "  - \"94_ctrl\"\n",
    "  - \"95_ctrl\"\n",
    "  - \"96_ctrl\"\n",
    "  - \"97_ctrl\"\n",
    "  - \"98_ctrl\"\n",
    "  - \"99_ctrl\"\n",
    "  - \"100_ctrl\"\n",
    "  - \"101_ctrl\"\n",
    "  - \"102_ctrl\"\n",
    "  - \"103_ctrl\"\n",
    "  - \"104_ctrl\"\n",
    "  - \"105_ctrl\"\n",
    "  - \"106_ctrl\"\n",
    "  - \"107_ctrl\"\n",
    "  - \"108_ctrl\"\n",
    "  - \"109_ctrl\"\n",
    "  - \"110_ctrl\"\n",
    "  - \"111_ctrl\"\n",
    "  - \"112_ctrl\"\n",
    "  - \"113_ctrl\"\n",
    "  - \"114_ctrl\"\n",
    "  - \"115_ctrl\"\n",
    "  - \"116_ctrl\"\n",
    "  - \"117_ctrl\"\n",
    "  - \"118_ctrl\"\n",
    "  - \"119_ctrl\"\n",
    "  - \"120_ctrl\"\n",
    "  - \"121_ctrl\"\n",
    "  - \"122_ctrl\"\n",
    "  - \"123_ctrl\"\n",
    "  - \"124_ctrl\"\n",
    "  - \"125_ctrl\"\n",
    "  - \"126_ctrl\"\n",
    "  - \"127_ctrl\"\n",
    "  - \"128_ctrl\"\n",
    "  - \"129_ctrl\"\n",
    "  - \"130_ctrl\"\n",
    "  - \"131_ctrl\"\n",
    "  - \"132_ctrl\"\n",
    "  - \"133_ctrl\"\n",
    "  - \"134_ctrl\"\n",
    "  - \"135_ctrl\"\n",
    "  - \"136_ctrl\"\n",
    "  - \"137_ctrl\"\n",
    "  - \"138_ctrl\"\n",
    "  - \"139_ctrl\"\n",
    "  - \"140_ctrl\"\n",
    "  - \"141_ctrl\"\n",
    "  - \"142_ctrl\"\n",
    "  - \"143_ctrl\"\n",
    "  - \"144_ctrl\"\n",
    "  - \"145_ctrl\"\n",
    "  - \"146_ctrl\"\n",
    "  - \"147_ctrl\"\n",
    "  - \"148_ctrl\"\n",
    "  - \"149_ctrl\"\n",
    "  - \"150_ctrl\"\n",
    "  - \"151_ctrl\"\n",
    "  - \"152_ctrl\"\n",
    "  - \"153_ctrl\"\n",
    "  - \"154_ctrl\"\n",
    "  - \"155_ctrl\"\n",
    "  - \"156_ctrl\"\n",
    "  - \"157_ctrl\"\n",
    "  - \"158_ctrl\"\n",
    "  - \"159_ctrl\"\n",
    "  - \"160_ctrl\"\n",
    "  - \"161_ctrl\"\n",
    "  - \"162_ctrl\"\n",
    "  - \"163_ctrl\"\n",
    "  - \"164_ctrl\"\n",
    "  - \"165_ctrl\"\n",
    "  - \"166_ctrl\"\n",
    "  - \"167_ctrl\"\n",
    "  - \"168_ctrl\"\n",
    "  - \"169_ctrl\"\n",
    "  - \"170_ctrl\"\n",
    "  - \"171_ctrl\"\n",
    "  - \"172_ctrl\"\n",
    "  - \"173_ctrl\"\n",
    "  - \"174_ctrl\"\n",
    "  - \"175_ctrl\"\n",
    "  - \"176_ctrl\"\n",
    "  - \"177_ctrl\"\n",
    "  - \"178_ctrl\"\n",
    "  - \"179_ctrl\"\n",
    "  - \"180_ctrl\"\n",
    "  - \"181_ctrl\"\n",
    "  - \"182_ctrl\"\n",
    "  - \"183_ctrl\"\n",
    "  - \"184_ctrl\"\n",
    "  - \"185_ctrl\"\n",
    "  - \"186_ctrl\"\n",
    "  - \"187_ctrl\"\n",
    "  - \"188_ctrl\"\n",
    "  - \"189_ctrl\"\n",
    "  - \"190_ctrl\"\n",
    "  - \"191_ctrl\"\n",
    "  - \"192_ctrl\"\n",
    "  - \"193_ctrl\"\n",
    "  - \"194_ctrl\"\n",
    "  - \"195_ctrl\"\n",
    "  - \"196_ctrl\"\n",
    "  - \"197_ctrl\"\n",
    "  - \"198_ctrl\"\n",
    "  - \"199_ctrl\"\n",
    "  - \"200_ctrl\"\n",
    "  \"\"\"\n",
    "}\n",
    "\n",
    "test_K562_yaml_contents = {\"globals.yaml\":\"\"\"\n",
    "checkpoint_interval: 30300 # 100 epochs #60000\n",
    "sample_interval: 30300 #60000\n",
    "n_epochs: 120\n",
    "batch_size: 64\n",
    "optimizer: \"adamw\"\n",
    "lr: 0.0001 #0.0001\n",
    "device: \"cuda\"\n",
    "compute_attributions: true\n",
    "\n",
    "\"\"\",\n",
    "\"input_cnn.yaml\": \"\"\" \n",
    "input_info:\n",
    "  input_source: ./inputs/landscape_arrays/K562/test/ \n",
    "  input_name: gene_expression\n",
    "  input_type: array\n",
    "\n",
    "model_config:\n",
    "  model_type: cnn\n",
    "  #pre_normalization: \"instancenorm\"\n",
    "  model_init_config:\n",
    "    num_output_features: 512 # before fc_repr_dim\n",
    "    layers: [4,4]\n",
    "    kernel_height: 1\n",
    "    down_stride_width: 2\n",
    "    first_stride_expansion_width: 1\n",
    "    first_kernel_expansion_height: 4 #5\n",
    "    kernel_width: 10  #10\n",
    "    dilation_factor_width: 2\n",
    "    dilation_factor_height: 1\n",
    "    channel_exp_base: 5  #3 #-1\n",
    "    first_channel_expansion: 2\n",
    "    rb_do: .3\n",
    "    stochastic_depth_p: .1\n",
    "    attention_inclusion_cutoff: 1 #50 #256\n",
    "\n",
    "\"\"\",\n",
    "\"fusion.yaml\": \"\"\"\n",
    "model_config:\n",
    "  fc_do: 0.4\n",
    "  fc_task_dim: 256\n",
    "  layers:\n",
    "  - 2\n",
    "  rb_do: 0.4\n",
    "  stochastic_depth_p: 0.5\n",
    "model_type: \"mlp-residual\"\n",
    "\"\"\",\n",
    "\"outputs_2_cond.yaml\":\"\"\"\n",
    "output_info:\n",
    "  output_name: expression_output\n",
    "  output_source: ./targets/K562/test_targets.csv \n",
    "  output_type: tabular\n",
    "\n",
    "model_config: # <- new\n",
    "  model_type: linear # <- new\n",
    "\n",
    "output_type_info:\n",
    "  con_loss_name: \"SmoothL1Loss\"\n",
    "  target_con_columns:\n",
    "  - \"-200_ctrl\"\n",
    "  - \"-199_ctrl\"\n",
    "  - \"-198_ctrl\"\n",
    "  - \"-197_ctrl\"\n",
    "  - \"-196_ctrl\"\n",
    "  - \"-195_ctrl\"\n",
    "  - \"-194_ctrl\"\n",
    "  - \"-193_ctrl\"\n",
    "  - \"-192_ctrl\"\n",
    "  - \"-191_ctrl\"\n",
    "  - \"-190_ctrl\"\n",
    "  - \"-189_ctrl\"\n",
    "  - \"-188_ctrl\"\n",
    "  - \"-187_ctrl\"\n",
    "  - \"-186_ctrl\"\n",
    "  - \"-185_ctrl\"\n",
    "  - \"-184_ctrl\"\n",
    "  - \"-183_ctrl\"\n",
    "  - \"-182_ctrl\"\n",
    "  - \"-181_ctrl\"\n",
    "  - \"-180_ctrl\"\n",
    "  - \"-179_ctrl\"\n",
    "  - \"-178_ctrl\"\n",
    "  - \"-177_ctrl\"\n",
    "  - \"-176_ctrl\"\n",
    "  - \"-175_ctrl\"\n",
    "  - \"-174_ctrl\"\n",
    "  - \"-173_ctrl\"\n",
    "  - \"-172_ctrl\"\n",
    "  - \"-171_ctrl\"\n",
    "  - \"-170_ctrl\"\n",
    "  - \"-169_ctrl\"\n",
    "  - \"-168_ctrl\"\n",
    "  - \"-167_ctrl\"\n",
    "  - \"-166_ctrl\"\n",
    "  - \"-165_ctrl\"\n",
    "  - \"-164_ctrl\"\n",
    "  - \"-163_ctrl\"\n",
    "  - \"-162_ctrl\"\n",
    "  - \"-161_ctrl\"\n",
    "  - \"-160_ctrl\"\n",
    "  - \"-159_ctrl\"\n",
    "  - \"-158_ctrl\"\n",
    "  - \"-157_ctrl\"\n",
    "  - \"-156_ctrl\"\n",
    "  - \"-155_ctrl\"\n",
    "  - \"-154_ctrl\"\n",
    "  - \"-153_ctrl\"\n",
    "  - \"-152_ctrl\"\n",
    "  - \"-151_ctrl\"\n",
    "  - \"-150_ctrl\"\n",
    "  - \"-149_ctrl\"\n",
    "  - \"-148_ctrl\"\n",
    "  - \"-147_ctrl\"\n",
    "  - \"-146_ctrl\"\n",
    "  - \"-145_ctrl\"\n",
    "  - \"-144_ctrl\"\n",
    "  - \"-143_ctrl\"\n",
    "  - \"-142_ctrl\"\n",
    "  - \"-141_ctrl\"\n",
    "  - \"-140_ctrl\"\n",
    "  - \"-139_ctrl\"\n",
    "  - \"-138_ctrl\"\n",
    "  - \"-137_ctrl\"\n",
    "  - \"-136_ctrl\"\n",
    "  - \"-135_ctrl\"\n",
    "  - \"-134_ctrl\"\n",
    "  - \"-133_ctrl\"\n",
    "  - \"-132_ctrl\"\n",
    "  - \"-131_ctrl\"\n",
    "  - \"-130_ctrl\"\n",
    "  - \"-129_ctrl\"\n",
    "  - \"-128_ctrl\"\n",
    "  - \"-127_ctrl\"\n",
    "  - \"-126_ctrl\"\n",
    "  - \"-125_ctrl\"\n",
    "  - \"-124_ctrl\"\n",
    "  - \"-123_ctrl\"\n",
    "  - \"-122_ctrl\"\n",
    "  - \"-121_ctrl\"\n",
    "  - \"-120_ctrl\"\n",
    "  - \"-119_ctrl\"\n",
    "  - \"-118_ctrl\"\n",
    "  - \"-117_ctrl\"\n",
    "  - \"-116_ctrl\"\n",
    "  - \"-115_ctrl\"\n",
    "  - \"-114_ctrl\"\n",
    "  - \"-113_ctrl\"\n",
    "  - \"-112_ctrl\"\n",
    "  - \"-111_ctrl\"\n",
    "  - \"-110_ctrl\"\n",
    "  - \"-109_ctrl\"\n",
    "  - \"-108_ctrl\"\n",
    "  - \"-107_ctrl\"\n",
    "  - \"-106_ctrl\"\n",
    "  - \"-105_ctrl\"\n",
    "  - \"-104_ctrl\"\n",
    "  - \"-103_ctrl\"\n",
    "  - \"-102_ctrl\"\n",
    "  - \"-101_ctrl\"\n",
    "  - \"-100_ctrl\"\n",
    "  - \"-99_ctrl\"\n",
    "  - \"-98_ctrl\"\n",
    "  - \"-97_ctrl\"\n",
    "  - \"-96_ctrl\"\n",
    "  - \"-95_ctrl\"\n",
    "  - \"-94_ctrl\"\n",
    "  - \"-93_ctrl\"\n",
    "  - \"-92_ctrl\"\n",
    "  - \"-91_ctrl\"\n",
    "  - \"-90_ctrl\"\n",
    "  - \"-89_ctrl\"\n",
    "  - \"-88_ctrl\"\n",
    "  - \"-87_ctrl\"\n",
    "  - \"-86_ctrl\"\n",
    "  - \"-85_ctrl\"\n",
    "  - \"-84_ctrl\"\n",
    "  - \"-83_ctrl\"\n",
    "  - \"-82_ctrl\"\n",
    "  - \"-81_ctrl\"\n",
    "  - \"-80_ctrl\"\n",
    "  - \"-79_ctrl\"\n",
    "  - \"-78_ctrl\"\n",
    "  - \"-77_ctrl\"\n",
    "  - \"-76_ctrl\"\n",
    "  - \"-75_ctrl\"\n",
    "  - \"-74_ctrl\"\n",
    "  - \"-73_ctrl\"\n",
    "  - \"-72_ctrl\"\n",
    "  - \"-71_ctrl\"\n",
    "  - \"-70_ctrl\"\n",
    "  - \"-69_ctrl\"\n",
    "  - \"-68_ctrl\"\n",
    "  - \"-67_ctrl\"\n",
    "  - \"-66_ctrl\"\n",
    "  - \"-65_ctrl\"\n",
    "  - \"-64_ctrl\"\n",
    "  - \"-63_ctrl\"\n",
    "  - \"-62_ctrl\"\n",
    "  - \"-61_ctrl\"\n",
    "  - \"-60_ctrl\"\n",
    "  - \"-59_ctrl\"\n",
    "  - \"-58_ctrl\"\n",
    "  - \"-57_ctrl\"\n",
    "  - \"-56_ctrl\"\n",
    "  - \"-55_ctrl\"\n",
    "  - \"-54_ctrl\"\n",
    "  - \"-53_ctrl\"\n",
    "  - \"-52_ctrl\"\n",
    "  - \"-51_ctrl\"\n",
    "  - \"-50_ctrl\"\n",
    "  - \"-49_ctrl\"\n",
    "  - \"-48_ctrl\"\n",
    "  - \"-47_ctrl\"\n",
    "  - \"-46_ctrl\"\n",
    "  - \"-45_ctrl\"\n",
    "  - \"-44_ctrl\"\n",
    "  - \"-43_ctrl\"\n",
    "  - \"-42_ctrl\"\n",
    "  - \"-41_ctrl\"\n",
    "  - \"-40_ctrl\"\n",
    "  - \"-39_ctrl\"\n",
    "  - \"-38_ctrl\"\n",
    "  - \"-37_ctrl\"\n",
    "  - \"-36_ctrl\"\n",
    "  - \"-35_ctrl\"\n",
    "  - \"-34_ctrl\"\n",
    "  - \"-33_ctrl\"\n",
    "  - \"-32_ctrl\"\n",
    "  - \"-31_ctrl\"\n",
    "  - \"-30_ctrl\"\n",
    "  - \"-29_ctrl\"\n",
    "  - \"-28_ctrl\"\n",
    "  - \"-27_ctrl\"\n",
    "  - \"-26_ctrl\"\n",
    "  - \"-25_ctrl\"\n",
    "  - \"-24_ctrl\"\n",
    "  - \"-23_ctrl\"\n",
    "  - \"-22_ctrl\"\n",
    "  - \"-21_ctrl\"\n",
    "  - \"-20_ctrl\"\n",
    "  - \"-19_ctrl\"\n",
    "  - \"-18_ctrl\"\n",
    "  - \"-17_ctrl\"\n",
    "  - \"-16_ctrl\"\n",
    "  - \"-15_ctrl\"\n",
    "  - \"-14_ctrl\"\n",
    "  - \"-13_ctrl\"\n",
    "  - \"-12_ctrl\"\n",
    "  - \"-11_ctrl\"\n",
    "  - \"-10_ctrl\"\n",
    "  - \"-9_ctrl\"\n",
    "  - \"-8_ctrl\"\n",
    "  - \"-7_ctrl\"\n",
    "  - \"-6_ctrl\"\n",
    "  - \"-5_ctrl\"\n",
    "  - \"-4_ctrl\"\n",
    "  - \"-3_ctrl\"\n",
    "  - \"-2_ctrl\"\n",
    "  - \"-1_ctrl\"\n",
    "  - \"0_ctrl\"\n",
    "  - \"1_ctrl\"\n",
    "  - \"2_ctrl\"\n",
    "  - \"3_ctrl\"\n",
    "  - \"4_ctrl\"\n",
    "  - \"5_ctrl\"\n",
    "  - \"6_ctrl\"\n",
    "  - \"7_ctrl\"\n",
    "  - \"8_ctrl\"\n",
    "  - \"9_ctrl\"\n",
    "  - \"10_ctrl\"\n",
    "  - \"11_ctrl\"\n",
    "  - \"12_ctrl\"\n",
    "  - \"13_ctrl\"\n",
    "  - \"14_ctrl\"\n",
    "  - \"15_ctrl\"\n",
    "  - \"16_ctrl\"\n",
    "  - \"17_ctrl\"\n",
    "  - \"18_ctrl\"\n",
    "  - \"19_ctrl\"\n",
    "  - \"20_ctrl\"\n",
    "  - \"21_ctrl\"\n",
    "  - \"22_ctrl\"\n",
    "  - \"23_ctrl\"\n",
    "  - \"24_ctrl\"\n",
    "  - \"25_ctrl\"\n",
    "  - \"26_ctrl\"\n",
    "  - \"27_ctrl\"\n",
    "  - \"28_ctrl\"\n",
    "  - \"29_ctrl\"\n",
    "  - \"30_ctrl\"\n",
    "  - \"31_ctrl\"\n",
    "  - \"32_ctrl\"\n",
    "  - \"33_ctrl\"\n",
    "  - \"34_ctrl\"\n",
    "  - \"35_ctrl\"\n",
    "  - \"36_ctrl\"\n",
    "  - \"37_ctrl\"\n",
    "  - \"38_ctrl\"\n",
    "  - \"39_ctrl\"\n",
    "  - \"40_ctrl\"\n",
    "  - \"41_ctrl\"\n",
    "  - \"42_ctrl\"\n",
    "  - \"43_ctrl\"\n",
    "  - \"44_ctrl\"\n",
    "  - \"45_ctrl\"\n",
    "  - \"46_ctrl\"\n",
    "  - \"47_ctrl\"\n",
    "  - \"48_ctrl\"\n",
    "  - \"49_ctrl\"\n",
    "  - \"50_ctrl\"\n",
    "  - \"51_ctrl\"\n",
    "  - \"52_ctrl\"\n",
    "  - \"53_ctrl\"\n",
    "  - \"54_ctrl\"\n",
    "  - \"55_ctrl\"\n",
    "  - \"56_ctrl\"\n",
    "  - \"57_ctrl\"\n",
    "  - \"58_ctrl\"\n",
    "  - \"59_ctrl\"\n",
    "  - \"60_ctrl\"\n",
    "  - \"61_ctrl\"\n",
    "  - \"62_ctrl\"\n",
    "  - \"63_ctrl\"\n",
    "  - \"64_ctrl\"\n",
    "  - \"65_ctrl\"\n",
    "  - \"66_ctrl\"\n",
    "  - \"67_ctrl\"\n",
    "  - \"68_ctrl\"\n",
    "  - \"69_ctrl\"\n",
    "  - \"70_ctrl\"\n",
    "  - \"71_ctrl\"\n",
    "  - \"72_ctrl\"\n",
    "  - \"73_ctrl\"\n",
    "  - \"74_ctrl\"\n",
    "  - \"75_ctrl\"\n",
    "  - \"76_ctrl\"\n",
    "  - \"77_ctrl\"\n",
    "  - \"78_ctrl\"\n",
    "  - \"79_ctrl\"\n",
    "  - \"80_ctrl\"\n",
    "  - \"81_ctrl\"\n",
    "  - \"82_ctrl\"\n",
    "  - \"83_ctrl\"\n",
    "  - \"84_ctrl\"\n",
    "  - \"85_ctrl\"\n",
    "  - \"86_ctrl\"\n",
    "  - \"87_ctrl\"\n",
    "  - \"88_ctrl\"\n",
    "  - \"89_ctrl\"\n",
    "  - \"90_ctrl\"\n",
    "  - \"91_ctrl\"\n",
    "  - \"92_ctrl\"\n",
    "  - \"93_ctrl\"\n",
    "  - \"94_ctrl\"\n",
    "  - \"95_ctrl\"\n",
    "  - \"96_ctrl\"\n",
    "  - \"97_ctrl\"\n",
    "  - \"98_ctrl\"\n",
    "  - \"99_ctrl\"\n",
    "  - \"100_ctrl\"\n",
    "  - \"101_ctrl\"\n",
    "  - \"102_ctrl\"\n",
    "  - \"103_ctrl\"\n",
    "  - \"104_ctrl\"\n",
    "  - \"105_ctrl\"\n",
    "  - \"106_ctrl\"\n",
    "  - \"107_ctrl\"\n",
    "  - \"108_ctrl\"\n",
    "  - \"109_ctrl\"\n",
    "  - \"110_ctrl\"\n",
    "  - \"111_ctrl\"\n",
    "  - \"112_ctrl\"\n",
    "  - \"113_ctrl\"\n",
    "  - \"114_ctrl\"\n",
    "  - \"115_ctrl\"\n",
    "  - \"116_ctrl\"\n",
    "  - \"117_ctrl\"\n",
    "  - \"118_ctrl\"\n",
    "  - \"119_ctrl\"\n",
    "  - \"120_ctrl\"\n",
    "  - \"121_ctrl\"\n",
    "  - \"122_ctrl\"\n",
    "  - \"123_ctrl\"\n",
    "  - \"124_ctrl\"\n",
    "  - \"125_ctrl\"\n",
    "  - \"126_ctrl\"\n",
    "  - \"127_ctrl\"\n",
    "  - \"128_ctrl\"\n",
    "  - \"129_ctrl\"\n",
    "  - \"130_ctrl\"\n",
    "  - \"131_ctrl\"\n",
    "  - \"132_ctrl\"\n",
    "  - \"133_ctrl\"\n",
    "  - \"134_ctrl\"\n",
    "  - \"135_ctrl\"\n",
    "  - \"136_ctrl\"\n",
    "  - \"137_ctrl\"\n",
    "  - \"138_ctrl\"\n",
    "  - \"139_ctrl\"\n",
    "  - \"140_ctrl\"\n",
    "  - \"141_ctrl\"\n",
    "  - \"142_ctrl\"\n",
    "  - \"143_ctrl\"\n",
    "  - \"144_ctrl\"\n",
    "  - \"145_ctrl\"\n",
    "  - \"146_ctrl\"\n",
    "  - \"147_ctrl\"\n",
    "  - \"148_ctrl\"\n",
    "  - \"149_ctrl\"\n",
    "  - \"150_ctrl\"\n",
    "  - \"151_ctrl\"\n",
    "  - \"152_ctrl\"\n",
    "  - \"153_ctrl\"\n",
    "  - \"154_ctrl\"\n",
    "  - \"155_ctrl\"\n",
    "  - \"156_ctrl\"\n",
    "  - \"157_ctrl\"\n",
    "  - \"158_ctrl\"\n",
    "  - \"159_ctrl\"\n",
    "  - \"160_ctrl\"\n",
    "  - \"161_ctrl\"\n",
    "  - \"162_ctrl\"\n",
    "  - \"163_ctrl\"\n",
    "  - \"164_ctrl\"\n",
    "  - \"165_ctrl\"\n",
    "  - \"166_ctrl\"\n",
    "  - \"167_ctrl\"\n",
    "  - \"168_ctrl\"\n",
    "  - \"169_ctrl\"\n",
    "  - \"170_ctrl\"\n",
    "  - \"171_ctrl\"\n",
    "  - \"172_ctrl\"\n",
    "  - \"173_ctrl\"\n",
    "  - \"174_ctrl\"\n",
    "  - \"175_ctrl\"\n",
    "  - \"176_ctrl\"\n",
    "  - \"177_ctrl\"\n",
    "  - \"178_ctrl\"\n",
    "  - \"179_ctrl\"\n",
    "  - \"180_ctrl\"\n",
    "  - \"181_ctrl\"\n",
    "  - \"182_ctrl\"\n",
    "  - \"183_ctrl\"\n",
    "  - \"184_ctrl\"\n",
    "  - \"185_ctrl\"\n",
    "  - \"186_ctrl\"\n",
    "  - \"187_ctrl\"\n",
    "  - \"188_ctrl\"\n",
    "  - \"189_ctrl\"\n",
    "  - \"190_ctrl\"\n",
    "  - \"191_ctrl\"\n",
    "  - \"192_ctrl\"\n",
    "  - \"193_ctrl\"\n",
    "  - \"194_ctrl\"\n",
    "  - \"195_ctrl\"\n",
    "  - \"196_ctrl\"\n",
    "  - \"197_ctrl\"\n",
    "  - \"198_ctrl\"\n",
    "  - \"199_ctrl\"\n",
    "  - \"200_ctrl\"\n",
    "  \"\"\"\n",
    "}\n",
    "\n",
    "training_prom_CHiC_yaml_contents = {\"globals.yaml\":\"\"\"\n",
    "output_folder: ./runs/gene_expression_prom_CHiC_pure_conv/  #gene_expression_prediction_no_H3K27ac_uncoupled/     #gene_expression_cnn_1kbp_401_bins_2_cond_reloaded/ \n",
    "manual_valid_ids_file: ./annotations/manual_validation_ids_chr17.txt  #manual_validation_ids_chr17_uncoupled.txt\n",
    "checkpoint_interval: 30300 \n",
    "sample_interval: 30300 \n",
    "n_epochs: 120\n",
    "batch_size: 64\n",
    "optimizer: \"adamw\"\n",
    "lr: 0.0001 \n",
    "device: \"cuda\"\n",
    "compute_attributions: false\n",
    "\"\"\",\n",
    "\"input_cnn.yaml\": \"\"\" \n",
    "input_info:\n",
    "  input_source: ./inputs/landscape_arrays/training/ \n",
    "  input_name: gene_expression\n",
    "  input_type: array\n",
    "\n",
    "model_config:\n",
    "  model_type: cnn\n",
    "  #pre_normalization: \"instancenorm\"\n",
    "  model_init_config:\n",
    "    num_output_features: 512 # before fc_repr_dim\n",
    "    layers: [4,4]\n",
    "    kernel_height: 1\n",
    "    down_stride_width: 2\n",
    "    first_stride_expansion_width: 1\n",
    "    first_kernel_expansion_height: 4 #5\n",
    "    kernel_width: 10  #10\n",
    "    dilation_factor_width: 2\n",
    "    dilation_factor_height: 1\n",
    "    channel_exp_base: 5  #3 #-1\n",
    "    first_channel_expansion: 2\n",
    "    rb_do: .3\n",
    "    stochastic_depth_p: .1\n",
    "    attention_inclusion_cutoff: 1 #50 #256\n",
    "\n",
    "\"\"\",\n",
    "\"\"\"input_cnn_microc.yaml\"\"\": \"\"\"\n",
    "input_info:\n",
    "  input_source: ./inputs/Promoter-CHiC/training/ \n",
    "  input_name: contact_maps\n",
    "  input_type: array\n",
    "\n",
    "model_config:\n",
    "  model_type: cnn\n",
    "  #pre_normalization: \"instancenorm\"\n",
    "  model_init_config:\n",
    "    num_output_features: 512 #before fc_repr_dim\n",
    "    layers: [1,2]\n",
    "    kernel_height: 5\n",
    "    down_stride_width: 2\n",
    "    down_stride_height: 2 #5\n",
    "    kernel_width: 5  #10\n",
    "    dilation_factor_width: 2\n",
    "    dilation_factor_height: 2\n",
    "    channel_exp_base: 2  #3 #-1\n",
    "    first_channel_expansion: 2\n",
    "    rb_do: .3\n",
    "    stochastic_depth_p: .1\n",
    "    attention_inclusion_cutoff: 1 #128 #256\n",
    "\"\"\",\n",
    "\"fusion.yaml\": \"\"\"\n",
    "model_config:\n",
    "  fc_do: 0.4\n",
    "  fc_task_dim: 256\n",
    "  layers:\n",
    "  - 2\n",
    "  rb_do: 0.4\n",
    "  stochastic_depth_p: 0.5\n",
    "model_type: \"mlp-residual\"\n",
    "\"\"\",\n",
    "\"outputs_2_cond.yaml\":\"\"\"\n",
    "output_info:\n",
    "  output_name: expression_output\n",
    "  output_source: ./targets/training_targets.csv \n",
    "  output_type: tabular\n",
    "\n",
    "model_config: # <- new\n",
    "  model_type: linear # <- new\n",
    "\n",
    "output_type_info:\n",
    "  con_loss_name: \"SmoothL1Loss\"\n",
    "  target_con_columns:\n",
    "  - \"-200_ctrl\"\n",
    "  - \"-199_ctrl\"\n",
    "  - \"-198_ctrl\"\n",
    "  - \"-197_ctrl\"\n",
    "  - \"-196_ctrl\"\n",
    "  - \"-195_ctrl\"\n",
    "  - \"-194_ctrl\"\n",
    "  - \"-193_ctrl\"\n",
    "  - \"-192_ctrl\"\n",
    "  - \"-191_ctrl\"\n",
    "  - \"-190_ctrl\"\n",
    "  - \"-189_ctrl\"\n",
    "  - \"-188_ctrl\"\n",
    "  - \"-187_ctrl\"\n",
    "  - \"-186_ctrl\"\n",
    "  - \"-185_ctrl\"\n",
    "  - \"-184_ctrl\"\n",
    "  - \"-183_ctrl\"\n",
    "  - \"-182_ctrl\"\n",
    "  - \"-181_ctrl\"\n",
    "  - \"-180_ctrl\"\n",
    "  - \"-179_ctrl\"\n",
    "  - \"-178_ctrl\"\n",
    "  - \"-177_ctrl\"\n",
    "  - \"-176_ctrl\"\n",
    "  - \"-175_ctrl\"\n",
    "  - \"-174_ctrl\"\n",
    "  - \"-173_ctrl\"\n",
    "  - \"-172_ctrl\"\n",
    "  - \"-171_ctrl\"\n",
    "  - \"-170_ctrl\"\n",
    "  - \"-169_ctrl\"\n",
    "  - \"-168_ctrl\"\n",
    "  - \"-167_ctrl\"\n",
    "  - \"-166_ctrl\"\n",
    "  - \"-165_ctrl\"\n",
    "  - \"-164_ctrl\"\n",
    "  - \"-163_ctrl\"\n",
    "  - \"-162_ctrl\"\n",
    "  - \"-161_ctrl\"\n",
    "  - \"-160_ctrl\"\n",
    "  - \"-159_ctrl\"\n",
    "  - \"-158_ctrl\"\n",
    "  - \"-157_ctrl\"\n",
    "  - \"-156_ctrl\"\n",
    "  - \"-155_ctrl\"\n",
    "  - \"-154_ctrl\"\n",
    "  - \"-153_ctrl\"\n",
    "  - \"-152_ctrl\"\n",
    "  - \"-151_ctrl\"\n",
    "  - \"-150_ctrl\"\n",
    "  - \"-149_ctrl\"\n",
    "  - \"-148_ctrl\"\n",
    "  - \"-147_ctrl\"\n",
    "  - \"-146_ctrl\"\n",
    "  - \"-145_ctrl\"\n",
    "  - \"-144_ctrl\"\n",
    "  - \"-143_ctrl\"\n",
    "  - \"-142_ctrl\"\n",
    "  - \"-141_ctrl\"\n",
    "  - \"-140_ctrl\"\n",
    "  - \"-139_ctrl\"\n",
    "  - \"-138_ctrl\"\n",
    "  - \"-137_ctrl\"\n",
    "  - \"-136_ctrl\"\n",
    "  - \"-135_ctrl\"\n",
    "  - \"-134_ctrl\"\n",
    "  - \"-133_ctrl\"\n",
    "  - \"-132_ctrl\"\n",
    "  - \"-131_ctrl\"\n",
    "  - \"-130_ctrl\"\n",
    "  - \"-129_ctrl\"\n",
    "  - \"-128_ctrl\"\n",
    "  - \"-127_ctrl\"\n",
    "  - \"-126_ctrl\"\n",
    "  - \"-125_ctrl\"\n",
    "  - \"-124_ctrl\"\n",
    "  - \"-123_ctrl\"\n",
    "  - \"-122_ctrl\"\n",
    "  - \"-121_ctrl\"\n",
    "  - \"-120_ctrl\"\n",
    "  - \"-119_ctrl\"\n",
    "  - \"-118_ctrl\"\n",
    "  - \"-117_ctrl\"\n",
    "  - \"-116_ctrl\"\n",
    "  - \"-115_ctrl\"\n",
    "  - \"-114_ctrl\"\n",
    "  - \"-113_ctrl\"\n",
    "  - \"-112_ctrl\"\n",
    "  - \"-111_ctrl\"\n",
    "  - \"-110_ctrl\"\n",
    "  - \"-109_ctrl\"\n",
    "  - \"-108_ctrl\"\n",
    "  - \"-107_ctrl\"\n",
    "  - \"-106_ctrl\"\n",
    "  - \"-105_ctrl\"\n",
    "  - \"-104_ctrl\"\n",
    "  - \"-103_ctrl\"\n",
    "  - \"-102_ctrl\"\n",
    "  - \"-101_ctrl\"\n",
    "  - \"-100_ctrl\"\n",
    "  - \"-99_ctrl\"\n",
    "  - \"-98_ctrl\"\n",
    "  - \"-97_ctrl\"\n",
    "  - \"-96_ctrl\"\n",
    "  - \"-95_ctrl\"\n",
    "  - \"-94_ctrl\"\n",
    "  - \"-93_ctrl\"\n",
    "  - \"-92_ctrl\"\n",
    "  - \"-91_ctrl\"\n",
    "  - \"-90_ctrl\"\n",
    "  - \"-89_ctrl\"\n",
    "  - \"-88_ctrl\"\n",
    "  - \"-87_ctrl\"\n",
    "  - \"-86_ctrl\"\n",
    "  - \"-85_ctrl\"\n",
    "  - \"-84_ctrl\"\n",
    "  - \"-83_ctrl\"\n",
    "  - \"-82_ctrl\"\n",
    "  - \"-81_ctrl\"\n",
    "  - \"-80_ctrl\"\n",
    "  - \"-79_ctrl\"\n",
    "  - \"-78_ctrl\"\n",
    "  - \"-77_ctrl\"\n",
    "  - \"-76_ctrl\"\n",
    "  - \"-75_ctrl\"\n",
    "  - \"-74_ctrl\"\n",
    "  - \"-73_ctrl\"\n",
    "  - \"-72_ctrl\"\n",
    "  - \"-71_ctrl\"\n",
    "  - \"-70_ctrl\"\n",
    "  - \"-69_ctrl\"\n",
    "  - \"-68_ctrl\"\n",
    "  - \"-67_ctrl\"\n",
    "  - \"-66_ctrl\"\n",
    "  - \"-65_ctrl\"\n",
    "  - \"-64_ctrl\"\n",
    "  - \"-63_ctrl\"\n",
    "  - \"-62_ctrl\"\n",
    "  - \"-61_ctrl\"\n",
    "  - \"-60_ctrl\"\n",
    "  - \"-59_ctrl\"\n",
    "  - \"-58_ctrl\"\n",
    "  - \"-57_ctrl\"\n",
    "  - \"-56_ctrl\"\n",
    "  - \"-55_ctrl\"\n",
    "  - \"-54_ctrl\"\n",
    "  - \"-53_ctrl\"\n",
    "  - \"-52_ctrl\"\n",
    "  - \"-51_ctrl\"\n",
    "  - \"-50_ctrl\"\n",
    "  - \"-49_ctrl\"\n",
    "  - \"-48_ctrl\"\n",
    "  - \"-47_ctrl\"\n",
    "  - \"-46_ctrl\"\n",
    "  - \"-45_ctrl\"\n",
    "  - \"-44_ctrl\"\n",
    "  - \"-43_ctrl\"\n",
    "  - \"-42_ctrl\"\n",
    "  - \"-41_ctrl\"\n",
    "  - \"-40_ctrl\"\n",
    "  - \"-39_ctrl\"\n",
    "  - \"-38_ctrl\"\n",
    "  - \"-37_ctrl\"\n",
    "  - \"-36_ctrl\"\n",
    "  - \"-35_ctrl\"\n",
    "  - \"-34_ctrl\"\n",
    "  - \"-33_ctrl\"\n",
    "  - \"-32_ctrl\"\n",
    "  - \"-31_ctrl\"\n",
    "  - \"-30_ctrl\"\n",
    "  - \"-29_ctrl\"\n",
    "  - \"-28_ctrl\"\n",
    "  - \"-27_ctrl\"\n",
    "  - \"-26_ctrl\"\n",
    "  - \"-25_ctrl\"\n",
    "  - \"-24_ctrl\"\n",
    "  - \"-23_ctrl\"\n",
    "  - \"-22_ctrl\"\n",
    "  - \"-21_ctrl\"\n",
    "  - \"-20_ctrl\"\n",
    "  - \"-19_ctrl\"\n",
    "  - \"-18_ctrl\"\n",
    "  - \"-17_ctrl\"\n",
    "  - \"-16_ctrl\"\n",
    "  - \"-15_ctrl\"\n",
    "  - \"-14_ctrl\"\n",
    "  - \"-13_ctrl\"\n",
    "  - \"-12_ctrl\"\n",
    "  - \"-11_ctrl\"\n",
    "  - \"-10_ctrl\"\n",
    "  - \"-9_ctrl\"\n",
    "  - \"-8_ctrl\"\n",
    "  - \"-7_ctrl\"\n",
    "  - \"-6_ctrl\"\n",
    "  - \"-5_ctrl\"\n",
    "  - \"-4_ctrl\"\n",
    "  - \"-3_ctrl\"\n",
    "  - \"-2_ctrl\"\n",
    "  - \"-1_ctrl\"\n",
    "  - \"0_ctrl\"\n",
    "  - \"1_ctrl\"\n",
    "  - \"2_ctrl\"\n",
    "  - \"3_ctrl\"\n",
    "  - \"4_ctrl\"\n",
    "  - \"5_ctrl\"\n",
    "  - \"6_ctrl\"\n",
    "  - \"7_ctrl\"\n",
    "  - \"8_ctrl\"\n",
    "  - \"9_ctrl\"\n",
    "  - \"10_ctrl\"\n",
    "  - \"11_ctrl\"\n",
    "  - \"12_ctrl\"\n",
    "  - \"13_ctrl\"\n",
    "  - \"14_ctrl\"\n",
    "  - \"15_ctrl\"\n",
    "  - \"16_ctrl\"\n",
    "  - \"17_ctrl\"\n",
    "  - \"18_ctrl\"\n",
    "  - \"19_ctrl\"\n",
    "  - \"20_ctrl\"\n",
    "  - \"21_ctrl\"\n",
    "  - \"22_ctrl\"\n",
    "  - \"23_ctrl\"\n",
    "  - \"24_ctrl\"\n",
    "  - \"25_ctrl\"\n",
    "  - \"26_ctrl\"\n",
    "  - \"27_ctrl\"\n",
    "  - \"28_ctrl\"\n",
    "  - \"29_ctrl\"\n",
    "  - \"30_ctrl\"\n",
    "  - \"31_ctrl\"\n",
    "  - \"32_ctrl\"\n",
    "  - \"33_ctrl\"\n",
    "  - \"34_ctrl\"\n",
    "  - \"35_ctrl\"\n",
    "  - \"36_ctrl\"\n",
    "  - \"37_ctrl\"\n",
    "  - \"38_ctrl\"\n",
    "  - \"39_ctrl\"\n",
    "  - \"40_ctrl\"\n",
    "  - \"41_ctrl\"\n",
    "  - \"42_ctrl\"\n",
    "  - \"43_ctrl\"\n",
    "  - \"44_ctrl\"\n",
    "  - \"45_ctrl\"\n",
    "  - \"46_ctrl\"\n",
    "  - \"47_ctrl\"\n",
    "  - \"48_ctrl\"\n",
    "  - \"49_ctrl\"\n",
    "  - \"50_ctrl\"\n",
    "  - \"51_ctrl\"\n",
    "  - \"52_ctrl\"\n",
    "  - \"53_ctrl\"\n",
    "  - \"54_ctrl\"\n",
    "  - \"55_ctrl\"\n",
    "  - \"56_ctrl\"\n",
    "  - \"57_ctrl\"\n",
    "  - \"58_ctrl\"\n",
    "  - \"59_ctrl\"\n",
    "  - \"60_ctrl\"\n",
    "  - \"61_ctrl\"\n",
    "  - \"62_ctrl\"\n",
    "  - \"63_ctrl\"\n",
    "  - \"64_ctrl\"\n",
    "  - \"65_ctrl\"\n",
    "  - \"66_ctrl\"\n",
    "  - \"67_ctrl\"\n",
    "  - \"68_ctrl\"\n",
    "  - \"69_ctrl\"\n",
    "  - \"70_ctrl\"\n",
    "  - \"71_ctrl\"\n",
    "  - \"72_ctrl\"\n",
    "  - \"73_ctrl\"\n",
    "  - \"74_ctrl\"\n",
    "  - \"75_ctrl\"\n",
    "  - \"76_ctrl\"\n",
    "  - \"77_ctrl\"\n",
    "  - \"78_ctrl\"\n",
    "  - \"79_ctrl\"\n",
    "  - \"80_ctrl\"\n",
    "  - \"81_ctrl\"\n",
    "  - \"82_ctrl\"\n",
    "  - \"83_ctrl\"\n",
    "  - \"84_ctrl\"\n",
    "  - \"85_ctrl\"\n",
    "  - \"86_ctrl\"\n",
    "  - \"87_ctrl\"\n",
    "  - \"88_ctrl\"\n",
    "  - \"89_ctrl\"\n",
    "  - \"90_ctrl\"\n",
    "  - \"91_ctrl\"\n",
    "  - \"92_ctrl\"\n",
    "  - \"93_ctrl\"\n",
    "  - \"94_ctrl\"\n",
    "  - \"95_ctrl\"\n",
    "  - \"96_ctrl\"\n",
    "  - \"97_ctrl\"\n",
    "  - \"98_ctrl\"\n",
    "  - \"99_ctrl\"\n",
    "  - \"100_ctrl\"\n",
    "  - \"101_ctrl\"\n",
    "  - \"102_ctrl\"\n",
    "  - \"103_ctrl\"\n",
    "  - \"104_ctrl\"\n",
    "  - \"105_ctrl\"\n",
    "  - \"106_ctrl\"\n",
    "  - \"107_ctrl\"\n",
    "  - \"108_ctrl\"\n",
    "  - \"109_ctrl\"\n",
    "  - \"110_ctrl\"\n",
    "  - \"111_ctrl\"\n",
    "  - \"112_ctrl\"\n",
    "  - \"113_ctrl\"\n",
    "  - \"114_ctrl\"\n",
    "  - \"115_ctrl\"\n",
    "  - \"116_ctrl\"\n",
    "  - \"117_ctrl\"\n",
    "  - \"118_ctrl\"\n",
    "  - \"119_ctrl\"\n",
    "  - \"120_ctrl\"\n",
    "  - \"121_ctrl\"\n",
    "  - \"122_ctrl\"\n",
    "  - \"123_ctrl\"\n",
    "  - \"124_ctrl\"\n",
    "  - \"125_ctrl\"\n",
    "  - \"126_ctrl\"\n",
    "  - \"127_ctrl\"\n",
    "  - \"128_ctrl\"\n",
    "  - \"129_ctrl\"\n",
    "  - \"130_ctrl\"\n",
    "  - \"131_ctrl\"\n",
    "  - \"132_ctrl\"\n",
    "  - \"133_ctrl\"\n",
    "  - \"134_ctrl\"\n",
    "  - \"135_ctrl\"\n",
    "  - \"136_ctrl\"\n",
    "  - \"137_ctrl\"\n",
    "  - \"138_ctrl\"\n",
    "  - \"139_ctrl\"\n",
    "  - \"140_ctrl\"\n",
    "  - \"141_ctrl\"\n",
    "  - \"142_ctrl\"\n",
    "  - \"143_ctrl\"\n",
    "  - \"144_ctrl\"\n",
    "  - \"145_ctrl\"\n",
    "  - \"146_ctrl\"\n",
    "  - \"147_ctrl\"\n",
    "  - \"148_ctrl\"\n",
    "  - \"149_ctrl\"\n",
    "  - \"150_ctrl\"\n",
    "  - \"151_ctrl\"\n",
    "  - \"152_ctrl\"\n",
    "  - \"153_ctrl\"\n",
    "  - \"154_ctrl\"\n",
    "  - \"155_ctrl\"\n",
    "  - \"156_ctrl\"\n",
    "  - \"157_ctrl\"\n",
    "  - \"158_ctrl\"\n",
    "  - \"159_ctrl\"\n",
    "  - \"160_ctrl\"\n",
    "  - \"161_ctrl\"\n",
    "  - \"162_ctrl\"\n",
    "  - \"163_ctrl\"\n",
    "  - \"164_ctrl\"\n",
    "  - \"165_ctrl\"\n",
    "  - \"166_ctrl\"\n",
    "  - \"167_ctrl\"\n",
    "  - \"168_ctrl\"\n",
    "  - \"169_ctrl\"\n",
    "  - \"170_ctrl\"\n",
    "  - \"171_ctrl\"\n",
    "  - \"172_ctrl\"\n",
    "  - \"173_ctrl\"\n",
    "  - \"174_ctrl\"\n",
    "  - \"175_ctrl\"\n",
    "  - \"176_ctrl\"\n",
    "  - \"177_ctrl\"\n",
    "  - \"178_ctrl\"\n",
    "  - \"179_ctrl\"\n",
    "  - \"180_ctrl\"\n",
    "  - \"181_ctrl\"\n",
    "  - \"182_ctrl\"\n",
    "  - \"183_ctrl\"\n",
    "  - \"184_ctrl\"\n",
    "  - \"185_ctrl\"\n",
    "  - \"186_ctrl\"\n",
    "  - \"187_ctrl\"\n",
    "  - \"188_ctrl\"\n",
    "  - \"189_ctrl\"\n",
    "  - \"190_ctrl\"\n",
    "  - \"191_ctrl\"\n",
    "  - \"192_ctrl\"\n",
    "  - \"193_ctrl\"\n",
    "  - \"194_ctrl\"\n",
    "  - \"195_ctrl\"\n",
    "  - \"196_ctrl\"\n",
    "  - \"197_ctrl\"\n",
    "  - \"198_ctrl\"\n",
    "  - \"199_ctrl\"\n",
    "  - \"200_ctrl\"\n",
    "  \"\"\"\n",
    "}\n",
    "\n",
    "test_prom_CHiC_yaml_contents = {\"globals.yaml\":\"\"\"\n",
    "checkpoint_interval: 30300 \n",
    "sample_interval: 30300 \n",
    "n_epochs: 120\n",
    "batch_size: 64\n",
    "optimizer: \"adamw\"\n",
    "lr: 0.0001 \n",
    "device: \"cuda\"\n",
    "compute_attributions: true\n",
    "\"\"\",\n",
    "\"input_cnn.yaml\": \"\"\" \n",
    "input_info:\n",
    "  input_source: ./inputs/landscape_arrays/test/ \n",
    "  input_name: gene_expression\n",
    "  input_type: array\n",
    "\n",
    "model_config:\n",
    "  model_type: cnn\n",
    "  #pre_normalization: \"instancenorm\"\n",
    "  model_init_config:\n",
    "    num_output_features: 512 # before fc_repr_dim\n",
    "    layers: [4,4]\n",
    "    kernel_height: 1\n",
    "    down_stride_width: 2\n",
    "    first_stride_expansion_width: 1\n",
    "    first_kernel_expansion_height: 4 #5\n",
    "    kernel_width: 10  #10\n",
    "    dilation_factor_width: 2\n",
    "    dilation_factor_height: 1\n",
    "    channel_exp_base: 5  #3 #-1\n",
    "    first_channel_expansion: 2\n",
    "    rb_do: .3\n",
    "    stochastic_depth_p: .1\n",
    "    attention_inclusion_cutoff: 1 #50 #256\n",
    "\n",
    "\"\"\",\n",
    "\"\"\"input_cnn_microc.yaml\"\"\": \"\"\"\n",
    "input_info:\n",
    "  input_source: ./inputs/Promoter-CHiC/test/ \n",
    "  input_name: contact_maps\n",
    "  input_type: array\n",
    "\n",
    "model_config:\n",
    "  model_type: cnn\n",
    "  #pre_normalization: \"instancenorm\"\n",
    "  model_init_config:\n",
    "    num_output_features: 512 #before fc_repr_dim\n",
    "    layers: [1,2]\n",
    "    kernel_height: 5\n",
    "    down_stride_width: 2\n",
    "    down_stride_height: 2 #5\n",
    "    kernel_width: 5  #10\n",
    "    dilation_factor_width: 2\n",
    "    dilation_factor_height: 2\n",
    "    channel_exp_base: 2  #3 #-1\n",
    "    first_channel_expansion: 2\n",
    "    rb_do: .3\n",
    "    stochastic_depth_p: .1\n",
    "    attention_inclusion_cutoff: 1 #128 #256\n",
    "\"\"\",\n",
    "\"fusion.yaml\": \"\"\"\n",
    "model_config:\n",
    "  fc_do: 0.4\n",
    "  fc_task_dim: 256\n",
    "  layers:\n",
    "  - 2\n",
    "  rb_do: 0.4\n",
    "  stochastic_depth_p: 0.5\n",
    "model_type: \"mlp-residual\"\n",
    "\"\"\",\n",
    "\"outputs_2_cond.yaml\":\"\"\"\n",
    "output_info:\n",
    "  output_name: expression_output\n",
    "  output_source: ./targets/test_targets.csv \n",
    "  output_type: tabular\n",
    "\n",
    "model_config: # <- new\n",
    "  model_type: linear # <- new\n",
    "\n",
    "output_type_info:\n",
    "  con_loss_name: \"SmoothL1Loss\"\n",
    "  target_con_columns:\n",
    "  - \"-200_ctrl\"\n",
    "  - \"-199_ctrl\"\n",
    "  - \"-198_ctrl\"\n",
    "  - \"-197_ctrl\"\n",
    "  - \"-196_ctrl\"\n",
    "  - \"-195_ctrl\"\n",
    "  - \"-194_ctrl\"\n",
    "  - \"-193_ctrl\"\n",
    "  - \"-192_ctrl\"\n",
    "  - \"-191_ctrl\"\n",
    "  - \"-190_ctrl\"\n",
    "  - \"-189_ctrl\"\n",
    "  - \"-188_ctrl\"\n",
    "  - \"-187_ctrl\"\n",
    "  - \"-186_ctrl\"\n",
    "  - \"-185_ctrl\"\n",
    "  - \"-184_ctrl\"\n",
    "  - \"-183_ctrl\"\n",
    "  - \"-182_ctrl\"\n",
    "  - \"-181_ctrl\"\n",
    "  - \"-180_ctrl\"\n",
    "  - \"-179_ctrl\"\n",
    "  - \"-178_ctrl\"\n",
    "  - \"-177_ctrl\"\n",
    "  - \"-176_ctrl\"\n",
    "  - \"-175_ctrl\"\n",
    "  - \"-174_ctrl\"\n",
    "  - \"-173_ctrl\"\n",
    "  - \"-172_ctrl\"\n",
    "  - \"-171_ctrl\"\n",
    "  - \"-170_ctrl\"\n",
    "  - \"-169_ctrl\"\n",
    "  - \"-168_ctrl\"\n",
    "  - \"-167_ctrl\"\n",
    "  - \"-166_ctrl\"\n",
    "  - \"-165_ctrl\"\n",
    "  - \"-164_ctrl\"\n",
    "  - \"-163_ctrl\"\n",
    "  - \"-162_ctrl\"\n",
    "  - \"-161_ctrl\"\n",
    "  - \"-160_ctrl\"\n",
    "  - \"-159_ctrl\"\n",
    "  - \"-158_ctrl\"\n",
    "  - \"-157_ctrl\"\n",
    "  - \"-156_ctrl\"\n",
    "  - \"-155_ctrl\"\n",
    "  - \"-154_ctrl\"\n",
    "  - \"-153_ctrl\"\n",
    "  - \"-152_ctrl\"\n",
    "  - \"-151_ctrl\"\n",
    "  - \"-150_ctrl\"\n",
    "  - \"-149_ctrl\"\n",
    "  - \"-148_ctrl\"\n",
    "  - \"-147_ctrl\"\n",
    "  - \"-146_ctrl\"\n",
    "  - \"-145_ctrl\"\n",
    "  - \"-144_ctrl\"\n",
    "  - \"-143_ctrl\"\n",
    "  - \"-142_ctrl\"\n",
    "  - \"-141_ctrl\"\n",
    "  - \"-140_ctrl\"\n",
    "  - \"-139_ctrl\"\n",
    "  - \"-138_ctrl\"\n",
    "  - \"-137_ctrl\"\n",
    "  - \"-136_ctrl\"\n",
    "  - \"-135_ctrl\"\n",
    "  - \"-134_ctrl\"\n",
    "  - \"-133_ctrl\"\n",
    "  - \"-132_ctrl\"\n",
    "  - \"-131_ctrl\"\n",
    "  - \"-130_ctrl\"\n",
    "  - \"-129_ctrl\"\n",
    "  - \"-128_ctrl\"\n",
    "  - \"-127_ctrl\"\n",
    "  - \"-126_ctrl\"\n",
    "  - \"-125_ctrl\"\n",
    "  - \"-124_ctrl\"\n",
    "  - \"-123_ctrl\"\n",
    "  - \"-122_ctrl\"\n",
    "  - \"-121_ctrl\"\n",
    "  - \"-120_ctrl\"\n",
    "  - \"-119_ctrl\"\n",
    "  - \"-118_ctrl\"\n",
    "  - \"-117_ctrl\"\n",
    "  - \"-116_ctrl\"\n",
    "  - \"-115_ctrl\"\n",
    "  - \"-114_ctrl\"\n",
    "  - \"-113_ctrl\"\n",
    "  - \"-112_ctrl\"\n",
    "  - \"-111_ctrl\"\n",
    "  - \"-110_ctrl\"\n",
    "  - \"-109_ctrl\"\n",
    "  - \"-108_ctrl\"\n",
    "  - \"-107_ctrl\"\n",
    "  - \"-106_ctrl\"\n",
    "  - \"-105_ctrl\"\n",
    "  - \"-104_ctrl\"\n",
    "  - \"-103_ctrl\"\n",
    "  - \"-102_ctrl\"\n",
    "  - \"-101_ctrl\"\n",
    "  - \"-100_ctrl\"\n",
    "  - \"-99_ctrl\"\n",
    "  - \"-98_ctrl\"\n",
    "  - \"-97_ctrl\"\n",
    "  - \"-96_ctrl\"\n",
    "  - \"-95_ctrl\"\n",
    "  - \"-94_ctrl\"\n",
    "  - \"-93_ctrl\"\n",
    "  - \"-92_ctrl\"\n",
    "  - \"-91_ctrl\"\n",
    "  - \"-90_ctrl\"\n",
    "  - \"-89_ctrl\"\n",
    "  - \"-88_ctrl\"\n",
    "  - \"-87_ctrl\"\n",
    "  - \"-86_ctrl\"\n",
    "  - \"-85_ctrl\"\n",
    "  - \"-84_ctrl\"\n",
    "  - \"-83_ctrl\"\n",
    "  - \"-82_ctrl\"\n",
    "  - \"-81_ctrl\"\n",
    "  - \"-80_ctrl\"\n",
    "  - \"-79_ctrl\"\n",
    "  - \"-78_ctrl\"\n",
    "  - \"-77_ctrl\"\n",
    "  - \"-76_ctrl\"\n",
    "  - \"-75_ctrl\"\n",
    "  - \"-74_ctrl\"\n",
    "  - \"-73_ctrl\"\n",
    "  - \"-72_ctrl\"\n",
    "  - \"-71_ctrl\"\n",
    "  - \"-70_ctrl\"\n",
    "  - \"-69_ctrl\"\n",
    "  - \"-68_ctrl\"\n",
    "  - \"-67_ctrl\"\n",
    "  - \"-66_ctrl\"\n",
    "  - \"-65_ctrl\"\n",
    "  - \"-64_ctrl\"\n",
    "  - \"-63_ctrl\"\n",
    "  - \"-62_ctrl\"\n",
    "  - \"-61_ctrl\"\n",
    "  - \"-60_ctrl\"\n",
    "  - \"-59_ctrl\"\n",
    "  - \"-58_ctrl\"\n",
    "  - \"-57_ctrl\"\n",
    "  - \"-56_ctrl\"\n",
    "  - \"-55_ctrl\"\n",
    "  - \"-54_ctrl\"\n",
    "  - \"-53_ctrl\"\n",
    "  - \"-52_ctrl\"\n",
    "  - \"-51_ctrl\"\n",
    "  - \"-50_ctrl\"\n",
    "  - \"-49_ctrl\"\n",
    "  - \"-48_ctrl\"\n",
    "  - \"-47_ctrl\"\n",
    "  - \"-46_ctrl\"\n",
    "  - \"-45_ctrl\"\n",
    "  - \"-44_ctrl\"\n",
    "  - \"-43_ctrl\"\n",
    "  - \"-42_ctrl\"\n",
    "  - \"-41_ctrl\"\n",
    "  - \"-40_ctrl\"\n",
    "  - \"-39_ctrl\"\n",
    "  - \"-38_ctrl\"\n",
    "  - \"-37_ctrl\"\n",
    "  - \"-36_ctrl\"\n",
    "  - \"-35_ctrl\"\n",
    "  - \"-34_ctrl\"\n",
    "  - \"-33_ctrl\"\n",
    "  - \"-32_ctrl\"\n",
    "  - \"-31_ctrl\"\n",
    "  - \"-30_ctrl\"\n",
    "  - \"-29_ctrl\"\n",
    "  - \"-28_ctrl\"\n",
    "  - \"-27_ctrl\"\n",
    "  - \"-26_ctrl\"\n",
    "  - \"-25_ctrl\"\n",
    "  - \"-24_ctrl\"\n",
    "  - \"-23_ctrl\"\n",
    "  - \"-22_ctrl\"\n",
    "  - \"-21_ctrl\"\n",
    "  - \"-20_ctrl\"\n",
    "  - \"-19_ctrl\"\n",
    "  - \"-18_ctrl\"\n",
    "  - \"-17_ctrl\"\n",
    "  - \"-16_ctrl\"\n",
    "  - \"-15_ctrl\"\n",
    "  - \"-14_ctrl\"\n",
    "  - \"-13_ctrl\"\n",
    "  - \"-12_ctrl\"\n",
    "  - \"-11_ctrl\"\n",
    "  - \"-10_ctrl\"\n",
    "  - \"-9_ctrl\"\n",
    "  - \"-8_ctrl\"\n",
    "  - \"-7_ctrl\"\n",
    "  - \"-6_ctrl\"\n",
    "  - \"-5_ctrl\"\n",
    "  - \"-4_ctrl\"\n",
    "  - \"-3_ctrl\"\n",
    "  - \"-2_ctrl\"\n",
    "  - \"-1_ctrl\"\n",
    "  - \"0_ctrl\"\n",
    "  - \"1_ctrl\"\n",
    "  - \"2_ctrl\"\n",
    "  - \"3_ctrl\"\n",
    "  - \"4_ctrl\"\n",
    "  - \"5_ctrl\"\n",
    "  - \"6_ctrl\"\n",
    "  - \"7_ctrl\"\n",
    "  - \"8_ctrl\"\n",
    "  - \"9_ctrl\"\n",
    "  - \"10_ctrl\"\n",
    "  - \"11_ctrl\"\n",
    "  - \"12_ctrl\"\n",
    "  - \"13_ctrl\"\n",
    "  - \"14_ctrl\"\n",
    "  - \"15_ctrl\"\n",
    "  - \"16_ctrl\"\n",
    "  - \"17_ctrl\"\n",
    "  - \"18_ctrl\"\n",
    "  - \"19_ctrl\"\n",
    "  - \"20_ctrl\"\n",
    "  - \"21_ctrl\"\n",
    "  - \"22_ctrl\"\n",
    "  - \"23_ctrl\"\n",
    "  - \"24_ctrl\"\n",
    "  - \"25_ctrl\"\n",
    "  - \"26_ctrl\"\n",
    "  - \"27_ctrl\"\n",
    "  - \"28_ctrl\"\n",
    "  - \"29_ctrl\"\n",
    "  - \"30_ctrl\"\n",
    "  - \"31_ctrl\"\n",
    "  - \"32_ctrl\"\n",
    "  - \"33_ctrl\"\n",
    "  - \"34_ctrl\"\n",
    "  - \"35_ctrl\"\n",
    "  - \"36_ctrl\"\n",
    "  - \"37_ctrl\"\n",
    "  - \"38_ctrl\"\n",
    "  - \"39_ctrl\"\n",
    "  - \"40_ctrl\"\n",
    "  - \"41_ctrl\"\n",
    "  - \"42_ctrl\"\n",
    "  - \"43_ctrl\"\n",
    "  - \"44_ctrl\"\n",
    "  - \"45_ctrl\"\n",
    "  - \"46_ctrl\"\n",
    "  - \"47_ctrl\"\n",
    "  - \"48_ctrl\"\n",
    "  - \"49_ctrl\"\n",
    "  - \"50_ctrl\"\n",
    "  - \"51_ctrl\"\n",
    "  - \"52_ctrl\"\n",
    "  - \"53_ctrl\"\n",
    "  - \"54_ctrl\"\n",
    "  - \"55_ctrl\"\n",
    "  - \"56_ctrl\"\n",
    "  - \"57_ctrl\"\n",
    "  - \"58_ctrl\"\n",
    "  - \"59_ctrl\"\n",
    "  - \"60_ctrl\"\n",
    "  - \"61_ctrl\"\n",
    "  - \"62_ctrl\"\n",
    "  - \"63_ctrl\"\n",
    "  - \"64_ctrl\"\n",
    "  - \"65_ctrl\"\n",
    "  - \"66_ctrl\"\n",
    "  - \"67_ctrl\"\n",
    "  - \"68_ctrl\"\n",
    "  - \"69_ctrl\"\n",
    "  - \"70_ctrl\"\n",
    "  - \"71_ctrl\"\n",
    "  - \"72_ctrl\"\n",
    "  - \"73_ctrl\"\n",
    "  - \"74_ctrl\"\n",
    "  - \"75_ctrl\"\n",
    "  - \"76_ctrl\"\n",
    "  - \"77_ctrl\"\n",
    "  - \"78_ctrl\"\n",
    "  - \"79_ctrl\"\n",
    "  - \"80_ctrl\"\n",
    "  - \"81_ctrl\"\n",
    "  - \"82_ctrl\"\n",
    "  - \"83_ctrl\"\n",
    "  - \"84_ctrl\"\n",
    "  - \"85_ctrl\"\n",
    "  - \"86_ctrl\"\n",
    "  - \"87_ctrl\"\n",
    "  - \"88_ctrl\"\n",
    "  - \"89_ctrl\"\n",
    "  - \"90_ctrl\"\n",
    "  - \"91_ctrl\"\n",
    "  - \"92_ctrl\"\n",
    "  - \"93_ctrl\"\n",
    "  - \"94_ctrl\"\n",
    "  - \"95_ctrl\"\n",
    "  - \"96_ctrl\"\n",
    "  - \"97_ctrl\"\n",
    "  - \"98_ctrl\"\n",
    "  - \"99_ctrl\"\n",
    "  - \"100_ctrl\"\n",
    "  - \"101_ctrl\"\n",
    "  - \"102_ctrl\"\n",
    "  - \"103_ctrl\"\n",
    "  - \"104_ctrl\"\n",
    "  - \"105_ctrl\"\n",
    "  - \"106_ctrl\"\n",
    "  - \"107_ctrl\"\n",
    "  - \"108_ctrl\"\n",
    "  - \"109_ctrl\"\n",
    "  - \"110_ctrl\"\n",
    "  - \"111_ctrl\"\n",
    "  - \"112_ctrl\"\n",
    "  - \"113_ctrl\"\n",
    "  - \"114_ctrl\"\n",
    "  - \"115_ctrl\"\n",
    "  - \"116_ctrl\"\n",
    "  - \"117_ctrl\"\n",
    "  - \"118_ctrl\"\n",
    "  - \"119_ctrl\"\n",
    "  - \"120_ctrl\"\n",
    "  - \"121_ctrl\"\n",
    "  - \"122_ctrl\"\n",
    "  - \"123_ctrl\"\n",
    "  - \"124_ctrl\"\n",
    "  - \"125_ctrl\"\n",
    "  - \"126_ctrl\"\n",
    "  - \"127_ctrl\"\n",
    "  - \"128_ctrl\"\n",
    "  - \"129_ctrl\"\n",
    "  - \"130_ctrl\"\n",
    "  - \"131_ctrl\"\n",
    "  - \"132_ctrl\"\n",
    "  - \"133_ctrl\"\n",
    "  - \"134_ctrl\"\n",
    "  - \"135_ctrl\"\n",
    "  - \"136_ctrl\"\n",
    "  - \"137_ctrl\"\n",
    "  - \"138_ctrl\"\n",
    "  - \"139_ctrl\"\n",
    "  - \"140_ctrl\"\n",
    "  - \"141_ctrl\"\n",
    "  - \"142_ctrl\"\n",
    "  - \"143_ctrl\"\n",
    "  - \"144_ctrl\"\n",
    "  - \"145_ctrl\"\n",
    "  - \"146_ctrl\"\n",
    "  - \"147_ctrl\"\n",
    "  - \"148_ctrl\"\n",
    "  - \"149_ctrl\"\n",
    "  - \"150_ctrl\"\n",
    "  - \"151_ctrl\"\n",
    "  - \"152_ctrl\"\n",
    "  - \"153_ctrl\"\n",
    "  - \"154_ctrl\"\n",
    "  - \"155_ctrl\"\n",
    "  - \"156_ctrl\"\n",
    "  - \"157_ctrl\"\n",
    "  - \"158_ctrl\"\n",
    "  - \"159_ctrl\"\n",
    "  - \"160_ctrl\"\n",
    "  - \"161_ctrl\"\n",
    "  - \"162_ctrl\"\n",
    "  - \"163_ctrl\"\n",
    "  - \"164_ctrl\"\n",
    "  - \"165_ctrl\"\n",
    "  - \"166_ctrl\"\n",
    "  - \"167_ctrl\"\n",
    "  - \"168_ctrl\"\n",
    "  - \"169_ctrl\"\n",
    "  - \"170_ctrl\"\n",
    "  - \"171_ctrl\"\n",
    "  - \"172_ctrl\"\n",
    "  - \"173_ctrl\"\n",
    "  - \"174_ctrl\"\n",
    "  - \"175_ctrl\"\n",
    "  - \"176_ctrl\"\n",
    "  - \"177_ctrl\"\n",
    "  - \"178_ctrl\"\n",
    "  - \"179_ctrl\"\n",
    "  - \"180_ctrl\"\n",
    "  - \"181_ctrl\"\n",
    "  - \"182_ctrl\"\n",
    "  - \"183_ctrl\"\n",
    "  - \"184_ctrl\"\n",
    "  - \"185_ctrl\"\n",
    "  - \"186_ctrl\"\n",
    "  - \"187_ctrl\"\n",
    "  - \"188_ctrl\"\n",
    "  - \"189_ctrl\"\n",
    "  - \"190_ctrl\"\n",
    "  - \"191_ctrl\"\n",
    "  - \"192_ctrl\"\n",
    "  - \"193_ctrl\"\n",
    "  - \"194_ctrl\"\n",
    "  - \"195_ctrl\"\n",
    "  - \"196_ctrl\"\n",
    "  - \"197_ctrl\"\n",
    "  - \"198_ctrl\"\n",
    "  - \"199_ctrl\"\n",
    "  - \"200_ctrl\"\n",
    "  \"\"\"\n",
    "}\n",
    "\n",
    "training_K562_POLR2A_yaml_contents = {\"globals.yaml\":\"\"\"\n",
    "output_folder: ./runs/gene_expression_only_chrom_K562_POLR2A_train/ \n",
    "manual_valid_ids_file: ./annotations/human/manual_validation_ids_chr17_human.txt  #manual_validation_ids_chr17_uncoupled.txt\n",
    "checkpoint_interval: 30300 # 100 epochs #60000\n",
    "sample_interval: 30300 #60000\n",
    "n_epochs: 120\n",
    "batch_size: 64\n",
    "optimizer: \"adamw\"\n",
    "lr: 0.0001 #0.0001\n",
    "device: \"cuda\"\n",
    "compute_attributions: false\n",
    "\n",
    "\"\"\",\n",
    "\"input_cnn.yaml\": \"\"\" \n",
    "input_info:\n",
    "  input_source: ./inputs/landscape_arrays/K562/training/ \n",
    "  input_name: gene_expression\n",
    "  input_type: array\n",
    "\n",
    "model_config:\n",
    "  model_type: cnn\n",
    "  #pre_normalization: \"instancenorm\"\n",
    "  model_init_config:\n",
    "    num_output_features: 512 # before fc_repr_dim\n",
    "    layers: [4,4]\n",
    "    kernel_height: 1\n",
    "    down_stride_width: 2\n",
    "    first_stride_expansion_width: 1\n",
    "    first_kernel_expansion_height: 4 #5\n",
    "    kernel_width: 10  #10\n",
    "    dilation_factor_width: 2\n",
    "    dilation_factor_height: 1\n",
    "    channel_exp_base: 5  #3 #-1\n",
    "    first_channel_expansion: 2\n",
    "    rb_do: .3\n",
    "    stochastic_depth_p: .1\n",
    "    attention_inclusion_cutoff: 1 #50 #256\n",
    "\n",
    "\"\"\",\n",
    "\"fusion.yaml\": \"\"\"\n",
    "model_config:\n",
    "  fc_do: 0.4\n",
    "  fc_task_dim: 256\n",
    "  layers:\n",
    "  - 2\n",
    "  rb_do: 0.4\n",
    "  stochastic_depth_p: 0.5\n",
    "model_type: \"mlp-residual\"\n",
    "\"\"\",\n",
    "\"outputs_2_cond.yaml\":\"\"\"\n",
    "output_info:\n",
    "  output_name: expression_output\n",
    "  output_source: ./targets/K562/polii/training_polii_targets.csv \n",
    "  output_type: tabular\n",
    "\n",
    "model_config: # <- new\n",
    "  model_type: linear # <- new\n",
    "\n",
    "output_type_info:\n",
    "  con_loss_name: \"SmoothL1Loss\"\n",
    "  target_con_columns:\n",
    "  - \"-200_ctrl\"\n",
    "  - \"-199_ctrl\"\n",
    "  - \"-198_ctrl\"\n",
    "  - \"-197_ctrl\"\n",
    "  - \"-196_ctrl\"\n",
    "  - \"-195_ctrl\"\n",
    "  - \"-194_ctrl\"\n",
    "  - \"-193_ctrl\"\n",
    "  - \"-192_ctrl\"\n",
    "  - \"-191_ctrl\"\n",
    "  - \"-190_ctrl\"\n",
    "  - \"-189_ctrl\"\n",
    "  - \"-188_ctrl\"\n",
    "  - \"-187_ctrl\"\n",
    "  - \"-186_ctrl\"\n",
    "  - \"-185_ctrl\"\n",
    "  - \"-184_ctrl\"\n",
    "  - \"-183_ctrl\"\n",
    "  - \"-182_ctrl\"\n",
    "  - \"-181_ctrl\"\n",
    "  - \"-180_ctrl\"\n",
    "  - \"-179_ctrl\"\n",
    "  - \"-178_ctrl\"\n",
    "  - \"-177_ctrl\"\n",
    "  - \"-176_ctrl\"\n",
    "  - \"-175_ctrl\"\n",
    "  - \"-174_ctrl\"\n",
    "  - \"-173_ctrl\"\n",
    "  - \"-172_ctrl\"\n",
    "  - \"-171_ctrl\"\n",
    "  - \"-170_ctrl\"\n",
    "  - \"-169_ctrl\"\n",
    "  - \"-168_ctrl\"\n",
    "  - \"-167_ctrl\"\n",
    "  - \"-166_ctrl\"\n",
    "  - \"-165_ctrl\"\n",
    "  - \"-164_ctrl\"\n",
    "  - \"-163_ctrl\"\n",
    "  - \"-162_ctrl\"\n",
    "  - \"-161_ctrl\"\n",
    "  - \"-160_ctrl\"\n",
    "  - \"-159_ctrl\"\n",
    "  - \"-158_ctrl\"\n",
    "  - \"-157_ctrl\"\n",
    "  - \"-156_ctrl\"\n",
    "  - \"-155_ctrl\"\n",
    "  - \"-154_ctrl\"\n",
    "  - \"-153_ctrl\"\n",
    "  - \"-152_ctrl\"\n",
    "  - \"-151_ctrl\"\n",
    "  - \"-150_ctrl\"\n",
    "  - \"-149_ctrl\"\n",
    "  - \"-148_ctrl\"\n",
    "  - \"-147_ctrl\"\n",
    "  - \"-146_ctrl\"\n",
    "  - \"-145_ctrl\"\n",
    "  - \"-144_ctrl\"\n",
    "  - \"-143_ctrl\"\n",
    "  - \"-142_ctrl\"\n",
    "  - \"-141_ctrl\"\n",
    "  - \"-140_ctrl\"\n",
    "  - \"-139_ctrl\"\n",
    "  - \"-138_ctrl\"\n",
    "  - \"-137_ctrl\"\n",
    "  - \"-136_ctrl\"\n",
    "  - \"-135_ctrl\"\n",
    "  - \"-134_ctrl\"\n",
    "  - \"-133_ctrl\"\n",
    "  - \"-132_ctrl\"\n",
    "  - \"-131_ctrl\"\n",
    "  - \"-130_ctrl\"\n",
    "  - \"-129_ctrl\"\n",
    "  - \"-128_ctrl\"\n",
    "  - \"-127_ctrl\"\n",
    "  - \"-126_ctrl\"\n",
    "  - \"-125_ctrl\"\n",
    "  - \"-124_ctrl\"\n",
    "  - \"-123_ctrl\"\n",
    "  - \"-122_ctrl\"\n",
    "  - \"-121_ctrl\"\n",
    "  - \"-120_ctrl\"\n",
    "  - \"-119_ctrl\"\n",
    "  - \"-118_ctrl\"\n",
    "  - \"-117_ctrl\"\n",
    "  - \"-116_ctrl\"\n",
    "  - \"-115_ctrl\"\n",
    "  - \"-114_ctrl\"\n",
    "  - \"-113_ctrl\"\n",
    "  - \"-112_ctrl\"\n",
    "  - \"-111_ctrl\"\n",
    "  - \"-110_ctrl\"\n",
    "  - \"-109_ctrl\"\n",
    "  - \"-108_ctrl\"\n",
    "  - \"-107_ctrl\"\n",
    "  - \"-106_ctrl\"\n",
    "  - \"-105_ctrl\"\n",
    "  - \"-104_ctrl\"\n",
    "  - \"-103_ctrl\"\n",
    "  - \"-102_ctrl\"\n",
    "  - \"-101_ctrl\"\n",
    "  - \"-100_ctrl\"\n",
    "  - \"-99_ctrl\"\n",
    "  - \"-98_ctrl\"\n",
    "  - \"-97_ctrl\"\n",
    "  - \"-96_ctrl\"\n",
    "  - \"-95_ctrl\"\n",
    "  - \"-94_ctrl\"\n",
    "  - \"-93_ctrl\"\n",
    "  - \"-92_ctrl\"\n",
    "  - \"-91_ctrl\"\n",
    "  - \"-90_ctrl\"\n",
    "  - \"-89_ctrl\"\n",
    "  - \"-88_ctrl\"\n",
    "  - \"-87_ctrl\"\n",
    "  - \"-86_ctrl\"\n",
    "  - \"-85_ctrl\"\n",
    "  - \"-84_ctrl\"\n",
    "  - \"-83_ctrl\"\n",
    "  - \"-82_ctrl\"\n",
    "  - \"-81_ctrl\"\n",
    "  - \"-80_ctrl\"\n",
    "  - \"-79_ctrl\"\n",
    "  - \"-78_ctrl\"\n",
    "  - \"-77_ctrl\"\n",
    "  - \"-76_ctrl\"\n",
    "  - \"-75_ctrl\"\n",
    "  - \"-74_ctrl\"\n",
    "  - \"-73_ctrl\"\n",
    "  - \"-72_ctrl\"\n",
    "  - \"-71_ctrl\"\n",
    "  - \"-70_ctrl\"\n",
    "  - \"-69_ctrl\"\n",
    "  - \"-68_ctrl\"\n",
    "  - \"-67_ctrl\"\n",
    "  - \"-66_ctrl\"\n",
    "  - \"-65_ctrl\"\n",
    "  - \"-64_ctrl\"\n",
    "  - \"-63_ctrl\"\n",
    "  - \"-62_ctrl\"\n",
    "  - \"-61_ctrl\"\n",
    "  - \"-60_ctrl\"\n",
    "  - \"-59_ctrl\"\n",
    "  - \"-58_ctrl\"\n",
    "  - \"-57_ctrl\"\n",
    "  - \"-56_ctrl\"\n",
    "  - \"-55_ctrl\"\n",
    "  - \"-54_ctrl\"\n",
    "  - \"-53_ctrl\"\n",
    "  - \"-52_ctrl\"\n",
    "  - \"-51_ctrl\"\n",
    "  - \"-50_ctrl\"\n",
    "  - \"-49_ctrl\"\n",
    "  - \"-48_ctrl\"\n",
    "  - \"-47_ctrl\"\n",
    "  - \"-46_ctrl\"\n",
    "  - \"-45_ctrl\"\n",
    "  - \"-44_ctrl\"\n",
    "  - \"-43_ctrl\"\n",
    "  - \"-42_ctrl\"\n",
    "  - \"-41_ctrl\"\n",
    "  - \"-40_ctrl\"\n",
    "  - \"-39_ctrl\"\n",
    "  - \"-38_ctrl\"\n",
    "  - \"-37_ctrl\"\n",
    "  - \"-36_ctrl\"\n",
    "  - \"-35_ctrl\"\n",
    "  - \"-34_ctrl\"\n",
    "  - \"-33_ctrl\"\n",
    "  - \"-32_ctrl\"\n",
    "  - \"-31_ctrl\"\n",
    "  - \"-30_ctrl\"\n",
    "  - \"-29_ctrl\"\n",
    "  - \"-28_ctrl\"\n",
    "  - \"-27_ctrl\"\n",
    "  - \"-26_ctrl\"\n",
    "  - \"-25_ctrl\"\n",
    "  - \"-24_ctrl\"\n",
    "  - \"-23_ctrl\"\n",
    "  - \"-22_ctrl\"\n",
    "  - \"-21_ctrl\"\n",
    "  - \"-20_ctrl\"\n",
    "  - \"-19_ctrl\"\n",
    "  - \"-18_ctrl\"\n",
    "  - \"-17_ctrl\"\n",
    "  - \"-16_ctrl\"\n",
    "  - \"-15_ctrl\"\n",
    "  - \"-14_ctrl\"\n",
    "  - \"-13_ctrl\"\n",
    "  - \"-12_ctrl\"\n",
    "  - \"-11_ctrl\"\n",
    "  - \"-10_ctrl\"\n",
    "  - \"-9_ctrl\"\n",
    "  - \"-8_ctrl\"\n",
    "  - \"-7_ctrl\"\n",
    "  - \"-6_ctrl\"\n",
    "  - \"-5_ctrl\"\n",
    "  - \"-4_ctrl\"\n",
    "  - \"-3_ctrl\"\n",
    "  - \"-2_ctrl\"\n",
    "  - \"-1_ctrl\"\n",
    "  - \"0_ctrl\"\n",
    "  - \"1_ctrl\"\n",
    "  - \"2_ctrl\"\n",
    "  - \"3_ctrl\"\n",
    "  - \"4_ctrl\"\n",
    "  - \"5_ctrl\"\n",
    "  - \"6_ctrl\"\n",
    "  - \"7_ctrl\"\n",
    "  - \"8_ctrl\"\n",
    "  - \"9_ctrl\"\n",
    "  - \"10_ctrl\"\n",
    "  - \"11_ctrl\"\n",
    "  - \"12_ctrl\"\n",
    "  - \"13_ctrl\"\n",
    "  - \"14_ctrl\"\n",
    "  - \"15_ctrl\"\n",
    "  - \"16_ctrl\"\n",
    "  - \"17_ctrl\"\n",
    "  - \"18_ctrl\"\n",
    "  - \"19_ctrl\"\n",
    "  - \"20_ctrl\"\n",
    "  - \"21_ctrl\"\n",
    "  - \"22_ctrl\"\n",
    "  - \"23_ctrl\"\n",
    "  - \"24_ctrl\"\n",
    "  - \"25_ctrl\"\n",
    "  - \"26_ctrl\"\n",
    "  - \"27_ctrl\"\n",
    "  - \"28_ctrl\"\n",
    "  - \"29_ctrl\"\n",
    "  - \"30_ctrl\"\n",
    "  - \"31_ctrl\"\n",
    "  - \"32_ctrl\"\n",
    "  - \"33_ctrl\"\n",
    "  - \"34_ctrl\"\n",
    "  - \"35_ctrl\"\n",
    "  - \"36_ctrl\"\n",
    "  - \"37_ctrl\"\n",
    "  - \"38_ctrl\"\n",
    "  - \"39_ctrl\"\n",
    "  - \"40_ctrl\"\n",
    "  - \"41_ctrl\"\n",
    "  - \"42_ctrl\"\n",
    "  - \"43_ctrl\"\n",
    "  - \"44_ctrl\"\n",
    "  - \"45_ctrl\"\n",
    "  - \"46_ctrl\"\n",
    "  - \"47_ctrl\"\n",
    "  - \"48_ctrl\"\n",
    "  - \"49_ctrl\"\n",
    "  - \"50_ctrl\"\n",
    "  - \"51_ctrl\"\n",
    "  - \"52_ctrl\"\n",
    "  - \"53_ctrl\"\n",
    "  - \"54_ctrl\"\n",
    "  - \"55_ctrl\"\n",
    "  - \"56_ctrl\"\n",
    "  - \"57_ctrl\"\n",
    "  - \"58_ctrl\"\n",
    "  - \"59_ctrl\"\n",
    "  - \"60_ctrl\"\n",
    "  - \"61_ctrl\"\n",
    "  - \"62_ctrl\"\n",
    "  - \"63_ctrl\"\n",
    "  - \"64_ctrl\"\n",
    "  - \"65_ctrl\"\n",
    "  - \"66_ctrl\"\n",
    "  - \"67_ctrl\"\n",
    "  - \"68_ctrl\"\n",
    "  - \"69_ctrl\"\n",
    "  - \"70_ctrl\"\n",
    "  - \"71_ctrl\"\n",
    "  - \"72_ctrl\"\n",
    "  - \"73_ctrl\"\n",
    "  - \"74_ctrl\"\n",
    "  - \"75_ctrl\"\n",
    "  - \"76_ctrl\"\n",
    "  - \"77_ctrl\"\n",
    "  - \"78_ctrl\"\n",
    "  - \"79_ctrl\"\n",
    "  - \"80_ctrl\"\n",
    "  - \"81_ctrl\"\n",
    "  - \"82_ctrl\"\n",
    "  - \"83_ctrl\"\n",
    "  - \"84_ctrl\"\n",
    "  - \"85_ctrl\"\n",
    "  - \"86_ctrl\"\n",
    "  - \"87_ctrl\"\n",
    "  - \"88_ctrl\"\n",
    "  - \"89_ctrl\"\n",
    "  - \"90_ctrl\"\n",
    "  - \"91_ctrl\"\n",
    "  - \"92_ctrl\"\n",
    "  - \"93_ctrl\"\n",
    "  - \"94_ctrl\"\n",
    "  - \"95_ctrl\"\n",
    "  - \"96_ctrl\"\n",
    "  - \"97_ctrl\"\n",
    "  - \"98_ctrl\"\n",
    "  - \"99_ctrl\"\n",
    "  - \"100_ctrl\"\n",
    "  - \"101_ctrl\"\n",
    "  - \"102_ctrl\"\n",
    "  - \"103_ctrl\"\n",
    "  - \"104_ctrl\"\n",
    "  - \"105_ctrl\"\n",
    "  - \"106_ctrl\"\n",
    "  - \"107_ctrl\"\n",
    "  - \"108_ctrl\"\n",
    "  - \"109_ctrl\"\n",
    "  - \"110_ctrl\"\n",
    "  - \"111_ctrl\"\n",
    "  - \"112_ctrl\"\n",
    "  - \"113_ctrl\"\n",
    "  - \"114_ctrl\"\n",
    "  - \"115_ctrl\"\n",
    "  - \"116_ctrl\"\n",
    "  - \"117_ctrl\"\n",
    "  - \"118_ctrl\"\n",
    "  - \"119_ctrl\"\n",
    "  - \"120_ctrl\"\n",
    "  - \"121_ctrl\"\n",
    "  - \"122_ctrl\"\n",
    "  - \"123_ctrl\"\n",
    "  - \"124_ctrl\"\n",
    "  - \"125_ctrl\"\n",
    "  - \"126_ctrl\"\n",
    "  - \"127_ctrl\"\n",
    "  - \"128_ctrl\"\n",
    "  - \"129_ctrl\"\n",
    "  - \"130_ctrl\"\n",
    "  - \"131_ctrl\"\n",
    "  - \"132_ctrl\"\n",
    "  - \"133_ctrl\"\n",
    "  - \"134_ctrl\"\n",
    "  - \"135_ctrl\"\n",
    "  - \"136_ctrl\"\n",
    "  - \"137_ctrl\"\n",
    "  - \"138_ctrl\"\n",
    "  - \"139_ctrl\"\n",
    "  - \"140_ctrl\"\n",
    "  - \"141_ctrl\"\n",
    "  - \"142_ctrl\"\n",
    "  - \"143_ctrl\"\n",
    "  - \"144_ctrl\"\n",
    "  - \"145_ctrl\"\n",
    "  - \"146_ctrl\"\n",
    "  - \"147_ctrl\"\n",
    "  - \"148_ctrl\"\n",
    "  - \"149_ctrl\"\n",
    "  - \"150_ctrl\"\n",
    "  - \"151_ctrl\"\n",
    "  - \"152_ctrl\"\n",
    "  - \"153_ctrl\"\n",
    "  - \"154_ctrl\"\n",
    "  - \"155_ctrl\"\n",
    "  - \"156_ctrl\"\n",
    "  - \"157_ctrl\"\n",
    "  - \"158_ctrl\"\n",
    "  - \"159_ctrl\"\n",
    "  - \"160_ctrl\"\n",
    "  - \"161_ctrl\"\n",
    "  - \"162_ctrl\"\n",
    "  - \"163_ctrl\"\n",
    "  - \"164_ctrl\"\n",
    "  - \"165_ctrl\"\n",
    "  - \"166_ctrl\"\n",
    "  - \"167_ctrl\"\n",
    "  - \"168_ctrl\"\n",
    "  - \"169_ctrl\"\n",
    "  - \"170_ctrl\"\n",
    "  - \"171_ctrl\"\n",
    "  - \"172_ctrl\"\n",
    "  - \"173_ctrl\"\n",
    "  - \"174_ctrl\"\n",
    "  - \"175_ctrl\"\n",
    "  - \"176_ctrl\"\n",
    "  - \"177_ctrl\"\n",
    "  - \"178_ctrl\"\n",
    "  - \"179_ctrl\"\n",
    "  - \"180_ctrl\"\n",
    "  - \"181_ctrl\"\n",
    "  - \"182_ctrl\"\n",
    "  - \"183_ctrl\"\n",
    "  - \"184_ctrl\"\n",
    "  - \"185_ctrl\"\n",
    "  - \"186_ctrl\"\n",
    "  - \"187_ctrl\"\n",
    "  - \"188_ctrl\"\n",
    "  - \"189_ctrl\"\n",
    "  - \"190_ctrl\"\n",
    "  - \"191_ctrl\"\n",
    "  - \"192_ctrl\"\n",
    "  - \"193_ctrl\"\n",
    "  - \"194_ctrl\"\n",
    "  - \"195_ctrl\"\n",
    "  - \"196_ctrl\"\n",
    "  - \"197_ctrl\"\n",
    "  - \"198_ctrl\"\n",
    "  - \"199_ctrl\"\n",
    "  - \"200_ctrl\"\n",
    "  \"\"\"\n",
    "}\n",
    "\n",
    "test_K562_POLR2A_enhancer_centric_yaml_contents = {\"globals.yaml\":\"\"\"\n",
    "checkpoint_interval: 30300 # 100 epochs #60000\n",
    "sample_interval: 30300 #60000\n",
    "n_epochs: 120\n",
    "batch_size: 64\n",
    "optimizer: \"adamw\"\n",
    "lr: 0.0001 #0.0001\n",
    "device: \"cuda\"\n",
    "compute_attributions: true\n",
    "\"\"\",\n",
    "\"input_cnn.yaml\": \"\"\" \n",
    "input_info:\n",
    "  input_source: ./inputs/perturbed_landscape_arrays/human_enhancer_arrays/\n",
    "  input_name: gene_expression\n",
    "  input_type: array\n",
    "\n",
    "model_config:\n",
    "  model_type: cnn\n",
    "  #pre_normalization: \"instancenorm\"\n",
    "  model_init_config:\n",
    "    num_output_features: 512 # before fc_repr_dim\n",
    "    layers: [4,4]\n",
    "    kernel_height: 1\n",
    "    down_stride_width: 2\n",
    "    first_stride_expansion_width: 1\n",
    "    first_kernel_expansion_height: 4 #5\n",
    "    kernel_width: 10  #10\n",
    "    dilation_factor_width: 2\n",
    "    dilation_factor_height: 1\n",
    "    channel_exp_base: 5  #3 #-1\n",
    "    first_channel_expansion: 2\n",
    "    rb_do: .3\n",
    "    stochastic_depth_p: .1\n",
    "    attention_inclusion_cutoff: 1 #50 #256\n",
    "\n",
    "\"\"\",\n",
    "\"fusion.yaml\": \"\"\"\n",
    "model_config:\n",
    "  fc_do: 0.4\n",
    "  fc_task_dim: 256\n",
    "  layers:\n",
    "  - 2\n",
    "  rb_do: 0.4\n",
    "  stochastic_depth_p: 0.5\n",
    "model_type: \"mlp-residual\"\n",
    "\"\"\",\n",
    "\"outputs_2_cond.yaml\":\"\"\"\n",
    "output_info:\n",
    "  output_name: expression_output\n",
    "  output_source: ./targets/K562/Enhancer_centered_targets_human.csv\n",
    "  output_type: tabular\n",
    "\n",
    "model_config: # <- new\n",
    "  model_type: linear # <- new\n",
    "\n",
    "output_type_info:\n",
    "  con_loss_name: \"SmoothL1Loss\"\n",
    "  target_con_columns:\n",
    "  - \"-200_ctrl\"\n",
    "  - \"-199_ctrl\"\n",
    "  - \"-198_ctrl\"\n",
    "  - \"-197_ctrl\"\n",
    "  - \"-196_ctrl\"\n",
    "  - \"-195_ctrl\"\n",
    "  - \"-194_ctrl\"\n",
    "  - \"-193_ctrl\"\n",
    "  - \"-192_ctrl\"\n",
    "  - \"-191_ctrl\"\n",
    "  - \"-190_ctrl\"\n",
    "  - \"-189_ctrl\"\n",
    "  - \"-188_ctrl\"\n",
    "  - \"-187_ctrl\"\n",
    "  - \"-186_ctrl\"\n",
    "  - \"-185_ctrl\"\n",
    "  - \"-184_ctrl\"\n",
    "  - \"-183_ctrl\"\n",
    "  - \"-182_ctrl\"\n",
    "  - \"-181_ctrl\"\n",
    "  - \"-180_ctrl\"\n",
    "  - \"-179_ctrl\"\n",
    "  - \"-178_ctrl\"\n",
    "  - \"-177_ctrl\"\n",
    "  - \"-176_ctrl\"\n",
    "  - \"-175_ctrl\"\n",
    "  - \"-174_ctrl\"\n",
    "  - \"-173_ctrl\"\n",
    "  - \"-172_ctrl\"\n",
    "  - \"-171_ctrl\"\n",
    "  - \"-170_ctrl\"\n",
    "  - \"-169_ctrl\"\n",
    "  - \"-168_ctrl\"\n",
    "  - \"-167_ctrl\"\n",
    "  - \"-166_ctrl\"\n",
    "  - \"-165_ctrl\"\n",
    "  - \"-164_ctrl\"\n",
    "  - \"-163_ctrl\"\n",
    "  - \"-162_ctrl\"\n",
    "  - \"-161_ctrl\"\n",
    "  - \"-160_ctrl\"\n",
    "  - \"-159_ctrl\"\n",
    "  - \"-158_ctrl\"\n",
    "  - \"-157_ctrl\"\n",
    "  - \"-156_ctrl\"\n",
    "  - \"-155_ctrl\"\n",
    "  - \"-154_ctrl\"\n",
    "  - \"-153_ctrl\"\n",
    "  - \"-152_ctrl\"\n",
    "  - \"-151_ctrl\"\n",
    "  - \"-150_ctrl\"\n",
    "  - \"-149_ctrl\"\n",
    "  - \"-148_ctrl\"\n",
    "  - \"-147_ctrl\"\n",
    "  - \"-146_ctrl\"\n",
    "  - \"-145_ctrl\"\n",
    "  - \"-144_ctrl\"\n",
    "  - \"-143_ctrl\"\n",
    "  - \"-142_ctrl\"\n",
    "  - \"-141_ctrl\"\n",
    "  - \"-140_ctrl\"\n",
    "  - \"-139_ctrl\"\n",
    "  - \"-138_ctrl\"\n",
    "  - \"-137_ctrl\"\n",
    "  - \"-136_ctrl\"\n",
    "  - \"-135_ctrl\"\n",
    "  - \"-134_ctrl\"\n",
    "  - \"-133_ctrl\"\n",
    "  - \"-132_ctrl\"\n",
    "  - \"-131_ctrl\"\n",
    "  - \"-130_ctrl\"\n",
    "  - \"-129_ctrl\"\n",
    "  - \"-128_ctrl\"\n",
    "  - \"-127_ctrl\"\n",
    "  - \"-126_ctrl\"\n",
    "  - \"-125_ctrl\"\n",
    "  - \"-124_ctrl\"\n",
    "  - \"-123_ctrl\"\n",
    "  - \"-122_ctrl\"\n",
    "  - \"-121_ctrl\"\n",
    "  - \"-120_ctrl\"\n",
    "  - \"-119_ctrl\"\n",
    "  - \"-118_ctrl\"\n",
    "  - \"-117_ctrl\"\n",
    "  - \"-116_ctrl\"\n",
    "  - \"-115_ctrl\"\n",
    "  - \"-114_ctrl\"\n",
    "  - \"-113_ctrl\"\n",
    "  - \"-112_ctrl\"\n",
    "  - \"-111_ctrl\"\n",
    "  - \"-110_ctrl\"\n",
    "  - \"-109_ctrl\"\n",
    "  - \"-108_ctrl\"\n",
    "  - \"-107_ctrl\"\n",
    "  - \"-106_ctrl\"\n",
    "  - \"-105_ctrl\"\n",
    "  - \"-104_ctrl\"\n",
    "  - \"-103_ctrl\"\n",
    "  - \"-102_ctrl\"\n",
    "  - \"-101_ctrl\"\n",
    "  - \"-100_ctrl\"\n",
    "  - \"-99_ctrl\"\n",
    "  - \"-98_ctrl\"\n",
    "  - \"-97_ctrl\"\n",
    "  - \"-96_ctrl\"\n",
    "  - \"-95_ctrl\"\n",
    "  - \"-94_ctrl\"\n",
    "  - \"-93_ctrl\"\n",
    "  - \"-92_ctrl\"\n",
    "  - \"-91_ctrl\"\n",
    "  - \"-90_ctrl\"\n",
    "  - \"-89_ctrl\"\n",
    "  - \"-88_ctrl\"\n",
    "  - \"-87_ctrl\"\n",
    "  - \"-86_ctrl\"\n",
    "  - \"-85_ctrl\"\n",
    "  - \"-84_ctrl\"\n",
    "  - \"-83_ctrl\"\n",
    "  - \"-82_ctrl\"\n",
    "  - \"-81_ctrl\"\n",
    "  - \"-80_ctrl\"\n",
    "  - \"-79_ctrl\"\n",
    "  - \"-78_ctrl\"\n",
    "  - \"-77_ctrl\"\n",
    "  - \"-76_ctrl\"\n",
    "  - \"-75_ctrl\"\n",
    "  - \"-74_ctrl\"\n",
    "  - \"-73_ctrl\"\n",
    "  - \"-72_ctrl\"\n",
    "  - \"-71_ctrl\"\n",
    "  - \"-70_ctrl\"\n",
    "  - \"-69_ctrl\"\n",
    "  - \"-68_ctrl\"\n",
    "  - \"-67_ctrl\"\n",
    "  - \"-66_ctrl\"\n",
    "  - \"-65_ctrl\"\n",
    "  - \"-64_ctrl\"\n",
    "  - \"-63_ctrl\"\n",
    "  - \"-62_ctrl\"\n",
    "  - \"-61_ctrl\"\n",
    "  - \"-60_ctrl\"\n",
    "  - \"-59_ctrl\"\n",
    "  - \"-58_ctrl\"\n",
    "  - \"-57_ctrl\"\n",
    "  - \"-56_ctrl\"\n",
    "  - \"-55_ctrl\"\n",
    "  - \"-54_ctrl\"\n",
    "  - \"-53_ctrl\"\n",
    "  - \"-52_ctrl\"\n",
    "  - \"-51_ctrl\"\n",
    "  - \"-50_ctrl\"\n",
    "  - \"-49_ctrl\"\n",
    "  - \"-48_ctrl\"\n",
    "  - \"-47_ctrl\"\n",
    "  - \"-46_ctrl\"\n",
    "  - \"-45_ctrl\"\n",
    "  - \"-44_ctrl\"\n",
    "  - \"-43_ctrl\"\n",
    "  - \"-42_ctrl\"\n",
    "  - \"-41_ctrl\"\n",
    "  - \"-40_ctrl\"\n",
    "  - \"-39_ctrl\"\n",
    "  - \"-38_ctrl\"\n",
    "  - \"-37_ctrl\"\n",
    "  - \"-36_ctrl\"\n",
    "  - \"-35_ctrl\"\n",
    "  - \"-34_ctrl\"\n",
    "  - \"-33_ctrl\"\n",
    "  - \"-32_ctrl\"\n",
    "  - \"-31_ctrl\"\n",
    "  - \"-30_ctrl\"\n",
    "  - \"-29_ctrl\"\n",
    "  - \"-28_ctrl\"\n",
    "  - \"-27_ctrl\"\n",
    "  - \"-26_ctrl\"\n",
    "  - \"-25_ctrl\"\n",
    "  - \"-24_ctrl\"\n",
    "  - \"-23_ctrl\"\n",
    "  - \"-22_ctrl\"\n",
    "  - \"-21_ctrl\"\n",
    "  - \"-20_ctrl\"\n",
    "  - \"-19_ctrl\"\n",
    "  - \"-18_ctrl\"\n",
    "  - \"-17_ctrl\"\n",
    "  - \"-16_ctrl\"\n",
    "  - \"-15_ctrl\"\n",
    "  - \"-14_ctrl\"\n",
    "  - \"-13_ctrl\"\n",
    "  - \"-12_ctrl\"\n",
    "  - \"-11_ctrl\"\n",
    "  - \"-10_ctrl\"\n",
    "  - \"-9_ctrl\"\n",
    "  - \"-8_ctrl\"\n",
    "  - \"-7_ctrl\"\n",
    "  - \"-6_ctrl\"\n",
    "  - \"-5_ctrl\"\n",
    "  - \"-4_ctrl\"\n",
    "  - \"-3_ctrl\"\n",
    "  - \"-2_ctrl\"\n",
    "  - \"-1_ctrl\"\n",
    "  - \"0_ctrl\"\n",
    "  - \"1_ctrl\"\n",
    "  - \"2_ctrl\"\n",
    "  - \"3_ctrl\"\n",
    "  - \"4_ctrl\"\n",
    "  - \"5_ctrl\"\n",
    "  - \"6_ctrl\"\n",
    "  - \"7_ctrl\"\n",
    "  - \"8_ctrl\"\n",
    "  - \"9_ctrl\"\n",
    "  - \"10_ctrl\"\n",
    "  - \"11_ctrl\"\n",
    "  - \"12_ctrl\"\n",
    "  - \"13_ctrl\"\n",
    "  - \"14_ctrl\"\n",
    "  - \"15_ctrl\"\n",
    "  - \"16_ctrl\"\n",
    "  - \"17_ctrl\"\n",
    "  - \"18_ctrl\"\n",
    "  - \"19_ctrl\"\n",
    "  - \"20_ctrl\"\n",
    "  - \"21_ctrl\"\n",
    "  - \"22_ctrl\"\n",
    "  - \"23_ctrl\"\n",
    "  - \"24_ctrl\"\n",
    "  - \"25_ctrl\"\n",
    "  - \"26_ctrl\"\n",
    "  - \"27_ctrl\"\n",
    "  - \"28_ctrl\"\n",
    "  - \"29_ctrl\"\n",
    "  - \"30_ctrl\"\n",
    "  - \"31_ctrl\"\n",
    "  - \"32_ctrl\"\n",
    "  - \"33_ctrl\"\n",
    "  - \"34_ctrl\"\n",
    "  - \"35_ctrl\"\n",
    "  - \"36_ctrl\"\n",
    "  - \"37_ctrl\"\n",
    "  - \"38_ctrl\"\n",
    "  - \"39_ctrl\"\n",
    "  - \"40_ctrl\"\n",
    "  - \"41_ctrl\"\n",
    "  - \"42_ctrl\"\n",
    "  - \"43_ctrl\"\n",
    "  - \"44_ctrl\"\n",
    "  - \"45_ctrl\"\n",
    "  - \"46_ctrl\"\n",
    "  - \"47_ctrl\"\n",
    "  - \"48_ctrl\"\n",
    "  - \"49_ctrl\"\n",
    "  - \"50_ctrl\"\n",
    "  - \"51_ctrl\"\n",
    "  - \"52_ctrl\"\n",
    "  - \"53_ctrl\"\n",
    "  - \"54_ctrl\"\n",
    "  - \"55_ctrl\"\n",
    "  - \"56_ctrl\"\n",
    "  - \"57_ctrl\"\n",
    "  - \"58_ctrl\"\n",
    "  - \"59_ctrl\"\n",
    "  - \"60_ctrl\"\n",
    "  - \"61_ctrl\"\n",
    "  - \"62_ctrl\"\n",
    "  - \"63_ctrl\"\n",
    "  - \"64_ctrl\"\n",
    "  - \"65_ctrl\"\n",
    "  - \"66_ctrl\"\n",
    "  - \"67_ctrl\"\n",
    "  - \"68_ctrl\"\n",
    "  - \"69_ctrl\"\n",
    "  - \"70_ctrl\"\n",
    "  - \"71_ctrl\"\n",
    "  - \"72_ctrl\"\n",
    "  - \"73_ctrl\"\n",
    "  - \"74_ctrl\"\n",
    "  - \"75_ctrl\"\n",
    "  - \"76_ctrl\"\n",
    "  - \"77_ctrl\"\n",
    "  - \"78_ctrl\"\n",
    "  - \"79_ctrl\"\n",
    "  - \"80_ctrl\"\n",
    "  - \"81_ctrl\"\n",
    "  - \"82_ctrl\"\n",
    "  - \"83_ctrl\"\n",
    "  - \"84_ctrl\"\n",
    "  - \"85_ctrl\"\n",
    "  - \"86_ctrl\"\n",
    "  - \"87_ctrl\"\n",
    "  - \"88_ctrl\"\n",
    "  - \"89_ctrl\"\n",
    "  - \"90_ctrl\"\n",
    "  - \"91_ctrl\"\n",
    "  - \"92_ctrl\"\n",
    "  - \"93_ctrl\"\n",
    "  - \"94_ctrl\"\n",
    "  - \"95_ctrl\"\n",
    "  - \"96_ctrl\"\n",
    "  - \"97_ctrl\"\n",
    "  - \"98_ctrl\"\n",
    "  - \"99_ctrl\"\n",
    "  - \"100_ctrl\"\n",
    "  - \"101_ctrl\"\n",
    "  - \"102_ctrl\"\n",
    "  - \"103_ctrl\"\n",
    "  - \"104_ctrl\"\n",
    "  - \"105_ctrl\"\n",
    "  - \"106_ctrl\"\n",
    "  - \"107_ctrl\"\n",
    "  - \"108_ctrl\"\n",
    "  - \"109_ctrl\"\n",
    "  - \"110_ctrl\"\n",
    "  - \"111_ctrl\"\n",
    "  - \"112_ctrl\"\n",
    "  - \"113_ctrl\"\n",
    "  - \"114_ctrl\"\n",
    "  - \"115_ctrl\"\n",
    "  - \"116_ctrl\"\n",
    "  - \"117_ctrl\"\n",
    "  - \"118_ctrl\"\n",
    "  - \"119_ctrl\"\n",
    "  - \"120_ctrl\"\n",
    "  - \"121_ctrl\"\n",
    "  - \"122_ctrl\"\n",
    "  - \"123_ctrl\"\n",
    "  - \"124_ctrl\"\n",
    "  - \"125_ctrl\"\n",
    "  - \"126_ctrl\"\n",
    "  - \"127_ctrl\"\n",
    "  - \"128_ctrl\"\n",
    "  - \"129_ctrl\"\n",
    "  - \"130_ctrl\"\n",
    "  - \"131_ctrl\"\n",
    "  - \"132_ctrl\"\n",
    "  - \"133_ctrl\"\n",
    "  - \"134_ctrl\"\n",
    "  - \"135_ctrl\"\n",
    "  - \"136_ctrl\"\n",
    "  - \"137_ctrl\"\n",
    "  - \"138_ctrl\"\n",
    "  - \"139_ctrl\"\n",
    "  - \"140_ctrl\"\n",
    "  - \"141_ctrl\"\n",
    "  - \"142_ctrl\"\n",
    "  - \"143_ctrl\"\n",
    "  - \"144_ctrl\"\n",
    "  - \"145_ctrl\"\n",
    "  - \"146_ctrl\"\n",
    "  - \"147_ctrl\"\n",
    "  - \"148_ctrl\"\n",
    "  - \"149_ctrl\"\n",
    "  - \"150_ctrl\"\n",
    "  - \"151_ctrl\"\n",
    "  - \"152_ctrl\"\n",
    "  - \"153_ctrl\"\n",
    "  - \"154_ctrl\"\n",
    "  - \"155_ctrl\"\n",
    "  - \"156_ctrl\"\n",
    "  - \"157_ctrl\"\n",
    "  - \"158_ctrl\"\n",
    "  - \"159_ctrl\"\n",
    "  - \"160_ctrl\"\n",
    "  - \"161_ctrl\"\n",
    "  - \"162_ctrl\"\n",
    "  - \"163_ctrl\"\n",
    "  - \"164_ctrl\"\n",
    "  - \"165_ctrl\"\n",
    "  - \"166_ctrl\"\n",
    "  - \"167_ctrl\"\n",
    "  - \"168_ctrl\"\n",
    "  - \"169_ctrl\"\n",
    "  - \"170_ctrl\"\n",
    "  - \"171_ctrl\"\n",
    "  - \"172_ctrl\"\n",
    "  - \"173_ctrl\"\n",
    "  - \"174_ctrl\"\n",
    "  - \"175_ctrl\"\n",
    "  - \"176_ctrl\"\n",
    "  - \"177_ctrl\"\n",
    "  - \"178_ctrl\"\n",
    "  - \"179_ctrl\"\n",
    "  - \"180_ctrl\"\n",
    "  - \"181_ctrl\"\n",
    "  - \"182_ctrl\"\n",
    "  - \"183_ctrl\"\n",
    "  - \"184_ctrl\"\n",
    "  - \"185_ctrl\"\n",
    "  - \"186_ctrl\"\n",
    "  - \"187_ctrl\"\n",
    "  - \"188_ctrl\"\n",
    "  - \"189_ctrl\"\n",
    "  - \"190_ctrl\"\n",
    "  - \"191_ctrl\"\n",
    "  - \"192_ctrl\"\n",
    "  - \"193_ctrl\"\n",
    "  - \"194_ctrl\"\n",
    "  - \"195_ctrl\"\n",
    "  - \"196_ctrl\"\n",
    "  - \"197_ctrl\"\n",
    "  - \"198_ctrl\"\n",
    "  - \"199_ctrl\"\n",
    "  - \"200_ctrl\"\n",
    "  \"\"\"\n",
    "}\n",
    "\n",
    "\n",
    "# Predict promoter perturbations: configs\n",
    "for file,content in predict_prom_yaml_contents.items():\n",
    "  with open(config_paths[0] / file, 'w') as f:\n",
    "      f.write(content)\n",
    "  \n",
    "# Training shorter context model: configs\n",
    "for file,content in training_20kbp_yaml_contents.items():\n",
    "  with open(config_paths[1] / file, 'w') as f:\n",
    "      f.write(content)\n",
    "\n",
    "# Training only chrom pure conv with softplus activation SmoothL1 loss: configs\n",
    "for file,content in training_softplus_contents.items():\n",
    "  with open(config_paths[2] / file, 'w') as f:\n",
    "      f.write(content)\n",
    "\n",
    "# Training only chrom pure conv with softplus activation and Poisson NLL: configs\n",
    "for file,content in training_softplus_poisson_contents.items():\n",
    "  with open(config_paths[3] / file, 'w') as f:\n",
    "      f.write(content)\n",
    "\n",
    "# Testing shorter context model: configs\n",
    "for file,content in test_20kbp_yaml_contents.items():\n",
    "  with open(config_paths[4] / file, 'w') as f:\n",
    "      f.write(content)\n",
    "\n",
    "# Training with chr19 holdout: configs\n",
    "for file,content in training_chr19_holdout_yaml_contents.items():\n",
    "  with open(config_paths[5] / file, 'w') as f:\n",
    "      f.write(content)\n",
    "\n",
    "# Training with chr19 holdout: configs\n",
    "for file,content in test_chr19_holdout_yaml_contents.items():\n",
    "  with open(config_paths[6] / file, 'w') as f:\n",
    "      f.write(content)\n",
    "\n",
    "# Training with chr19 holdout: configs\n",
    "for file,content in training_no_H3K27ac_contents.items():\n",
    "  with open(config_paths[7] / file, 'w') as f:\n",
    "      f.write(content)\n",
    "\n",
    "# Test with chr19 holdout: configs\n",
    "for file,content in test_no_H3K27ac_yaml_contents.items():\n",
    "  with open(config_paths[8] / file, 'w') as f:\n",
    "      f.write(content)\n",
    "\n",
    "# Test with chr19 holdout: configs\n",
    "for file,content in predict_20kbp_context_yaml_contents.items():\n",
    "  with open(config_paths[9] / file, 'w') as f:\n",
    "      f.write(content)\n",
    "\n",
    "for file,content in training_1kbp_binning_yaml_contents.items():\n",
    "  with open(config_paths[10] / file, 'w') as f:\n",
    "      f.write(content)\n",
    "\n",
    "for file,content in test_1kbp_binning_yaml_contents.items():\n",
    "  with open(config_paths[11] / file, 'w') as f:\n",
    "      f.write(content)\n",
    "\n",
    "for file,content in enhancer_centric_yaml_contents.items():\n",
    "  with open(config_paths[12] / file, 'w') as f:\n",
    "      f.write(content)\n",
    "\n",
    "for file,content in training_K562_yaml_contents.items():\n",
    "  with open(config_paths[13] / file, 'w') as f:\n",
    "      f.write(content)\n",
    "\n",
    "for file,content in test_K562_yaml_contents.items():\n",
    "  with open(config_paths[14] / file, 'w') as f:\n",
    "      f.write(content)\n",
    "\n",
    "for file,content in training_prom_CHiC_yaml_contents.items():\n",
    "  with open(config_paths[15] / file, 'w') as f:\n",
    "      f.write(content)\n",
    "\n",
    "for file,content in test_prom_CHiC_yaml_contents.items():\n",
    "  with open(config_paths[16] / file, 'w') as f:\n",
    "      f.write(content)\n",
    "\n",
    "for file,content in training_K562_POLR2A_yaml_contents.items():\n",
    "  with open(config_paths[17] / file, 'w') as f:\n",
    "      f.write(content)\n",
    "\n",
    "for file,content in test_K562_POLR2A_enhancer_centric_yaml_contents.items():\n",
    "  with open(config_paths[18] / file, 'w') as f:\n",
    "      f.write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III) Enhancer-centric perturbational analysis\n",
    "\n",
    "Here we will follow an enhancer-centric approach to identify the genes that feel the perturbations the most.\n",
    "\n",
    ">_Pipeline:_ \n",
    ">- Create new enhancer-centered input samples:\n",
    ">    - Inputs centered at active enhancers (H3K27ac > 10) in chr 19 (alternative test chr) for mESCs.\n",
    ">- Replace these for a silenced chromatin state with no accessibility, H3K4me3 and H3K27ac.\n",
    ">- Predict EU-seq for baseline and perturbed inputs.\n",
    ">- Measure output changes:\n",
    ">    - For each output position / bin.\n",
    ">    - Inside gene boundaries:\n",
    ">        -   Identify genes inside predicted boundaries and integrate predicted EU-seq within gene limits. \n",
    "\n",
    "DATA ANALYSIS PLOTS:\n",
    "- Distance relative effect.\n",
    "- Integrated EU-seq change (relative) vs. distance plot.\n",
    "- Enhancer width distribution.\n",
    "- Ranking of gene index of the most affected gene per enhancer.\n",
    "\n",
    "**Get enhancer-centered baseline & perturbed inputs:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile create_enhancer_centered_inputs.py\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyBigWig\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "\n",
    "def create_input_array(data_path: Path,\n",
    "                      input_shift: int,\n",
    "                      n_input_tracks: int, \n",
    "                      n_input_bins: int,\n",
    "                      bw_input_track_list: list,\n",
    "                      enhancer_id: str,\n",
    "                      center: int,\n",
    "                      chrom: str):\n",
    "    \"\"\"\n",
    "    Creates input arrays centered at enhancer midpoint\n",
    "    \"\"\"\n",
    "    input_sample = np.empty((n_input_tracks, n_input_bins))\n",
    "    for i, bw_path in enumerate(bw_input_track_list):\n",
    "        try:\n",
    "            bw = pyBigWig.open(str(data_path / bw_path), \"r\")\n",
    "            stats = bw.stats(chrom, center-input_shift, center+input_shift, type=\"mean\", nBins=n_input_bins)\n",
    "            bw.close()\n",
    "            \n",
    "            stats = np.array([float(value) if value is not None else 0. for value in stats])\n",
    "            stats = np.clip(np.array(stats), 0, None)  # ReLU\n",
    "            input_sample[i] = stats\n",
    "        except:\n",
    "            logging.info(f\"{enhancer_id} input landscape coordinates are out of bounds.\")\n",
    "    \n",
    "    return input_sample\n",
    "\n",
    "def create_perturbed_array(input_array: np.ndarray, start_idx: int, end_idx: int):\n",
    "    \"\"\"\n",
    "    Creates perturbed version with all marks except H3K27me3 silenced within enhancer boundaries\n",
    "    \"\"\"\n",
    "    perturbed = input_array.copy()\n",
    "    # Silence ATAC-seq, H3K4me3, and H3K27ac (indices 0, 1, 2) only within enhancer boundaries\n",
    "    perturbed[[0, 1, 2], start_idx:end_idx] = 0\n",
    "    return perturbed\n",
    "\n",
    "def process_enhancer(data_path, bw_input_track_list, input_shift, n_input_tracks, n_input_bins, \n",
    "                    INPUT_SHAPE, path, row_tuple, resolution=100, activity_threshold=10):\n",
    "    \"\"\"\n",
    "    Process a single enhancer and create its input arrays if enhancer is active\n",
    "    \"\"\"\n",
    "    enhancer_id, chrom, start, end, _, _ = row_tuple  # Now unpacking from tuple\n",
    "    start = int(start)\n",
    "    end = int(end)\n",
    "    center = (start + end) // 2\n",
    "\n",
    "    # Create regular input array\n",
    "    input_sample = create_input_array(data_path, input_shift, n_input_tracks, n_input_bins, \n",
    "                                    bw_input_track_list, enhancer_id, center, chrom)\n",
    "\n",
    "    if input_sample.shape == INPUT_SHAPE:\n",
    "        # Calculate enhancer boundaries in array coordinates\n",
    "        central_idx = n_input_bins // 2\n",
    "        start_idx = central_idx + (start - center) // resolution\n",
    "        end_idx = central_idx + (end - center) // resolution\n",
    "\n",
    "        # Check H3K27ac activity within enhancer boundaries\n",
    "        h3k27ac_idx = 2  # Index for H3K27ac track\n",
    "        enhancer_mean = np.mean(input_sample[h3k27ac_idx, start_idx:end_idx])\n",
    "\n",
    "        if enhancer_mean > activity_threshold:\n",
    "            # Save regular array\n",
    "            np.save(path / \"inputs\" / \"perturbed_landscape_arrays\" / \"enhancer_arrays\" / f\"{enhancer_id}.npy\", input_sample)\n",
    "            \n",
    "            # Create and save perturbed array\n",
    "            perturbed_sample = create_perturbed_array(input_sample, start_idx, end_idx)\n",
    "            np.save(path / \"inputs\" / \"perturbed_landscape_arrays\" / \"perturbed_enhancer_arrays\" / f\"{enhancer_id}_perturbed.npy\", perturbed_sample)\n",
    "            \n",
    "            return (enhancer_id, enhancer_id + \"_perturbed\")  # Return both IDs\n",
    "        else:\n",
    "            logging.info(f\"{enhancer_id}: Enhancer activity below threshold ({enhancer_mean:.2f} ≤ {activity_threshold})\")\n",
    "            return None\n",
    "    else:\n",
    "        logging.info(f\"{enhancer_id}: Input did not match the expected shape.\")\n",
    "        return None\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Setup paths\n",
    "    path = Path(\"../\")\n",
    "    data_path = path / \"GEO_files\"\n",
    "\n",
    "    # Create output directories\n",
    "    (path / \"inputs\" / \"perturbed_landscape_arrays\" / \"enhancer_arrays\").mkdir(parents=True, exist_ok=True)\n",
    "    (path / \"inputs\" / \"perturbed_landscape_arrays\" / \"perturbed_enhancer_arrays\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Start logger\n",
    "    LOG_FILENAME = \"/Enhancer_data_creation.log\"\n",
    "    logging.basicConfig(filename=str(path) + LOG_FILENAME, level=logging.INFO)\n",
    "\n",
    "    # Load enhancer annotations\n",
    "    enhancer_annotations_path = path / \"annotations\" / \"Final_Enhancer_annotation.tsv\"\n",
    "    enhancer_df = pd.read_csv(enhancer_annotations_path, sep=\"\\t\")\n",
    "    \n",
    "    # Filter for chr19 (alternative test)\n",
    "    chr4_enhancers = enhancer_df[enhancer_df['chr'] == 'chr19'] # Rather small chromosome, 8869 enhancer-like signatures including CTCF bound, 1491 pass the activity filter.\n",
    "\n",
    "    # Setup parameters\n",
    "    input_shift = 500050\n",
    "    n_input_bins = 10001\n",
    "    n_input_tracks = 4\n",
    "    INPUT_SHAPE = (4, 10001)\n",
    "    bw_input_track_list = [\"ATAC_Seq.bw\", \"H3K4me3.bw\", \"H3K27ac.bw\", \"H3K27me3.bw\"]\n",
    "\n",
    "    # Process enhancers in parallel\n",
    "    process_args = (data_path, bw_input_track_list, input_shift, n_input_tracks, n_input_bins, \n",
    "                   INPUT_SHAPE, path)\n",
    "    \n",
    "    activity_threshold = 10\n",
    "\n",
    "    # Convert DataFrame rows to tuples for multiprocessing\n",
    "    row_tuples = [tuple(row) for row in chr4_enhancers.values]\n",
    "\n",
    "    with multiprocessing.Pool(processes=multiprocessing.cpu_count() - 1) as pool:\n",
    "        results = pool.starmap(process_enhancer, \n",
    "                    [(process_args + (row_tuple, 100, activity_threshold)) for row_tuple in row_tuples])\n",
    "        \n",
    "        # Count how many enhancers were processed successfully\n",
    "        processed_count = sum(1 for result in results if result)\n",
    "        logging.info(f\"Processed {processed_count} active enhancers out of {len(chr4_enhancers)} total enhancers in chr4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create dummy targets for the enhancer-centered predictions:**\n",
    "\n",
    "For eir to run we need to provide a dummy target file, although it will not be used. Hence we create a target file with target IDs but full of zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_ids(directory):\n",
    "    \"\"\"Get all file IDs from a directory without the .npy extension\"\"\"\n",
    "    return [f.stem for f in Path(directory).glob('*.npy')]\n",
    "\n",
    "def create_unified_target_file(baseline_ids, perturbed_ids, output_path, n_bins=200):\n",
    "    \"\"\"Create a unified target file with zeros for both regular and perturbed IDs\"\"\"\n",
    "    # Create column names\n",
    "    columns = ['ID'] + [f'{i}_ctrl' for i in range(-n_bins, n_bins+1)]\n",
    "    \n",
    "    # Combine all IDs\n",
    "    all_ids = baseline_ids + perturbed_ids\n",
    "    \n",
    "    # Create DataFrame with zeros\n",
    "    df = pd.DataFrame(0, index=range(len(all_ids)), columns=columns)\n",
    "    \n",
    "    # Fill ID column\n",
    "    df['ID'] = all_ids\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Created {output_path} with {len(all_ids)} entries\")\n",
    "    print(f\"- Regular samples: {len(baseline_ids)}\")\n",
    "    print(f\"- Perturbed samples: {len(perturbed_ids)}\")\n",
    "\n",
    "# Setup paths\n",
    "base_path = Path(\"../\")\n",
    "baseline_dir = base_path / \"inputs\" / \"perturbed_landscape_arrays\" / \"enhancer_arrays\"\n",
    "perturbed_dir = base_path / \"inputs\" / \"perturbed_landscape_arrays\" / \"perturbed_enhancer_arrays\"\n",
    "output_dir = base_path / \"targets\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Get IDs from both directories\n",
    "baseline_ids = get_file_ids(baseline_dir)\n",
    "perturbed_ids = get_file_ids(perturbed_dir)\n",
    "\n",
    "# Create unified target file\n",
    "create_unified_target_file(baseline_ids, perturbed_ids,\n",
    "                            output_dir / \"Enhancer_centered_targets.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move perturbed inputs to the same folder to predict all at once.\n",
    "! mv ../inputs/perturbed_landscape_arrays/perturbed_enhancer_arrays/* ../inputs/perturbed_landscape_arrays/enhancer_arrays/\n",
    "! rm -r ../inputs/perturbed_landscape_arrays/perturbed_enhancer_arrays/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict enhancer-centered samples: baseline and perturbed**\n",
    "\n",
    "```bash\n",
    "eirpredict \\\n",
    "--global_configs ./configurations/conf_pure_conv_chr19_enhancer_centric/globals.yaml \\\n",
    "--input_configs ./configurations/conf_pure_conv_chr19_enhancer_centric/input_cnn.yaml \\\n",
    "--fusion_configs ./configurations/conf_pure_conv_chr19_enhancer_centric/fusion.yaml \\\n",
    "--output_configs ./configurations/conf_pure_conv_chr19_enhancer_centric/outputs_2_cond.yaml \\\n",
    "--evaluate \\\n",
    "--model_path ./runs/gene_expression_only_chrom_pure_conv_chr19_holdout/saved_models/gene_expression_only_chrom_pure_conv_chr19_holdout_model_30300_perf-average=0.6487.pt \\\n",
    "--output_folder ./runs/perturbation_runs/gene_expression_only_chrom_pure_conv_chr19_enhancer_centric\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quantify differences between baseline and perturbed predictions and store them in a table:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from scipy.integrate import simps\n",
    "\n",
    "def load_annotations(enhancer_path, gene_path):\n",
    "    \"\"\"Load and return enhancer and gene annotations.\"\"\"\n",
    "    enhancers_df = pd.read_csv(enhancer_path, sep='\\t')\n",
    "    genes_df = pd.read_csv(gene_path, sep='\\t')\n",
    "    return enhancers_df, genes_df\n",
    "\n",
    "def get_window_boundaries(enhancer_center, window_size=200500):\n",
    "    \"\"\"Calculate window boundaries given enhancer center.\"\"\"\n",
    "    return enhancer_center - window_size, enhancer_center + window_size\n",
    "\n",
    "def find_genes_in_window(genes_df, chrom, window_start, window_end):\n",
    "    \"\"\"Find genes fully contained within the window.\"\"\"\n",
    "    return genes_df[\n",
    "        (genes_df['chr'] == chrom) & \n",
    "        (genes_df['Start'] >= window_start) & \n",
    "        (genes_df['End'] <= window_end)\n",
    "    ].copy()\n",
    "\n",
    "def calculate_gene_area(predictions, gene_start_bin, gene_end_bin):\n",
    "    \"\"\"Calculate area under the curve for a gene using Simpson's rule.\"\"\"\n",
    "    gene_values = predictions[gene_start_bin:gene_end_bin + 1]\n",
    "    x = np.arange(len(gene_values))\n",
    "    area = simps(gene_values, x)\n",
    "    length = gene_end_bin - gene_start_bin + 1\n",
    "    return area / length if length > 0 else 0\n",
    "\n",
    "def get_gene_tss(gene_row):\n",
    "    \"\"\"Get TSS position based on strand.\"\"\"\n",
    "    return gene_row['Start'] if gene_row['Strand'] == '+' else gene_row['End']\n",
    "\n",
    "def calculate_distance_and_order(window_genes, enhancer_center):\n",
    "    \"\"\"Calculate distances to enhancer and determine gene order.\"\"\"\n",
    "    # Calculate TSS for each gene and distance to enhancer\n",
    "    distances = []\n",
    "    for _, gene in window_genes.iterrows():\n",
    "        tss = get_gene_tss(gene)\n",
    "        distance = abs(enhancer_center - tss)\n",
    "        distances.append({\n",
    "            'gene_id': gene['ID'],\n",
    "            'distance': distance,\n",
    "            'tss': tss\n",
    "        })\n",
    "    \n",
    "    # Sort genes by distance\n",
    "    sorted_distances = sorted(distances, key=lambda x: x['distance'])\n",
    "    \n",
    "    # Create distance mapping\n",
    "    distance_map = {d['gene_id']: {\n",
    "        'distance': d['distance'],\n",
    "        'order': idx + 1,\n",
    "        'tss': d['tss']\n",
    "    } for idx, d in enumerate(sorted_distances)}\n",
    "    \n",
    "    return distance_map\n",
    "\n",
    "def check_gene_adjacency(window_genes, distance_map, gene_id):\n",
    "    \"\"\"Check if a gene is adjacent to the enhancer (no genes between them).\"\"\"\n",
    "    gene_tss = distance_map[gene_id]['tss']\n",
    "    gene_distance = distance_map[gene_id]['distance']\n",
    "    \n",
    "    # Get the gene's info\n",
    "    gene_info = window_genes[window_genes['ID'] == gene_id].iloc[0]\n",
    "    \n",
    "    # Check for any genes between this gene and the enhancer\n",
    "    for _, other_gene in window_genes.iterrows():\n",
    "        if other_gene['ID'] == gene_id:\n",
    "            continue\n",
    "            \n",
    "        other_tss = get_gene_tss(other_gene)\n",
    "        other_distance = abs(other_tss - gene_tss)\n",
    "        \n",
    "        # If there's a gene with smaller distance, this gene is not adjacent\n",
    "        if other_distance < gene_distance:\n",
    "            return False\n",
    "            \n",
    "    return True\n",
    "\n",
    "def process_enhancer_predictions(enhancer_row, genes_df, predictions_df, window_size=200500, resolution=1000):\n",
    "    \"\"\"Process predictions for one enhancer and its genes.\"\"\"\n",
    "    enhancer_id = enhancer_row['cCRE_accession']\n",
    "    chrom = enhancer_row['chr']\n",
    "    \n",
    "    if enhancer_id not in predictions_df.index:\n",
    "        print(f\"Warning: {enhancer_id} not found in predictions\")\n",
    "        return []\n",
    "    \n",
    "    enhancer_center = (int(enhancer_row['start']) + int(enhancer_row['end'])) // 2\n",
    "    window_start, window_end = get_window_boundaries(enhancer_center, window_size)\n",
    "    \n",
    "    window_genes = find_genes_in_window(genes_df, chrom, window_start, window_end)\n",
    "    \n",
    "    # Calculate distances and gene order\n",
    "    distance_map = calculate_distance_and_order(window_genes, enhancer_center)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    try:\n",
    "        baseline_preds = predictions_df.loc[enhancer_id]\n",
    "        perturbed_preds = predictions_df.loc[f\"{enhancer_id}_perturbed\"]\n",
    "        \n",
    "        for _, gene in window_genes.iterrows():\n",
    "            gene_start_rel = gene['Start'] - enhancer_center\n",
    "            gene_end_rel = gene['End'] - enhancer_center\n",
    "            \n",
    "            gene_start_bin = (gene_start_rel + window_size) // resolution\n",
    "            gene_end_bin = (gene_end_rel + window_size) // resolution\n",
    "            \n",
    "            baseline_values = baseline_preds.values.astype(float)\n",
    "            perturbed_values = perturbed_preds.values.astype(float)\n",
    "            \n",
    "            baseline_area = calculate_gene_area(baseline_values, gene_start_bin, gene_end_bin)\n",
    "            perturbed_area = calculate_gene_area(perturbed_values, gene_start_bin, gene_end_bin)\n",
    "            \n",
    "            # Check if gene is adjacent\n",
    "            is_adjacent = check_gene_adjacency(window_genes, distance_map, gene['ID'])\n",
    "            \n",
    "            results.append({\n",
    "                'ID': f\"{enhancer_id}_{gene['ID']}\",\n",
    "                'baseline_area': baseline_area,\n",
    "                'perturbed_area': perturbed_area,\n",
    "                'gene_id': gene['ID'],\n",
    "                'gene_name': gene['Name'],\n",
    "                'gene_length': gene_end_bin - gene_start_bin + 1,\n",
    "                'distance_to_enhancer': distance_map[gene['ID']]['distance'],\n",
    "                'gene_order': distance_map[gene['ID']]['order'],\n",
    "                'is_adjacent': is_adjacent\n",
    "            })\n",
    "    except KeyError as e:\n",
    "        print(f\"Error processing enhancer {enhancer_id}: {str(e)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error processing enhancer {enhancer_id}: {str(e)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "########### Main script ############################\n",
    "base_path = Path(\"../\")\n",
    "enhancer_path = base_path / \"annotations\" / \"Final_Enhancer_annotation.tsv\"\n",
    "gene_path = base_path / \"annotations\" / \"Final_gene_annotations.tsv\"\n",
    "results_path = base_path / \"runs\" / \"perturbation_runs\" / \"gene_expression_only_chrom_pure_conv_chr19_enhancer_centric\" \n",
    "output_path = base_path / \"tables\" \n",
    "output_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "enhancers_df, genes_df = load_annotations(enhancer_path, gene_path)\n",
    "_, predictions_df, _ = _get_predictions(results_path,\n",
    "                                        N_BINS=200,\n",
    "                                        condition_list=[\"_ctrl\"],\n",
    "                                        is_training=False) \n",
    "\n",
    "print(\"Number of enhancers in annotation:\", len(enhancers_df))\n",
    "print(\"Number of predictions:\", len(predictions_df))\n",
    "print(\"Sample of prediction indices:\", list(predictions_df.index)[:5])\n",
    "print(\"Sample of enhancer IDs:\", list(enhancers_df['cCRE_accession'])[:5])\n",
    "\n",
    "all_results = []\n",
    "for _, enhancer in enhancers_df.iterrows():\n",
    "    results = process_enhancer_predictions(enhancer, genes_df, predictions_df)\n",
    "    if results:\n",
    "        all_results.extend(results)\n",
    "\n",
    "if all_results:\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    results_df.to_csv(output_path / \"Predicted_gene_areas_in_enhancer_centric_approach.csv\", index=False)\n",
    "    \n",
    "    print(f\"\\nProcessing complete:\")\n",
    "    print(f\"Processed {len(enhancers_df)} enhancers\")\n",
    "    print(f\"Found {len(results_df)} enhancer-gene pairs\")\n",
    "    print(f\"Results saved to {output_path}\")\n",
    "else:\n",
    "    print(\"\\nNo results were generated. Please check the error messages above.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize enhancer results:**\n",
    "\n",
    "- Distance dependent distribution of absolute differences between perturbed and baseline.\n",
    "- Gene area absolute differences as a function of distance.\n",
    "- Enhancer width distribution.\n",
    "- Histogram of most affected gene per enhancer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Set global font size\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "def analyze_enhancer_perturbations():\n",
    "    # Create save directory\n",
    "    savepath = Path(\"../figures/Figures_revisions/Enhancer_perturbations/\")\n",
    "    savepath.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    # Read enhancer annotations\n",
    "    enhancer_df = pd.read_csv(\"../annotations/Final_Enhancer_annotation.tsv\", sep='\\t')\n",
    "    enhancer_df['width'] = enhancer_df['end'] - enhancer_df['start']\n",
    "\n",
    "    # Read the predictions\n",
    "    ids, df, _ = _get_predictions(\n",
    "        results_path=Path(\"../runs/perturbation_runs/gene_expression_only_chrom_pure_conv_chr19_enhancer_centric/\"), \n",
    "        N_BINS=200,\n",
    "        condition_list=[\"_ctrl\"],\n",
    "        is_training=False\n",
    "    )\n",
    "\n",
    "    # Calculate absolute differences and get enhancer widths\n",
    "    abs_diff_data = []\n",
    "    enhancer_widths = []\n",
    "\n",
    "    for enhancer_id in df[~df.index.str.contains('perturbed')].index:\n",
    "        # Get regular and perturbed predictions\n",
    "        regular = df[df.index == enhancer_id].iloc[0]\n",
    "        perturbed = df[df.index == f\"{enhancer_id}_perturbed\"].iloc[0]\n",
    "        \n",
    "        # Get enhancer width\n",
    "        base_id = enhancer_id.split('_forward')[0] if '_forward' in enhancer_id else enhancer_id\n",
    "        width = enhancer_df[enhancer_df['cCRE_accession'] == base_id]['width'].iloc[0]\n",
    "        enhancer_widths.append(width)\n",
    "        \n",
    "        # Calculate differences for each position\n",
    "        for pos in df.columns:\n",
    "            position = int(pos.replace('_ctrl', ''))\n",
    "            abs_diff = abs(regular[pos] - perturbed[pos])\n",
    "            abs_diff_data.append({\n",
    "                'Position': position,\n",
    "                'Absolute_Difference': abs_diff\n",
    "            })\n",
    "\n",
    "    abs_diff_df = pd.DataFrame(abs_diff_data)\n",
    "\n",
    "    # Calculate statistics for each position\n",
    "    stats_df = abs_diff_df.groupby('Position').agg({\n",
    "        'Absolute_Difference': ['mean', 'std', 'min', 'max', \n",
    "                               lambda x: np.percentile(x, 25),\n",
    "                               lambda x: np.percentile(x, 75)]\n",
    "    }).reset_index()\n",
    "    stats_df.columns = ['Position', 'mean', 'std', 'min', 'max', 'q1', 'q3']\n",
    "\n",
    "    return stats_df, enhancer_widths\n",
    "\n",
    "def analyze_gene_areas():\n",
    "    # Read the data\n",
    "    df = pd.read_csv(Path('../tables/Predicted_gene_areas_in_enhancer_centric_approach.csv'))\n",
    "\n",
    "    # Calculate area difference\n",
    "    df['area_difference'] = abs(df['perturbed_area'] - df['baseline_area'])\n",
    "\n",
    "    # Calculate rolling statistics\n",
    "    sorted_df = df.sort_values('distance_to_enhancer').copy()\n",
    "    window_size = 5000  # 5kbp\n",
    "\n",
    "    # Initialize lists for rolling statistics\n",
    "    x_vals = []\n",
    "    means = []\n",
    "    stds = []\n",
    "\n",
    "    # Calculate rolling statistics manually\n",
    "    for center in sorted_df['distance_to_enhancer'].unique():\n",
    "        lower_bound = center - window_size/2\n",
    "        upper_bound = center + window_size/2\n",
    "        \n",
    "        window_mask = (sorted_df['distance_to_enhancer'] >= lower_bound) & \\\n",
    "                     (sorted_df['distance_to_enhancer'] <= upper_bound)\n",
    "        window_values = sorted_df.loc[window_mask, 'area_difference']\n",
    "        \n",
    "        if len(window_values) > 0:\n",
    "            x_vals.append(center/1000)  # Convert to kbp\n",
    "            means.append(window_values.mean())\n",
    "            stds.append(window_values.std())\n",
    "\n",
    "    # Get maximum effect genes\n",
    "    def get_enhancer_id(row_id):\n",
    "        return row_id.split('_ENSMUSG')[0]\n",
    "\n",
    "    df['enhancer_id'] = df['ID'].apply(get_enhancer_id)\n",
    "    df['abs_area_difference'] = abs(df['area_difference'])\n",
    "    max_diff_genes = df.loc[df.groupby('enhancer_id')['abs_area_difference'].idxmax()]\n",
    "\n",
    "    return df, np.array(x_vals), np.array(means), np.array(stds), max_diff_genes\n",
    "\n",
    "def create_combined_figure():\n",
    "    # Get data from both analyses\n",
    "    stats_df, enhancer_widths = analyze_enhancer_perturbations()\n",
    "    df, x_vals, means, stds, max_diff_genes = analyze_gene_areas()\n",
    "\n",
    "    # Create figure with custom layout\n",
    "    fig = plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Create GridSpec with custom ratios\n",
    "    # First row is for the absolute differences plot (full width)\n",
    "    # Second row has three plots with ratios 1:2:3\n",
    "    gs = fig.add_gridspec(2, 6, height_ratios=[1, 1])\n",
    "    \n",
    "    # Top row: Absolute differences plot (spans all columns)\n",
    "    ax1 = fig.add_subplot(gs[0, :])\n",
    "    ax1.plot(stats_df['Position'], stats_df['mean'], \n",
    "             color='red', label='Mean', linewidth=2)\n",
    "    ax1.fill_between(stats_df['Position'], \n",
    "                     stats_df['mean'] - stats_df['std'],\n",
    "                     stats_df['mean'] + stats_df['std'],\n",
    "                     alpha=0.2, color='red', label='Standard deviation')\n",
    "    ax1.plot(stats_df['Position'], stats_df['min'], \n",
    "             '--', color='gray', alpha=0.7, label='Max')\n",
    "    ax1.plot(stats_df['Position'], stats_df['max'], \n",
    "             '--', color='gray', alpha=0.7)\n",
    "    ax1.set_xlabel('Distance to the center of the silenced enhancer (kbp)', fontsize=14)\n",
    "    ax1.set_ylabel('Absolute Difference', fontsize=14)\n",
    "    ax1.set_xlim((-200, 200))\n",
    "    ax1.set_ylim((0, 55))\n",
    "    ax1.legend()\n",
    "\n",
    "    # Bottom row, first plot (1/6 width): Enhancer width distribution\n",
    "    ax2 = fig.add_subplot(gs[1, 0])\n",
    "    sns.boxplot(y=enhancer_widths, ax=ax2, \n",
    "                showfliers=False, width=0.5, color='lightgray')\n",
    "    ax2.set_ylabel('Enhancer Width (bp)', fontsize=14)\n",
    "    ax2.set_xlabel('')\n",
    "\n",
    "    # Bottom row, third plot (1/3 width): Gene Order Histogram (limited to order 10)\n",
    "    ax3 = fig.add_subplot(gs[1, 4:])\n",
    "    # Filter data for orders 1-10\n",
    "    max_diff_genes_filtered = max_diff_genes[max_diff_genes['gene_order'] <= 10]\n",
    "    sns.histplot(data=max_diff_genes_filtered, x='gene_order', discrete=True, \n",
    "                 ax=ax3, color='black', edgecolor='white')\n",
    "    \n",
    "    def ordinal(n):\n",
    "        return str(n) + (\"th\" if 4<=n<=20 else {1:\"st\",2:\"nd\",3:\"rd\"}.get(n%10, \"th\"))\n",
    "    \n",
    "    ax3.set_xticks(range(1, 11))\n",
    "    ax3.set_xticklabels([ordinal(i) for i in range(1, 11)], rotation=90)\n",
    "    ax3.set_xlabel('Gene Order')\n",
    "    ax3.set_ylabel('Count')\n",
    "\n",
    "    # Bottom row, second plot (1/2 width): Distance vs Area Difference\n",
    "    ax4 = fig.add_subplot(gs[1, 1:4])\n",
    "    ax4.scatter(df['distance_to_enhancer']/1000, df['area_difference'], \n",
    "                alpha=1, color='k', s=1, edgecolors=None, marker='.', \n",
    "                label='Individual genes')\n",
    "    ax4.plot(x_vals, means, color='red', \n",
    "             label='Rolling Mean (5kbp window)', linewidth=2)\n",
    "    ax4.fill_between(x_vals, means - stds, means + stds,\n",
    "                     alpha=.2, color='red', label='Rolling STD (5kbp window)')\n",
    "    ax4.set_xlabel('Distance between perturbed enhancer center and TSS (kbp)')\n",
    "    ax4.set_ylabel('Normalized gene area difference')\n",
    "    ax4.set_ylim(0, 20)\n",
    "    ax4.set_xlim(0, 200)\n",
    "    max_dist = int(np.ceil(df['distance_to_enhancer'].max()/1000/10)*10)\n",
    "    ax4.set_xticks(np.arange(0, max_dist, 10))\n",
    "    ax4.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    savepath = Path(\"../figures/Figures_revisions/Enhancer_perturbations/\")\n",
    "    fig.savefig(savepath / \"Combined_enhancer_analysis.png\", dpi=200, bbox_inches='tight')\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nEnhancer Width Statistics:\")\n",
    "    print(f\"Mean: {np.mean(enhancer_widths):.0f} bp\")\n",
    "    print(f\"Median: {np.median(enhancer_widths):.0f} bp\")\n",
    "    print(f\"Std: {np.std(enhancer_widths):.0f} bp\")\n",
    "    print(f\"Min: {np.min(enhancer_widths):.0f} bp\")\n",
    "    print(f\"Max: {np.max(enhancer_widths):.0f} bp\")\n",
    "    \n",
    "    print(\"\\nStatistics for genes with maximum absolute difference (up to order 10):\")\n",
    "    print(f\"Total number of enhancers: {len(max_diff_genes_filtered)}\")\n",
    "    print(\"\\nGene Order distribution:\")\n",
    "    print(max_diff_genes_filtered['gene_order'].value_counts().sort_index())\n",
    "\n",
    "def create_distance_histogram(results_df):\n",
    "    \"\"\"\n",
    "    Create histogram data for gene-enhancer distances.\n",
    "    \n",
    "    Args:\n",
    "        results_df: DataFrame containing the enhancer-gene analysis results\n",
    "    \"\"\"\n",
    "    # Calculate bin edges (every 20kb up to 200kb)\n",
    "    bin_size = 20000  # 20kb bins\n",
    "    max_distance = 200000  # 200kb max\n",
    "    bins = np.arange(0, max_distance + bin_size, bin_size)\n",
    "    \n",
    "    # Create the histogram\n",
    "    hist, bin_edges = np.histogram(results_df['distance_to_enhancer'], bins=bins)\n",
    "    \n",
    "    # Create bin labels for plotting\n",
    "    bin_labels = [f\"{int(bin_edges[i]/1000)}-{int(bin_edges[i+1]/1000)}kb\" \n",
    "                 for i in range(len(bin_edges)-1)]\n",
    "    \n",
    "    # Plot with matplotlib\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(bin_labels, hist, align='center')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.xlabel('Distance from Enhancer (kb)')\n",
    "    plt.ylabel('Number of Genes')\n",
    "    plt.title('Distribution of Gene-Enhancer Distances')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(output_path / \"gene_enhancer_distance_histogram.png\")\n",
    "    plt.close()\n",
    "\n",
    "def create_distance_histogram():\n",
    "    \"\"\"Create and save a histogram showing the distribution of gene-enhancer distances.\"\"\"\n",
    "    # Read the data\n",
    "    df = pd.read_csv(Path('../tables/Predicted_gene_areas_in_enhancer_centric_approach.csv'))\n",
    "    \n",
    "    # Create save directory\n",
    "    savepath = Path(\"../figures/Figures_revisions/Enhancer_perturbations/\")\n",
    "    savepath.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Create the figure\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Calculate bin edges (every 20kb up to 200kb)\n",
    "    bin_size = 20000  # 20kb bins\n",
    "    max_distance = 200000  # 200kb max\n",
    "    bins = np.arange(0, max_distance + bin_size, bin_size)\n",
    "    \n",
    "    # Create the histogram\n",
    "    plt.hist(df['distance_to_enhancer'], bins=bins, color='black', edgecolor='white')\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.xlabel('Distance between perturbed enhancer center and TSS (kbp)', fontsize=14)\n",
    "    plt.ylabel('Number of Genes', fontsize=14)\n",
    "    \n",
    "    # Convert x-axis ticks to kbp\n",
    "    plt.xticks(bins[::2], [f'{int(x/1000)}' for x in bins[::2]], rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(savepath / \"Gene_enhancer_distance_histogram.png\", dpi=200, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nGene-Enhancer Distance Statistics:\")\n",
    "    print(f\"Mean distance: {df['distance_to_enhancer'].mean()/1000:.1f} kbp\")\n",
    "    print(f\"Median distance: {df['distance_to_enhancer'].median()/1000:.1f} kbp\")\n",
    "    print(f\"Std distance: {df['distance_to_enhancer'].std()/1000:.1f} kbp\")\n",
    "    print(f\"Number of genes: {len(df)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_combined_figure()\n",
    "    create_distance_histogram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV) New models and analyses <center>\n",
    "The reviewers asked us to perform a number of tests regarding alternative models and data processing strategies:\n",
    "\n",
    ">**A) Assessing how the model learns to predict transcription**\n",
    ">\n",
    ">**B) Assessing the impact of shortening the context window**\n",
    ">\n",
    ">**C) Changing the last activation function or the loss**\n",
    ">\n",
    ">**D) Testing on another chromosome (chr19)**\n",
    ">\n",
    ">**E) Training without the enhancer mark (H3K27ac) as an input.**\n",
    ">\n",
    ">**F) Assessing the impact of smoothing**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> A) Assessing how the model learns to predict transcription<center>\n",
    "\n",
    "Here we extend the perturbational framework to further explore what rules does CLASTER learn. To do that, we create a number of synthetic inputs modifying a bit the properties of the input. The reference sample is centered at the gene Akirin2 (chr4) which showed a clear directionality and had no genes to the left. We will then feed these slightly different inputs to an already pretrained model, the basic model with only the chromatin branch and pure convolutions (no attention), obtain the predictions and compare them.\n",
    "\n",
    "The following perturbations will be performed:\n",
    "- Modulate promoter activity (height).\n",
    "- Modulate the extension of H3K4me3 and ATAC-seq signal towards both directions to determine direction of transcription. We will do that by flipping the promoter region.\n",
    "- Plug-in active superenhancer close to the promoter.\n",
    "- Plug-in isolated promoter in intergenic region.\n",
    "\n",
    "**Modifying promoter height:**\n",
    "\n",
    "In the region -100 to + 100 bins (-10kbp,10kbp) centered at the TSS of the gene Akirin2 (ENSMUSG00000028291.7) we will:\n",
    "- Get the chromatin state in this region and paste it in the intergenic region to the left.\n",
    "- Increase the signal (multiply by 2 all marks)\n",
    "- Decrease the signal (divide by 2 all marks)\n",
    "\n",
    "**Identifying signal drivers of transcriptional directionality:**\n",
    "- Check effects of accessibility / H3K27ac extension towards gene body. We will flip the promoter-proximal region.\n",
    "\n",
    "**Adding superenhancers:**\n",
    "\n",
    "Superenhancers for mESCs in mm9 can be found [here](https://asntech.org/dbsuper/adv_search.php?genome=mm9&cell_type%5B%5D=C_110&cell_type%5B%5D=C_111&cell_type%5B%5D=C_112&cell_type%5B%5D=C_113&cell_type%5B%5D=C_114&cell_type%5B%5D=C_115&cell_type%5B%5D=C_116&cell_type%5B%5D=C_087&cell_type%5B%5D=C_106&cell_type%5B%5D=C_117&cell_type%5B%5D=C_118&cell_type%5B%5D=C_119&cell_type%5B%5D=C_120&cell_type%5B%5D=C_121&cell_type%5B%5D=C_092&cell_type%5B%5D=C_122&cell_type%5B%5D=C_090&cell_type%5B%5D=C_123&cell_type%5B%5D=C_124&cell_type%5B%5D=C_089&cell_type%5B%5D=C_125&cell_type%5B%5D=C_126&cell_type%5B%5D=C_091&cell_type%5B%5D=C_127&cell_type%5B%5D=C_107&gene=&locus=&method=&submit=1)\n",
    "\n",
    "- mESCs have an active superenhancer region directly to the left of KLF4. We get that region and plug it in immediately to the left of Akirin2 (with x0.5 promoter) to observe the boost.\n",
    "\n",
    "**Adding a new gene in an intergenic region:**\n",
    "\n",
    "We get the promoter of Akirin2 and plug it again upstream.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = Path(\"../inputs/landscape_arrays/test/\")\n",
    "outputs_path = Path(\"../runs/test_runs/gene_expression_only_chrom_pure_conv/\")\n",
    "figures_path = Path(\"../figures/Figures_revisions/\")\n",
    "\n",
    "# Interesting locus: ENSMUSG00000028291.7_forward_prediction    ENSMUSG00000035696.15_forward.npy: Rnf38\n",
    "\n",
    "EXAMPLE_GENE = \"ENSMUSG00000003032.8_forward.npy\" #<- KLF4 # Akirin2: \"ENSMUSG00000028291.7_forward.npy\"\n",
    "# Inputs:\n",
    "i = 1\n",
    "input_id_list = []\n",
    "for a in os.scandir(input_path):\n",
    "    #if i % 100 == 0:\n",
    "    if a.name == EXAMPLE_GENE:\n",
    "        name = a.name.removesuffix('.npy')\n",
    "        input_id_list.append(name)\n",
    "        a = np.load(input_path / a.name)\n",
    "        fig = visualize_input_array(a)\n",
    "        fig.savefig(figures_path / 'Inputs' / f'{name}_input.png', dpi=200)\n",
    "        plt.close(fig)\n",
    "    i +=1\n",
    "\n",
    "# Outputs:\n",
    "ids, predicted, actual = _get_predictions(outputs_path, N_BINS=200, condition_list = [\"_ctrl\"])\n",
    "\n",
    "for ID,line_p,line_a in zip(ids,predicted.values,actual.values):   \n",
    "    if ID in input_id_list:\n",
    "        line_p = predicted.loc[ID]\n",
    "        line_a = actual.loc[ID]\n",
    "        fig = plot_target_predictions(line_p, line_a)\n",
    "        fig.savefig(figures_path / 'Outputs' / f\"{ID}_prediction.png\", dpi=200)\n",
    "        plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = Path(\"../inputs/landscape_arrays/test/\")\n",
    "savepath = Path(\"../inputs/perturbed_landscape_arrays/inserted_promoter_arrays/\")\n",
    "figures_path = Path(\"../figures/Figures_revisions/\")\n",
    "\n",
    "akirin2_baseline_input = np.load(input_path / \"ENSMUSG00000028291.7_forward.npy\")\n",
    "SEQLEN = akirin2_baseline_input.shape[1]\n",
    "N_BINS_SHIFT = 100 \n",
    "\n",
    "akirin2_promoter = akirin2_baseline_input[:,(SEQLEN // 2) -N_BINS_SHIFT:(SEQLEN // 2) +N_BINS_SHIFT ]\n",
    "\n",
    "sample_names = [\"ENSMUSG00000028291.7_forward\", \n",
    "                \"ENSMUSG00000028291.7_forward_0.5prom\",\n",
    "                \"ENSMUSG00000028291.7_forward_2prom\",\n",
    "                \"ENSMUSG00000028291.7_forward_flipprom\",\n",
    "                \"ENSMUSG00000028291.7_forward_proxenh\" ,\n",
    "                \"ENSMUSG00000028291.7_forward_proxprom\"]\n",
    "\n",
    "# Baseline sequence:\n",
    "fig = visualize_input_array(akirin2_baseline_input)\n",
    "fig.savefig(figures_path / 'Inputs' / f'{sample_names[0]}.png', dpi=200)\n",
    "plt.close(fig)\n",
    "np.save(savepath / f\"{sample_names[0]}.npy\", akirin2_baseline_input)\n",
    "\n",
    "# A) 2X less active promoter\n",
    "akirin2_less_prom = akirin2_baseline_input.copy()\n",
    "akirin2_less_prom[:,(SEQLEN // 2) -N_BINS_SHIFT:(SEQLEN // 2) +N_BINS_SHIFT ] = 0.5*akirin2_less_prom[:,(SEQLEN // 2) -N_BINS_SHIFT:(SEQLEN // 2) +N_BINS_SHIFT ]\n",
    "fig = visualize_input_array(akirin2_less_prom)\n",
    "fig.savefig(figures_path / 'Inputs' / f'{sample_names[1]}.png', dpi=200)\n",
    "plt.close(fig)\n",
    "np.save(savepath / f\"{sample_names[1]}.npy\",akirin2_less_prom)\n",
    "\n",
    "# B) 2X more active promoter\n",
    "akirin2_more_prom = akirin2_baseline_input.copy()\n",
    "akirin2_more_prom[:,(SEQLEN // 2) -N_BINS_SHIFT:(SEQLEN // 2) +N_BINS_SHIFT ] = 2*akirin2_more_prom[:,(SEQLEN // 2) -N_BINS_SHIFT:(SEQLEN // 2) +N_BINS_SHIFT ]\n",
    "fig = visualize_input_array(akirin2_more_prom)\n",
    "fig.savefig(figures_path / 'Inputs' / f'{sample_names[2]}.png', dpi=200)\n",
    "plt.close(fig)\n",
    "np.save(savepath / f\"{sample_names[2]}.npy\", akirin2_more_prom)\n",
    "\n",
    "# C) Flipped promoter extension:\n",
    "# B) 2X less active promoter\n",
    "akirin2_flip_prom = akirin2_baseline_input.copy()\n",
    "akirin2_flip_prom[:,(SEQLEN // 2) -N_BINS_SHIFT:(SEQLEN // 2) +N_BINS_SHIFT ] = np.flip(akirin2_flip_prom[:,(SEQLEN // 2) -N_BINS_SHIFT:(SEQLEN // 2) +N_BINS_SHIFT ], axis=1)\n",
    "fig = visualize_input_array(akirin2_flip_prom)\n",
    "fig.savefig(figures_path / 'Inputs' / f'{sample_names[3]}.png', dpi=200)\n",
    "plt.close(fig)\n",
    "np.save(savepath / f\"{sample_names[3]}.npy\", akirin2_flip_prom)\n",
    "\n",
    "\n",
    "# D) KLF4 SUPERENHANCER ADDED\n",
    "akirin2_prox_enh = akirin2_baseline_input.copy()\n",
    "# - Lower promoter activity (it is super active, genes with strong promoters don't rely on enhancers that much, see Narita et al.)\n",
    "akirin2_prox_enh[:,(SEQLEN // 2) -N_BINS_SHIFT:(SEQLEN // 2) +N_BINS_SHIFT ] = 0.5*akirin2_prox_enh[:,(SEQLEN // 2) -N_BINS_SHIFT:(SEQLEN // 2) +N_BINS_SHIFT ] # small promoter effect\n",
    "# - Copy paste KLF4 superenhancer state.\n",
    "ENH_WIDTH = 400\n",
    "ENH_START = ENH_WIDTH +  ENH_WIDTH // 2 #Enhancer starts at -60 kbp kbp from TSS\n",
    "ENH_TRUE_POS = -800\n",
    "klf4_baseline_input = np.load(input_path / \"ENSMUSG00000003032.8_forward.npy\")\n",
    "klf4_superenhancer = klf4_baseline_input[:,(SEQLEN // 2) + ENH_TRUE_POS : (SEQLEN // 2) + ENH_TRUE_POS + ENH_WIDTH]\n",
    "akirin2_prox_enh[:,(SEQLEN // 2) - ENH_START : (SEQLEN // 2) - ENH_START + ENH_WIDTH] = klf4_superenhancer\n",
    "fig = visualize_input_array(akirin2_prox_enh)\n",
    "fig.savefig(figures_path / 'Inputs' / f'{sample_names[4]}.png', dpi=200)\n",
    "plt.close(fig)\n",
    "np.save(savepath / f\"{sample_names[4]}.npy\", akirin2_prox_enh)\n",
    "\n",
    "# E) Akirin2 Promoter added:\n",
    "PROM_POS = -1000 # bins to the left where we plug in the artificial promoter\n",
    "akirin2_add_prom = akirin2_baseline_input.copy()\n",
    "akirin2_add_prom[:,(SEQLEN // 2) -N_BINS_SHIFT + PROM_POS :(SEQLEN // 2) +N_BINS_SHIFT + PROM_POS] = akirin2_add_prom[:,(SEQLEN // 2) -N_BINS_SHIFT:(SEQLEN // 2) +N_BINS_SHIFT ]\n",
    "fig = visualize_input_array(akirin2_add_prom)\n",
    "fig.savefig(figures_path / 'Inputs' / f'{sample_names[5]}.png', dpi=200)\n",
    "plt.close(fig)\n",
    "np.save(savepath / f\"{sample_names[5]}.npy\", akirin2_add_prom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy sythetic targets (for eir to work we need also targets matching inputs, which will be ignored)\n",
    "targets_path = Path(\"../targets/\")\n",
    "N_BINS = 200\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(0, \n",
    "                 index=pd.Index(sample_names, name='ID'),  # Set index with name 'ID'\n",
    "                 columns=[f'{i}_ctrl' for i in range(-N_BINS, N_BINS+1)])  # Set columns\n",
    "df.to_csv(targets_path / \"prom_perturbed_targets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running new models:**\n",
    "\n",
    "> _Note:_\n",
    "For GPU trained models, we need to test using the same device.\n",
    "\n",
    "To run on a SLURM based cluster:\n",
    "\n",
    "srun --partition=gpuqueue --gres=gpu:a100:1 -- \n",
    "\n",
    "**Predict promoter perturbed inputs:**\n",
    "\n",
    "```bash\n",
    "eirpredict \\\n",
    "--global_configs ./configurations/conf_pure_conv_predict_prom/globals.yaml \\\n",
    "--input_configs ./configurations/conf_pure_conv_predict_prom/input_cnn.yaml \\\n",
    "--fusion_configs ./configurations/conf_pure_conv_predict_prom/fusion.yaml \\\n",
    "--output_configs ./configurations/conf_pure_conv_predict_prom/outputs_2_cond.yaml \\\n",
    "--evaluate \\\n",
    "--model_path ./runs/gene_expression_only_chrom_pure_conv/saved_models/gene_expression_only_chrom_pure_conv_model_60600_perf-average=0.8161.pt \\\n",
    "--output_folder ./runs/perturbation_runs/gene_expression_only_chrom_pure_conv_prom_perturb \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plotting predicted profiles after the perturbations:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputs:\n",
    "original_outputs_path = Path(\"../runs/test_runs/gene_expression_only_chrom_pure_conv/\")\n",
    "outputs_path = Path(\"../runs/perturbation_runs/gene_expression_only_chrom_pure_conv_prom_perturb/\")\n",
    "figures_path = Path(\"../figures/Figures_revisions/\")\n",
    "ids, predicted, actual = _get_predictions(outputs_path, N_BINS=200, condition_list = [\"_ctrl\"])\n",
    "\n",
    "color_list = ['purple','black','lightseagreen', 'orange', 'firebrick', 'darkgreen']\n",
    "predicted_tracks=['baseline','x0.5 Promoter', 'x2 Promoter', 'Flipped promoter region', 'Proximal enhancer added', 'New promoter added']\n",
    "predictions = []\n",
    "\n",
    "for k, (ID,line_p,line_a) in enumerate(zip(ids,predicted.values,actual.values)):   \n",
    "    print(ID)\n",
    "    line_p = predicted.loc[ID]\n",
    "    predictions.append({'profile':line_p,\n",
    "                        'color':color_list[k],\n",
    "                        'alpha': 0.1,\n",
    "                        'label': predicted_tracks[k],\n",
    "                        'ls':'-'\n",
    "    })\n",
    "\n",
    "\n",
    "# Actual baseline signal:\n",
    "ID = \"ENSMUSG00000028291.7_forward\"\n",
    "*_ , actual = _get_predictions(original_outputs_path, N_BINS=200, condition_list = [\"_ctrl\"])\n",
    "\n",
    "actual_data = [{\n",
    "    'profile': actual.loc[ID],\n",
    "    'color': 'silver',\n",
    "    'alpha': 0.6,\n",
    "    'label': 'EU-seq',\n",
    "    'ls':'-'\n",
    "}]\n",
    "\n",
    "# Create plot\n",
    "fig = plot_generalized_predictions(predictions, actual_data)\n",
    "fig.savefig(figures_path / 'Outputs' / 'Perturbed_promoter.png', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> B) Creating a shorter context model: <center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reduced the input context to 20 kbp and the output context to the central 18 kbp.\n",
    "\n",
    "**Creating the new inputs and outputs:**\n",
    "\n",
    "This is a highly parallelizable process, so we will use the python multiprocessing package to exploit our slurm-based cluster. We ran:\n",
    "```bash\n",
    "srun --cpus-per-task=16 -- python ./create_cropped_inputs.py\n",
    "```\n",
    "\n",
    ">**Data processing:**\n",
    ">We just cropped the exact same inputs and outputs to the new desired length. \n",
    ">\n",
    ">**Model changes:**\n",
    ">We reduced the size of the model accordingly:\n",
    ">- Smaller feature extractor: too many convolutions would compress too much the input sequences.\n",
    ">- Smaller MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile create_cropped_inputs.py\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import os\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "\n",
    "def process_sample(filename, original_path, new_path, new_dim=201, original_dim=10001):\n",
    "    \"\"\"Process a single sample file.\"\"\"\n",
    "    crop_shift = (original_dim - new_dim) // 2\n",
    "    try:\n",
    "        array = np.load(original_path / filename)\n",
    "        new_array = array[:, crop_shift:-crop_shift]\n",
    "        np.save(new_path / filename, new_array)\n",
    "        return f\"Successfully processed {filename}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error processing {filename}: {str(e)}\"\n",
    "\n",
    "def main():\n",
    "    # Define paths\n",
    "    original_inputs_paths = [\n",
    "        Path(\"../inputs/landscape_arrays/training/\"),\n",
    "        Path(\"../inputs/landscape_arrays/test/\")\n",
    "    ]\n",
    "    new_inputs_paths = [\n",
    "        Path(\"../inputs/landscape_arrays/training_20kbp_context/\"),\n",
    "        Path(\"../inputs/landscape_arrays/test_20kbp_context/\")\n",
    "    ]\n",
    "\n",
    "    # Create output directories if they don't exist\n",
    "    for path in new_inputs_paths:\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Number of processes to use (leave one core free for system processes)\n",
    "    num_processes = max(1, cpu_count() - 1)\n",
    "    \n",
    "    for i, path in enumerate(original_inputs_paths):\n",
    "        # Get list of filenames instead of DirEntry objects\n",
    "        filenames = [f.name for f in os.scandir(path)]\n",
    "        \n",
    "        # Create partial function with fixed arguments\n",
    "        process_func = partial(\n",
    "            process_sample,\n",
    "            original_path=path,\n",
    "            new_path=new_inputs_paths[i]\n",
    "        )\n",
    "        \n",
    "        # Process files in parallel\n",
    "        print(f\"Processing files in {path} using {num_processes} processes...\")\n",
    "        with Pool(num_processes) as pool:\n",
    "            results = pool.map(process_func, filenames)\n",
    "        \n",
    "        # Print results\n",
    "        for result in results:\n",
    "            print(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training eir only chrom pure conv for 20kbp context:**\n",
    "\n",
    "```bash\n",
    "eirtrain \\\n",
    "--global_configs ./configurations/conf_pure_conv_20kbp_context/globals.yaml \\\n",
    "--input_configs ./configurations/conf_pure_conv_20kbp_context/input_cnn.yaml \\\n",
    "--fusion_configs ./configurations/conf_pure_conv_20kbp_context/fusion.yaml \\\n",
    "--output_configs ./configurations/conf_pure_conv_20kbp_context/outputs_2_cond.yaml\n",
    "```\n",
    "\n",
    "**Testing eir only chrom pure conv for 20kbp context:**\n",
    "\n",
    "```bash\n",
    "eirpredict \\\n",
    "--global_configs ./configurations/conf_pure_conv_20kbp_context_test/globals.yaml \\\n",
    "--input_configs ./configurations/conf_pure_conv_20kbp_context_test/input_cnn.yaml \\\n",
    "--fusion_configs ./configurations/conf_pure_conv_20kbp_context_test/fusion.yaml \\\n",
    "--output_configs ./configurations/conf_pure_conv_20kbp_context_test/outputs_2_cond.yaml \\\n",
    "--evaluate \\\n",
    "--model_path ./runs/gene_expression_only_chrom_pure_conv_20kbp_context/saved_models/gene_expression_only_chrom_pure_conv_20kbp_context_model_30300_perf-average=0.7867.pt \\\n",
    "--output_folder ./runs/test_runs/gene_expression_only_chrom_pure_conv_20kbp_context_test \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> C) Changing the last activation function or the loss:\n",
    "- Output activation to softplus. \n",
    "> ⚠️ WARNING ⚠️ : This is a temporary hotfix that overwrites the linear head to introduce the least changes to the eir code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modifications to eir's (version 0.1.42) code:**\n",
    "\n",
    "_Changing activation function of the last layer:_\n",
    "\n",
    "We will change the file defining the last layer activation functions for tabular outputs, which in on our environment can be found at ```claster_env/lib/python3.11/site-packages/eir/models/output/tabular/linear.py```.\n",
    "Here we will edit the function:\n",
    "\n",
    "```python\n",
    "\n",
    "#Original linear head\n",
    "def _get_linear_multi_task_branches(\n",
    "    input_dimension: int,\n",
    "    num_outputs_per_target: \"al_num_outputs_per_target\",\n",
    ") -> nn.ModuleDict:\n",
    "    multi_task_branches = nn.ModuleDict(\n",
    "        {\n",
    "            target: nn.Linear(\n",
    "                in_features=input_dimension,\n",
    "                out_features=num_outputs,\n",
    "            )\n",
    "            for target, num_outputs in num_outputs_per_target.items()\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return multi_task_branches\n",
    "```\n",
    "\n",
    "The new function will simply pass the output through a softplus transformation. The same principles would apply for ReLU:\n",
    "\n",
    "```python\n",
    "# New head\n",
    "def _get_linear_multi_task_branches(\n",
    "    input_dimension: int,\n",
    "    num_outputs_per_target: \"al_num_outputs_per_target\",\n",
    ") -> nn.ModuleDict:\n",
    "    multi_task_branches = nn.ModuleDict(\n",
    "        {\n",
    "            target: nn.Sequential(\n",
    "                nn.Linear(\n",
    "                    in_features=input_dimension,\n",
    "                    out_features=num_outputs,\n",
    "                ),\n",
    "                nn.Softplus()\n",
    "            )\n",
    "            for target, num_outputs in num_outputs_per_target.items()\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return multi_task_branches\n",
    "```\n",
    "\n",
    "_Setting the log_input to False in PoissonNLLL:_\n",
    "\n",
    "This is the edit in the file ```claster_env/lib/python3.11/site-packages/eir/train_utils/criteria.py```:\n",
    "\n",
    "```python\n",
    "def _calc_con_loss(input: torch.Tensor, target: torch.Tensor, loss_func: al_con_losses):\n",
    "    match loss_func:\n",
    "        case nn.PoissonNLLLoss():\n",
    "            # Before we had:\n",
    "            #return loss_func(log_input=input.squeeze(), target=target.squeeze())\n",
    "            # Create a new PoissonNLLLoss with log_input=False\n",
    "            poisson_loss = nn.PoissonNLLLoss(log_input=False)\n",
    "            # Note that despite the parameter name being log_input, we pass the raw input tensor\n",
    "            return poisson_loss(input.squeeze(), target.squeeze())\n",
    "        case _:\n",
    "            return loss_func(input=input.squeeze(), target=target.squeeze())\n",
    "\n",
    "```\n",
    "\n",
    "**Run models:**\n",
    "\n",
    "Training eir only chrom pure conv with softplus:\n",
    "```bash\n",
    "eirtrain \\\n",
    "--global_configs ./configurations/conf_pure_conv_softplus/globals.yaml \\\n",
    "--input_configs ./configurations/conf_pure_conv_softplus/input_cnn.yaml \\\n",
    "--fusion_configs ./configurations/conf_pure_conv_softplus/fusion.yaml \\\n",
    "--output_configs ./configurations/conf_pure_conv_softplus/outputs_2_cond.yaml\n",
    "\n",
    "eirtrain \\\n",
    "--global_configs ./configurations/conf_pure_conv_softplus_poisson/globals.yaml \\\n",
    "--input_configs ./configurations/conf_pure_conv_softplus_poisson/input_cnn.yaml \\\n",
    "--fusion_configs ./configurations/conf_pure_conv_softplus_poisson/fusion.yaml \\\n",
    "--output_configs ./configurations/conf_pure_conv_softplus_poisson/outputs_2_cond.yaml\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize predictions for Softplus and Poisson losses:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputs:\n",
    "original_outputs_path_dict = {Path(\"../runs/gene_expression_only_chrom_pure_conv_softplus_poisson_no_bias/results/\"):[\"Predicted EU-seq Softplus Poisson\", 'royalblue','-'],\n",
    "                              Path(\"../runs/gene_expression_only_chrom_pure_conv_softplus_no_bias/results/\"):[\"Predicted EU-seq Softplus SmoothL1\", 'orange','-'],\n",
    "                              Path(\"../runs/gene_expression_only_chrom_pure_conv_softplus_poisson_no_bias_log_input_false/results/\"):[\"Predicted EU-seq Softplus Poisson log input false\", 'darkred','-']}\n",
    "\n",
    "training_targets_path = Path(\"../targets/training_targets.csv\")\n",
    "\n",
    "figures_path = Path(\"../figures/Figures_revisions/\")\n",
    "ids = [\"ENSMUSG00000051977.16_forward\",\"ENSMUSG00000109212.3_forward\"]\n",
    "\n",
    "for k,ID in enumerate(ids):\n",
    "    predictions = []\n",
    "    for original_outputs_path,attributes in original_outputs_path_dict.items():\n",
    "        ids, predicted , actual = _get_predictions(original_outputs_path, N_BINS=200, condition_list = [\"_ctrl\"], is_training=True,batch_num=60600, CLIP=False)\n",
    "\n",
    "        line_p = predicted.loc[ID]\n",
    "        predictions.append({'profile':line_p,\n",
    "                            'color':attributes[1],\n",
    "                            'alpha': 0.05,\n",
    "                            'label': attributes[0],\n",
    "                            'ls':attributes[2]\n",
    "                            })\n",
    "        \n",
    "\n",
    "    # Actual baseline signal:\n",
    "    actual_data = [{\n",
    "        'profile': actual.loc[ID],\n",
    "        'color': 'silver',\n",
    "        'alpha': 0.6,\n",
    "        'label': 'EU-seq',\n",
    "        'ls':'-'\n",
    "    }]\n",
    "\n",
    "    # Averaged actual targets across samples:    \n",
    "    avg_training_targets = pd.read_csv(training_targets_path).set_index('ID').values.mean(axis=0)\n",
    "    predictions.append({'profile':avg_training_targets,\n",
    "                            'color':'darkgreen',\n",
    "                            'alpha': 0.1,\n",
    "                            'label': 'Mean EU-seq per bin',\n",
    "                            'ls':'--'\n",
    "                            })\n",
    "\n",
    "\n",
    "    # Create plot\n",
    "    fig = plot_generalized_predictions(predictions, actual_data)\n",
    "    fig.savefig(figures_path / 'Outputs' / f'Softplus_no_bias_{ID}.png', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> D) Testing on another chromosome (chr19):\n",
    "\n",
    "We will:\n",
    "- Keep the samples on chr4 as hold out.\n",
    "- Additionally mask samples from chr19 from training (set them as validation)\n",
    "- Introduce samples in chr17 in the training (were used for validation originally). \n",
    "- Note that we will not validate now since we already have the model's hyperparameters defined. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training eir for chr19 holdout:**\n",
    "\n",
    "```bash\n",
    "eirtrain \\\n",
    "--global_configs ./configurations/conf_pure_conv_chr19_holdout_train/globals.yaml \\\n",
    "--input_configs ./configurations/conf_pure_conv_chr19_holdout_train/input_cnn.yaml \\\n",
    "--fusion_configs ./configurations/conf_pure_conv_chr19_holdout_train/fusion.yaml \\\n",
    "--output_configs ./configurations/conf_pure_conv_chr19_holdout_train/outputs_2_cond.yaml\n",
    "```\n",
    "\n",
    "**Testing eir for chr19 holdout:**\n",
    "\n",
    "```bash\n",
    "eirpredict \\\n",
    "--global_configs ./configurations/conf_pure_conv_chr19_holdout_test/globals.yaml \\\n",
    "--input_configs ./configurations/conf_pure_conv_chr19_holdout_test/input_cnn.yaml \\\n",
    "--fusion_configs ./configurations/conf_pure_conv_chr19_holdout_test/fusion.yaml \\\n",
    "--output_configs ./configurations/conf_pure_conv_chr19_holdout_test/outputs_2_cond.yaml \\\n",
    "--evaluate \\\n",
    "--model_path ./runs/gene_expression_only_chrom_pure_conv_chr19_holdout/saved_models/gene_expression_only_chrom_pure_conv_chr19_holdout_model_30300_perf-average=0.6487.pt \\\n",
    "--output_folder ./runs/test_runs/gene_expression_only_chrom_pure_conv_chr19_holdout_test\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the notebook II_run_CLASTER:\n",
    "def create_validation_ids(path: Path, val_chrom: str = \"chr19\"):\n",
    "    \"\"\" All genes encoded in chr19 will be used as a validation set.\"\"\"\n",
    "    gene_annotations_df = pd.read_csv(path / \"Final_gene_annotations.tsv\", sep=\"\\t\").set_index('ID')\n",
    "    missing_samples_report = pd.read_csv(path / \"missing_samples_report.csv\")[\"Sample\"].values\n",
    "    indices = gene_annotations_df[gene_annotations_df[\"chr\"] == val_chrom].index\n",
    "\n",
    "    # Prepare the strings to write in the file\n",
    "    lines = [f\"{index}_forward\\n{index}_rev\\n\" for index in indices if index+'_forward' not in missing_samples_report]\n",
    "\n",
    "    # Write to the text file\n",
    "    with open(path / f\"manual_validation_ids_{val_chrom}.txt\", 'w') as file:\n",
    "        file.writelines(lines)\n",
    "\n",
    "path = Path(\"../annotations/\")\n",
    "create_validation_ids(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_path = Path(\"../annotations/manual_validation_ids_chr19.txt\")\n",
    "sample_ids = pd.read_csv(ids_path, header=None)[0].tolist()\n",
    "training_samples = [array.name for array in os.scandir(Path(\"../inputs/landscape_arrays/training/\"))]\n",
    "\n",
    "for sample in sample_ids:\n",
    "    if f\"{sample}.npy\" not in training_samples:\n",
    "        print(sample)\n",
    "print(len(sample_ids))\n",
    "\n",
    "b = pd.read_csv(Path(\"../annotations/Final_gene_annotations.tsv\"), sep='\\t')\n",
    "b = b[b['chr'] == 'chr19']\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Note:* samples ENSMUSG00000095993.1_forward and ENSMUSG00000095993.1_rev, corresponding to the last gene annotated in chr19 (Gm21060) were not in the original input training set, so we removed their corresponding ids in the hold out manual validation list for chr19.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Copying chr19\n",
    "# Read sample IDs using pandas\n",
    "ids_path = Path(\"../annotations/manual_validation_ids_chr19.txt\")\n",
    "sample_ids = pd.read_csv(ids_path, header=None)[0].tolist()\n",
    "\n",
    "# Define paths using pathlib\n",
    "source_dir = Path('../inputs/landscape_arrays/training')\n",
    "dest_dir = Path('../inputs/landscape_arrays/test_chr19')\n",
    "\n",
    "# Create destination directory if it doesn't exist\n",
    "dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# COPY matching .npy files (not move)\n",
    "moved_count = 0\n",
    "for file_path in source_dir.glob('*.npy'):\n",
    "    # Check if any sample ID from our list is in the filename\n",
    "    if any(sample_id in file_path.stem for sample_id in sample_ids):\n",
    "        dest_path = dest_dir / file_path.name\n",
    "        shutil.copy(str(file_path), str(dest_path))\n",
    "        moved_count += 1\n",
    "        print(f\"Copied: {file_path.name}\")\n",
    "\n",
    "print(f\"\\nTotal files moved: {moved_count}\")\n",
    "\n",
    "################## Retrieving the targets from the original file ################\n",
    "# Read sample IDs using pandas\n",
    "manual_ids = pd.read_csv('../annotations/manual_validation_ids_chr19.txt', header=None)[0].tolist()\n",
    "\n",
    "# Read the training targets file\n",
    "training_targets = pd.read_csv('../targets/training_targets.csv')\n",
    "\n",
    "# Filter rows where ID matches our manual validation IDs\n",
    "test_targets = training_targets[training_targets['ID'].isin(manual_ids)]\n",
    "\n",
    "# Save to new file in the test directory\n",
    "dest_dir = Path('../targets/')\n",
    "test_targets.to_csv(dest_dir / 'test_targets_chr19.csv', index=False)\n",
    "\n",
    "print(f\"Original targets shape: {training_targets.shape}\")\n",
    "print(f\"Test targets shape: {test_targets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> E) Training without the enhancer mark (H3K27ac) as an input.\n",
    "\n",
    "We removed the row corresponding to the enhancer mark H3K27ac from all the input numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile create_inputs_without_H3K27ac.py\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "\n",
    "def process_file(file_path, savepath):\n",
    "    # Load the array\n",
    "    data = np.load(file_path)\n",
    "    \n",
    "    # Remove H3K27ac row (ATAC, H3K4me3, H3K27ac, H3K27me3)\n",
    "    data_filtered = np.delete(data, 2, axis=0)\n",
    "    \n",
    "    # Save to new location with same filename\n",
    "    output_path = savepath / file_path.name\n",
    "    np.save(output_path, data_filtered)\n",
    "\n",
    "def main():\n",
    "    input_folder = Path('../inputs/landscape_arrays/test/') #Path('../inputs/landscape_arrays/training/')\n",
    "    savepath = Path('../inputs/landscape_arrays/test_no_H3K27ac/') # Path('../inputs/landscape_arrays/training_no_H3K27ac/')\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    savepath.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Get list of all files\n",
    "    files = list(input_folder.glob('*.npy'))\n",
    "    \n",
    "    # Set up multiprocessing\n",
    "    n_cpus = mp.cpu_count() - 1\n",
    "    \n",
    "    # Create partial function with fixed savepath\n",
    "    process_func = partial(process_file, savepath=savepath)\n",
    "    \n",
    "    # Create pool and map files to processes\n",
    "    with mp.Pool(n_cpus) as pool:\n",
    "        pool.map(process_func, files)\n",
    "    \n",
    "    print(\"Processing complete!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training eir only chrom pure conv with no H3K27ac:**\n",
    "\n",
    "```bash\n",
    "eirtrain \\\n",
    "--global_configs ./configurations/conf_pure_conv_no_H3K27ac_train/globals.yaml \\\n",
    "--input_configs ./configurations/conf_pure_conv_no_H3K27ac_train/input_cnn.yaml \\\n",
    "--fusion_configs ./configurations/conf_pure_conv_no_H3K27ac_train/fusion.yaml \\\n",
    "--output_configs ./configurations/conf_pure_conv_no_H3K27ac_train/outputs_2_cond.yaml\n",
    "```\n",
    "\n",
    "\n",
    "**Testing eir only chrom pure conv with no H3K27ac:**\n",
    "\n",
    "```bash\n",
    "eirpredict \\\n",
    "--global_configs ./configurations/conf_pure_conv_no_H3K27ac_test/globals.yaml \\\n",
    "--input_configs ./configurations/conf_pure_conv_no_H3K27ac_test/input_cnn.yaml \\\n",
    "--fusion_configs ./configurations/conf_pure_conv_no_H3K27ac_test/fusion.yaml \\\n",
    "--output_configs ./configurations/conf_pure_conv_no_H3K27ac_test/outputs_2_cond.yaml \\\n",
    "--evaluate \\\n",
    "--model_path ./runs/gene_expression_only_chrom_pure_conv_no_H3K27ac/saved_models/gene_expression_only_chrom_pure_conv_no_H3K27ac_model_60600_perf-average=0.7807.pt \\\n",
    "--output_folder ./runs/test_runs/gene_expression_only_chrom_pure_conv_no_H3K27ac_test\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> F) Predict untransformed EU-seq, only binned at 1kbp resolution. <center>\n",
    "\n",
    "Reviewer 2 was concerned about potential oversmoothing of the signals and the impact that this would cause in model performance. We proceeded to train CLASTER aiming to predict EU-seq profiles directly binned at a 1kbp resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile create_1kbp_binned_targets.py\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyBigWig\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "\n",
    "def create_target_array(data_path: Path,\n",
    "                        output_shift: int,\n",
    "                        n_output_bins:int,\n",
    "                        bw_target_track_list:list,\n",
    "                        ID: str,\n",
    "                        TSS: int,\n",
    "                        chrom: str):\n",
    "    \"\"\"\n",
    "    Targets are centered at the TSS, and obtained at a 20 bp resolution, smoothed and downsized to 1kbp resolution.\n",
    "    \"\"\"\n",
    "    # Initialize stats as None\n",
    "    stats = None\n",
    "    \n",
    "    for bw_path in bw_target_track_list:\n",
    "        try:\n",
    "            bw = pyBigWig.open(str(data_path / bw_path), \"r\")\n",
    "            stats = bw.stats(chrom,TSS-output_shift,TSS+output_shift,type=\"mean\",nBins=n_output_bins)\n",
    "            bw.close()\n",
    "            if stats is not None:  # Only process if we got valid stats\n",
    "                stats = np.array([float(value) if value is not None else 0. for value in stats])\n",
    "                stats = np.clip(np.array(stats),0,None)\n",
    "            else:\n",
    "                logging.info(f\"{ID} no valid stats obtained.\")\n",
    "                stats = np.zeros(n_output_bins)  # Return zeros if no valid stats\n",
    "        except Exception as e:\n",
    "            logging.info(f\"{ID} target coordinates are out of bounds. Error: {str(e)}\")\n",
    "            stats = np.zeros(n_output_bins)  # Return zeros in case of error\n",
    "    \n",
    "    if stats is None:\n",
    "        stats = np.zeros(n_output_bins)  # Fallback if no valid stats were obtained\n",
    "        \n",
    "    return stats.flatten(), np.flip(stats).flatten()\n",
    "\n",
    "def process_target_data(data_path, bw_target_track_list, output_shift, n_output_bins, OUTPUT_LENGTH, target_files, row):\n",
    "    ID, chrom, Start, End, Strand, Name, CRE_type = row\n",
    "    split = \"test\" if chrom == \"chr4\" else \"training\"\n",
    "    TSS = int(Start) if Strand == '+' else int(End)\n",
    "\n",
    "    target_sample_pos, target_sample_neg = create_target_array(data_path, output_shift, n_output_bins, bw_target_track_list, ID, TSS, chrom)\n",
    "\n",
    "    if (len(target_sample_pos) == OUTPUT_LENGTH):\n",
    "        target_file_path = target_files[split]\n",
    "        with open(target_file_path, 'a') as file:\n",
    "            file.write(f\"{ID}_forward,\" + \",\".join(map(str, target_sample_pos)) + \"\\n\")\n",
    "            file.write(f\"{ID}_rev,\" + \",\".join(map(str, target_sample_neg)) + \"\\n\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    path = Path(\"../\")\n",
    "    data_path = path / \"GEO_files\"\n",
    "    target_path = path / \"targets\"\n",
    "    target_path.mkdir(exist_ok=True, parents=True)\n",
    "    split_list = [\"training\",\"test\"]\n",
    "\n",
    "    # Start logger\n",
    "    LOG_FILENAME = \"/Landscape_data_obtention_1kbp_binned.log\"\n",
    "    logging.basicConfig(filename=str(path) + LOG_FILENAME, level=logging.INFO)  \n",
    "\n",
    "    gene_annotations_path = path / \"annotations\" / \"Final_gene_annotations.tsv\"\n",
    "    gene_annotations_df = pd.read_csv(gene_annotations_path, sep=\"\\t\")\n",
    "\n",
    "    output_shift = 200500 #200500 # Output in 1kbp resolution (500 extra per side to have extra central bin)\n",
    "    n_output_bins = 401\n",
    "    OUTPUT_LENGTH = 401 #802\n",
    "    N_BINS = 200\n",
    "    bw_target_track_list = [\"EU_Seq_Ctrl.bw\"] # \"EU_Seq_Treated.bw\"\n",
    "\n",
    "    target_files = {split: (path / \"targets\" / f\"{split}_targets_1kbp_binning.csv\") for split in [\"training\", \"validation\", \"test\"]}\n",
    "    for file_path in target_files.values():\n",
    "        with open(file_path, 'w') as file:\n",
    "            file.write(\"ID,\" + \",\".join([f\"{i}{cond}\" for cond in [\"_ctrl\"] for i in range(-N_BINS,N_BINS+1)]) + \"\\n\")\n",
    "\n",
    "    process_args = (data_path, bw_target_track_list, output_shift, n_output_bins, OUTPUT_LENGTH, target_files)\n",
    "\n",
    "    with multiprocessing.Pool(processes=multiprocessing.cpu_count() - 1) as pool:\n",
    "        pool.starmap(process_target_data, [(process_args + (row,)) for row in gene_annotations_df.to_numpy()])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**New targets analysis:**\n",
    "- Remove samples that were missing in the original analyses.\n",
    "- Compute correlation between binned and smoothed profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_training_targets = pd.read_csv(\"../targets/training_targets_1kbp_binning.csv\").set_index('ID')\n",
    "new_test_targets = pd.read_csv(\"../targets/test_targets_1kbp_binning.csv\").set_index('ID')\n",
    "\n",
    "old_training_targets = pd.read_csv(\"../targets/training_targets.csv\").set_index('ID')\n",
    "old_test_targets = pd.read_csv(\"../targets/test_targets.csv\").set_index('ID')\n",
    "\n",
    "\n",
    "new_training_targets = new_training_targets.reindex(old_training_targets.index)\n",
    "new_test_targets = new_test_targets.reindex(old_test_targets.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = new_test_targets.values.flatten()\n",
    "y = old_test_targets.values.flatten()\n",
    "r = pearsonr(x,y)\n",
    "\n",
    "figure_path = Path(\"../figures/Figures_revisions\")\n",
    "\n",
    "area = plot_correlations(figure_path, x, y, \"Smoothed_vs_1kbp_binned_correlation.png\", cmap=\"binary\", binlims=(0,200), density=True, min_val = 0, max_val=200)\n",
    "\n",
    "\n",
    "for ID in new_test_targets.index[::100]:\n",
    "    predictions = [{\n",
    "        'profile': new_test_targets.loc[ID],\n",
    "        'color': 'royalblue',\n",
    "        'alpha': 0.6,\n",
    "        'label': 'EU-seq binned 1kbp',\n",
    "        'ls':'-'\n",
    "\n",
    "    }]\n",
    "\n",
    "    actual_data = {\n",
    "        'profile': old_test_targets.loc[ID],\n",
    "        'color': 'silver',\n",
    "        'alpha': 0.6,\n",
    "        'label': 'EU-seq',\n",
    "        'ls':'-',\n",
    "    }\n",
    "\n",
    "    fig = plot_generalized_predictions(predictions, actual_data)\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training eir only chrom pure conv with no smoothing, directly binning the reads in 1kbp bins:**\n",
    "\n",
    "```bash\n",
    "eirtrain \\\n",
    "--global_configs ./configurations/conf_pure_conv_1kbp_binning_train/globals.yaml \\\n",
    "--input_configs ./configurations/conf_pure_conv_1kbp_binning_train/input_cnn.yaml \\\n",
    "--fusion_configs ./configurations/conf_pure_conv_1kbp_binning_train/fusion.yaml \\\n",
    "--output_configs ./configurations/conf_pure_conv_1kbp_binning_train/outputs_2_cond.yaml\n",
    "```\n",
    "\n",
    "**Testing eir only chrom pure conv with no smoothing, directly binning the reads in 1kbp bins:**\n",
    "\n",
    "```bash\n",
    "eirpredict \\\n",
    "--global_configs ./configurations/conf_pure_conv_1kbp_binning_test/globals.yaml \\\n",
    "--input_configs ./configurations/conf_pure_conv_1kbp_binning_test/input_cnn.yaml \\\n",
    "--fusion_configs ./configurations/conf_pure_conv_1kbp_binning_test/fusion.yaml \\\n",
    "--output_configs ./configurations/conf_pure_conv_1kbp_binning_test/outputs_2_cond.yaml \\\n",
    "--evaluate \\\n",
    "--model_path ./runs/gene_expression_only_chrom_pure_conv_1kbp_binning/saved_models/gene_expression_only_chrom_pure_conv_1kbp_binning_model_60600_perf-average=0.7832.pt \\\n",
    "--output_folder ./runs/test_runs/gene_expression_only_chrom_pure_conv_1kbp_binning_test\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze predictions and attributions of the new models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path_dict = {\n",
    "    \"Only Chromatin Pure Convolutions training\": {\"path\": Path(\"../runs/gene_expression_only_chrom_pure_conv/results/\"),\n",
    "                                                  \"is_training\": True,\n",
    "                                                  \"N_BINS\": 200,\n",
    "                                                  \"batch_num\": 60600, \n",
    "                                                  \"attributions\": False,\n",
    "                                                  \"label\": \"Gaussian\",\n",
    "                                                  \"color\": \"blue\"},\n",
    "    \"Only Chromatin Pure Convolution test\": {\"path\": Path(\"../runs/test_runs/gene_expression_only_chrom_pure_conv/\"),\n",
    "                                             \"is_training\": False,\n",
    "                                             \"N_BINS\": 200,\n",
    "                                             \"batch_num\": None,\n",
    "                                             \"attributions\": True,\n",
    "                                             \"label\": \"CLASTER test\",\n",
    "                                             \"color\": \"blue\"},\n",
    "    \"Only Chromatin with Attention training\": {\"path\": Path(\"../runs/gene_expression_only_chrom_attention/results/\"),\n",
    "                                              \"is_training\": True,\n",
    "                                              \"N_BINS\": 200,\n",
    "                                              \"batch_num\": 60600,\n",
    "                                              \"attributions\": False,\n",
    "                                              \"label\": \"Only chrom attention\",\n",
    "                                              \"color\": \"blue\"},\n",
    "    \"Only Chromatin with Attention test\": {\"path\": Path(\"../runs/test_runs/gene_expression_only_chrom_attention/\"),\n",
    "                                          \"is_training\": False,\n",
    "                                          \"N_BINS\": 200,\n",
    "                                          \"batch_num\": None,\n",
    "                                          \"attributions\": True,\n",
    "                                          \"label\": \"Only chrom attention\",\n",
    "                                          \"color\": \"blue\"},\n",
    "    \"Chromatin and Structure training\": {\"path\": Path(\"../runs/gene_expression_microc_pure_conv/results/\"),\n",
    "                                        \"is_training\": True,\n",
    "                                        \"N_BINS\": 200,\n",
    "                                        \"batch_num\": 60600,\n",
    "                                        \"attributions\": False,\n",
    "                                        \"label\": \"Chrom and structure\",\n",
    "                                        \"color\": \"blue\"},\n",
    "    \"Chromatin and Structure test\": {\"path\": Path(\"../runs/test_runs/gene_expression_microc_pure_conv/\"),\n",
    "                                    \"is_training\": False,\n",
    "                                    \"N_BINS\": 200,\n",
    "                                    \"batch_num\": None,\n",
    "                                    \"attributions\": True,\n",
    "                                    \"label\": \"Chrom and Structure\",\n",
    "                                    \"color\": \"blue\"},\n",
    "    \"Chromatin and Structure rotated training\": {\"path\": Path(\"../runs/gene_expression_microc_rotated_pure_conv/results/\"),\n",
    "                                               \"is_training\": True,\n",
    "                                               \"N_BINS\": 200,\n",
    "                                               \"batch_num\": 60600,\n",
    "                                               \"attributions\": False,\n",
    "                                               \"label\": \"Chrom and Structure rotated\",\n",
    "                                               \"color\": \"blue\"},\n",
    "    \"Chromatin and Structure rotated test\": {\"path\": Path(\"../runs/test_runs/gene_expression_microc_rotated_pure_conv/\"),\n",
    "                                           \"is_training\": False,\n",
    "                                           \"N_BINS\": 200,\n",
    "                                           \"batch_num\": None,\n",
    "                                           \"attributions\": True,\n",
    "                                           \"label\": \"Chrom and Structure rotated\",\n",
    "                                           \"color\": \"blue\"},\n",
    "    \"Only Chromatin Pure Convolutions Softplus SmoothL1 training\": {\"path\": Path(\"../runs/gene_expression_only_chrom_pure_conv_softplus_no_bias/results/\"),\n",
    "                                                                   \"is_training\": True,\n",
    "                                                                   \"N_BINS\": 200,\n",
    "                                                                   \"batch_num\": 60600,\n",
    "                                                                   \"attributions\": False,\n",
    "                                                                   \"label\": \"train Softplus SmoothL1\",\n",
    "                                                                   \"color\": \"purple\"},\n",
    "    \"Only Chromatin Pure Convolutions Softplus Poisson training\": {\"path\": Path(\"../runs/gene_expression_only_chrom_pure_conv_softplus_poisson_no_bias/results/\"),\n",
    "                                                                 \"is_training\": True,\n",
    "                                                                 \"N_BINS\": 200,\n",
    "                                                                 \"batch_num\": 60600,\n",
    "                                                                 \"attributions\": False,\n",
    "                                                                 \"label\": \"train Softplus Poisson\",\n",
    "                                                                 \"color\": \"orange\"},\n",
    "    \"Only Chromatin Pure Convolutions Softplus Poisson log_input=False training\": {\"path\": Path(\"../runs/gene_expression_only_chrom_pure_conv_softplus_poisson_no_bias_log_input_false/results/\"),\n",
    "                                                                                  \"is_training\": True,\n",
    "                                                                                  \"N_BINS\": 200,\n",
    "                                                                                  \"batch_num\": 60600,\n",
    "                                                                                  \"attributions\": False,\n",
    "                                                                                  \"label\": \"train Softplus Poisson log input false\",\n",
    "                                                                                  \"color\": \"darkred\"},\n",
    "    \"Only Chromatin Pure Convolutions 20kbp context training\": {\"path\": Path(\"../runs/gene_expression_only_chrom_pure_conv_20kbp_context/results/\"),\n",
    "                                                              \"is_training\": True,\n",
    "                                                              \"N_BINS\": 9,\n",
    "                                                              \"batch_num\": 30300,\n",
    "                                                              \"attributions\": False,\n",
    "                                                              \"label\": \"train 20kb context\",\n",
    "                                                              \"color\": \"yellow\"},\n",
    "    \"Only Chromatin Pure Convolutions 20kbp context test\": {\"path\": Path(\"../runs/test_runs/gene_expression_only_chrom_pure_conv_20kbp_context_test\"),\n",
    "                                                          \"is_training\": False,\n",
    "                                                          \"N_BINS\": 9,\n",
    "                                                          \"batch_num\": None,\n",
    "                                                          \"attributions\": False,\n",
    "                                                          \"label\": \"test 20kb context\",\n",
    "                                                          \"color\": \"green\"},\n",
    "    \"Only Chromatin Pure Convolutions chr19 holdout training\": {\"path\": Path(\"../runs/gene_expression_only_chrom_pure_conv_chr19_holdout/results/\"),\n",
    "                                                              \"is_training\": True,\n",
    "                                                              \"N_BINS\": 200,\n",
    "                                                              \"batch_num\": 30300,\n",
    "                                                              \"attributions\": False,\n",
    "                                                              \"label\": \"train chr19 holdout\",\n",
    "                                                              \"color\": \"lightblue\"},\n",
    "    \"Only Chromatin Pure Convolutions chr19 holdout test\": {\"path\": Path(\"../runs/test_runs/gene_expression_only_chrom_pure_conv_chr19_holdout_test/\"),\n",
    "                                                          \"is_training\": False,\n",
    "                                                          \"N_BINS\": 200,\n",
    "                                                          \"batch_num\": None,\n",
    "                                                          \"attributions\": True,\n",
    "                                                          \"label\": \"test chr19 holdout\",\n",
    "                                                          \"color\": \"lightblue\"},\n",
    "    \"Only Chromatin Pure Convolutions no H3K27ac training\": {\"path\": Path(\"../runs/gene_expression_only_chrom_pure_conv_no_H3K27ac/results/\"),\n",
    "                                                           \"is_training\": True,\n",
    "                                                           \"N_BINS\": 200,\n",
    "                                                           \"batch_num\": 60600,\n",
    "                                                           \"attributions\": False,\n",
    "                                                           \"label\": \"train no H3K27ac\",\n",
    "                                                           \"color\": \"darkgreen\"},\n",
    "    \"Only Chromatin Pure Convolutions no H3K27ac test\": {\"path\": Path(\"../runs/test_runs/gene_expression_only_chrom_pure_conv_no_H3K27ac_test/\"),\n",
    "                                                       \"is_training\": False,\n",
    "                                                       \"N_BINS\": 200,\n",
    "                                                       \"batch_num\": None,\n",
    "                                                       \"attributions\": True,\n",
    "                                                       \"label\": \"test no H3K27ac\",\n",
    "                                                       \"color\": \"darkgreen\"},\n",
    "    \"Only Chromatin Pure Convolutions 1kbp binning training\": {\"path\": Path(\"../runs/gene_expression_only_chrom_pure_conv_1kbp_binning/results/\"),\n",
    "                                                             \"is_training\": True,\n",
    "                                                             \"N_BINS\": 200,\n",
    "                                                             \"batch_num\": 60600,\n",
    "                                                             \"attributions\": False,\n",
    "                                                             \"label\": \"1kbp binning\",\n",
    "                                                             \"color\": \"darkorange\"},\n",
    "    \"Only Chromatin Pure Convolutions 1kbp binning test\": {\"path\": Path(\"../runs/test_runs/gene_expression_only_chrom_pure_conv_1kbp_binning_test/\"),\n",
    "                                                         \"is_training\": False,\n",
    "                                                         \"N_BINS\": 200,\n",
    "                                                         \"batch_num\": None,\n",
    "                                                         \"attributions\": True,\n",
    "                                                         \"label\": \"test 1kbp binning\",\n",
    "                                                         \"color\": \"darkblue\"},\n",
    "    \"Only Chromatin Pure Convolutions K562 RNA-seq training\": {\"path\": Path(\"../runs/gene_expression_only_chrom_K562_train/results/\"),\n",
    "                                                             \"is_training\": True,\n",
    "                                                             \"N_BINS\": 200,\n",
    "                                                             \"batch_num\": 60600,\n",
    "                                                             \"attributions\": False,\n",
    "                                                             \"label\": \"K562 RNA-seq\",\n",
    "                                                             \"color\": \"darkorange\"},\n",
    "    \"Only Chromatin Pure Convolutions K562 POLR2A training\": {\"path\": Path(\"../runs/gene_expression_only_chrom_K562_POLR2A_train/results/\"),\n",
    "                                                            \"is_training\": True,\n",
    "                                                            \"N_BINS\": 200,\n",
    "                                                            \"batch_num\": 30300,\n",
    "                                                            \"attributions\": False,\n",
    "                                                            \"label\": \"K562 POLR2A\",\n",
    "                                                            \"color\": \"darkblue\"},\n",
    "    \"Chromatin and Prom-CHiC training\": {\"path\": Path(\"../runs/gene_expression_prom_CHiC_pure_conv/results/\"),\n",
    "                                        \"is_training\": True,\n",
    "                                        \"N_BINS\": 200,\n",
    "                                        \"batch_num\": 60600,\n",
    "                                        \"attributions\": False,\n",
    "                                        \"label\": \"Prom-CHiC\",\n",
    "                                        \"color\": \"blue\"},\n",
    "    \"Chromatin and Prom-CHiC test\": {\"path\": Path(\"../runs/test_runs/gene_expression_prom_CHiC_pure_conv_test/\"),\n",
    "                                    \"is_training\": False,\n",
    "                                    \"N_BINS\": 200,\n",
    "                                    \"batch_num\": None,\n",
    "                                    \"attributions\": True,\n",
    "                                    \"label\": \"Prom CHiC test\",\n",
    "                                    \"color\": \"darkblue\"},\n",
    "    \"test_K562_enhancer_centric_POLR2A\":{\"path\":Path(\"../runs/perturbation_runs/gene_expression_only_chrom_K562_POLR2A_enhancer_centric/\"),\n",
    "        \"is_training\": False,\n",
    "        \"N_BINS\": 200, \n",
    "        \"batch_num\": None,\n",
    "        \"attributions\": True,\n",
    "        \"label\": \"K562 POLR2A enhancer-centric\",\n",
    "        \"color\": \"darkblue\"},  \n",
    "    \"test_K562_enhancer_centric_RNA\":{\"path\":Path(\"../runs/perturbation_runs/gene_expression_only_chrom_K562_RNA_enhancer_centric/\"),\n",
    "        \"is_training\": False,\n",
    "        \"N_BINS\": 200, \n",
    "        \"batch_num\": None,\n",
    "        \"attributions\": True,\n",
    "        \"label\": \"K562 RNA-seq enhancer-centric\",\n",
    "        \"color\": \"darkblue\"},  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot correlations:** \n",
    " > Note: the function ```calculate_correlations``` was developed for mouse mm10 gene annotations. Please comment on human entries to the previous dictionary to run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_pos_path: Path = Path(\"../annotations/gene_enhancer_relationships_corrected.tsv\")\n",
    "input_table_path: Path = Path(\"../annotations/gene_enhancer_relationships_perturbed.csv\")\n",
    "\n",
    "figure_path = Path(\"../figures/Figures_revisions/prediction_correlations/\")\n",
    "figure_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "condition_list = [\"_ctrl\"] #[\"\"]\n",
    "cmap = \"binary\"\n",
    "\n",
    "for title, attributes in results_path_dict.items():\n",
    "\n",
    "    pred_list_A_per_bin_dict, actual_list_A_per_bin_dict, pred_list_values_dict, actual_list_values_dict = calculate_correlations(attributes[\"path\"], \n",
    "                                                                                                                                  gene_pos_path,\n",
    "                                                                                                                                  figure_path, \n",
    "                                                                                                                                  input_table_path, \n",
    "                                                                                                                                N_BINS = attributes[\"N_BINS\"],\n",
    "                                                                                                                                  is_training=attributes[\"is_training\"],\n",
    "                                    \n",
    "                                                                                                                                  batch_num=attributes[\"batch_num\"])\n",
    "    for condition in condition_list:\n",
    "        # Prediction of all bins (1kbp resolution)\n",
    "        pred_list_values = np.log2(np.array(pred_list_values_dict[condition])+1) \n",
    "        actual_list_values = np.log2(np.array(actual_list_values_dict[condition])+1)\n",
    "        \n",
    "        pointwise = plot_correlations(figure_path, pred_list_values, actual_list_values, title + \"_pointwise_prediction\", cmap=cmap, binlims=(0,10))\n",
    "\n",
    "        # Central gene area (divided by gene length, equiv. to averaged EU-seq signal for the target gene)\n",
    "        pred_list_values = np.array(pred_list_A_per_bin_dict[condition]) \n",
    "        actual_list_values = np.array(actual_list_A_per_bin_dict[condition])\n",
    "        area = plot_correlations(figure_path, pred_list_values, actual_list_values, title + \"_gene_area_lengthnorm_prediction\", cmap=cmap, density=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize predictions of the different models:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputs:\n",
    "figures_path = Path(\"../figures/Figures_revisions/\")\n",
    "#ID = ENSMUSG00000090115.7_rev ENSMUSG00000043592.15_rev ENSMUSG00000023266.11_rev # Nice examples 'ENSMUSG00000079707.10_forward'\n",
    "val_ids = pd.read_csv(Path('../annotations/manual_validation_ids_chr17.txt')).values.flatten()\n",
    "\n",
    "for ID in val_ids[172::200]:#  [\"ENSMUSG00000090115.7_forward\", \"ENSMUSG00000043592.15_forward\", \"ENSMUSG00000023266.11_forward\"]: #val_ids[::8]:\n",
    "    predictions = []\n",
    "    actual_data = []\n",
    "    for k, (title, attributes) in  enumerate(results_path_dict.items()):\n",
    "        if \"training\" in title:\n",
    "            #print(title)\n",
    "            ids, predicted, actual = _get_predictions(attributes['path'],\n",
    "                                                    attributes['N_BINS'],\n",
    "                                                    [\"_ctrl\"], \n",
    "                                                    is_training=attributes['is_training'], \n",
    "                                                    batch_num=attributes['batch_num'])\n",
    "            \n",
    "            line_p = predicted.loc[ID]\n",
    "            predictions.append({'profile':line_p,\n",
    "                                'color': attributes['color'],\n",
    "                                'alpha': 0.2,\n",
    "                                'label': f\"Predicted {attributes['label']}\",\n",
    "                                'ls':'-'\n",
    "            })\n",
    "\n",
    "            EU_seq_colors = [\"k\",\"red\"]\n",
    "            # Actual baseline signal:\n",
    "            actual_data.append({\n",
    "                'profile': actual.loc[ID],\n",
    "                'color': EU_seq_colors[k],\n",
    "                'alpha': 0.2,\n",
    "                'label': f\"EU-seq {attributes['label']}\",\n",
    "                'ls':'-'\n",
    "            })\n",
    "\n",
    "    # Create plot\n",
    "    fig = plot_generalized_predictions(predictions, actual_data, N_BINS=200)\n",
    "    fig.savefig(figures_path / 'Outputs' / f'Model_predictions_{ID}.png', dpi=200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize attributions of the new models:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_path = Path(\"../figures/Figures_revisions/attributions/\")\n",
    "\n",
    "plt.rcParams.update({'font.size': 10})  # Adjust the size as needed\n",
    "\n",
    "def read_target_attributions(results_path: Path,\n",
    "                             figure_path: Path,\n",
    "                             track_dict: dict,\n",
    "                             N_BINS: int,\n",
    "                             l_in:int = 10001,\n",
    "                             n_in: int=4,\n",
    "                             TRAIN_ATTR: bool = True,\n",
    "                             SPLIT: int = None):\n",
    "\n",
    "    \"\"\"\n",
    "    This function is aimed to read attribution arrays from the chromatin landscape branch. Attribution arrays score the \n",
    "    contribution of each position in the input arrays (4,10.001) towards each output node/position (401).\n",
    "    Args:\n",
    "        results_path: path where the attribution arrays are stored.\n",
    "        figure_path: path where we want to store the outputs\n",
    "        track_dict: dictionary containing the names of the tracks and plot details\n",
    "        N_BINS: number of bins to one side of the central bin (200 normally)\n",
    "        l_in: input length \n",
    "        n_in: number of input channels\n",
    "        TRAIN_ATTR: whether this is a training or test run (folder structure changes).\n",
    "        SPLIT: if Train, we need to know which of the stored batches it is\n",
    "\n",
    "    Returns:\n",
    "        Plots with attribution scores.\n",
    "    \"\"\"\n",
    "    figure_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    N_IN_BINS = l_in // 2\n",
    "    x_val = np.arange(-N_IN_BINS,N_IN_BINS+1)\n",
    "    xlim = (-500,500)\n",
    "\n",
    "    # Getting attributions:\n",
    "    condition_list = [\"_ctrl\"]\n",
    "    landscape_avg = {condition : np.zeros((n_in,l_in)) for condition in condition_list}\n",
    "\n",
    "    for i in range(-N_BINS, N_BINS + 1):\n",
    "        for condition in condition_list:\n",
    "            # CHROMATIN MARK ATTRIBUTIONS:\n",
    "            added_folders = f\"/samples/{SPLIT}\" if TRAIN_ATTR else \"\" # Different folder structure\n",
    "            att_path = Path(results_path / f\"{i}{condition}{added_folders}/attributions/gene_expression/{i}{condition}.npy\")\n",
    "            if os.path.exists(att_path):\n",
    "                attributions = np.load(att_path)\n",
    "                #if (i == -73) and (condition == \"_ctrl\"):\n",
    "                fig, axs = plt.subplots(4, figsize=(8,2.7)) \n",
    "                for j,key in enumerate(track_dict.keys()):\n",
    "                    name = track_dict[key][\"name\"]\n",
    "                    axs[j].plot(x_val, abs(attributions[j]), label=name, lw=0, marker='o', markersize=0.8, markeredgecolor=\"none\", color=track_dict[key][\"color\"])\n",
    "                    axs[j].set_xlim((-5000,5001))\n",
    "                    axs[j].set_ylim((0,np.max(attributions[1])))\n",
    "                    axs[j].set_yticks([])\n",
    "                    axs[-1].set_yticks([0,np.max(attributions[1])],[0,1])\n",
    "                #fig.savefig(figure_path / f\"Chromatin_landscape_attributions_{i}_ctrl.png\",dpi=200)\n",
    "                plt.close(fig)\n",
    "\n",
    "                # Add a small phase to remove high frequency fluctuations:\n",
    "                eps = 10*np.random.rand(1)*np.random.choice([-1,1])\n",
    "                \n",
    "                centered_attr = np.roll(attributions, -10*i+int(eps),axis=1)\n",
    "                landscape_avg[condition] += 1/(2*N_BINS + 1)*(abs(centered_attr))\n",
    "\n",
    "    SCALING_FACTOR = np.max([landscape_avg[cond] for cond in condition_list])\n",
    "\n",
    "    for condition in condition_list:\n",
    "        fig, ax = plt.subplots(figsize=(8,2.7))   # \n",
    "        sub_axes = plt.axes([.615, .57, .27, .27])  \n",
    "        for i,key in enumerate(track_dict.keys()):\n",
    "            name = track_dict[key][\"name\"]\n",
    "            ax.plot(x_val, landscape_avg[condition][i]/SCALING_FACTOR, label=name, lw=0, marker='o', markersize=2, markeredgecolor=\"none\", color=track_dict[key][\"color\"])\n",
    "            sub_axes.plot(x_val, landscape_avg[condition][i]/SCALING_FACTOR, label=name, lw=0,marker='o',markersize=.5,markeredgecolor=\"none\", color=track_dict[key][\"color\"])\n",
    "        \n",
    "        #sub_axes.set_xlim(xlim)\n",
    "        sub_axes.set_xticks(np.arange(-5000,5001,5000),np.arange(-500,501,500))\n",
    "        sub_axes.set_xlim((-5000,5000))\n",
    "        sub_axes.set_yticks([])\n",
    "        ax.set_xlim(xlim)\n",
    "        ax.set_ylim(0,1) #(0,.002))\n",
    "        ax.legend(loc='upper left')\n",
    "        ax.set_ylabel(\"Attribution score\")\n",
    "        ax.set_xlabel(\"Distance to the predicted locus (kbp)\")\n",
    "        plt.subplots_adjust(bottom=0.2)\n",
    "        plt.tight_layout()\n",
    "        ax.set_xticks(np.arange(-400,401,100),np.arange(-40,41,10))\n",
    "        fig.savefig(figure_path / f\"Landscape_absolute_average_{condition}.png\", dpi=200)\n",
    "        plt.close(fig)\n",
    "\n",
    "# Visualize for the runs for which we have attributions available.\n",
    "for title, attributes in results_path_dict.items():\n",
    "    if attributes[\"attributions\"]:\n",
    "        if title == \"Only Chromatin Pure Convolutions no H3K27ac test\": # one input track less\n",
    "            track_dict = {0:{\"name\":\"ATAC-seq\",\"function\":\"Chromatin accessibility\",\"color\":\"k\"},\n",
    "            1:{\"name\":\"H3K4me3\",\"function\":\"Promoter\",\"color\":\"r\"},\n",
    "            2:{\"name\":\"H3K27me3\",\"function\":\"Chromatin silencing\",\"color\":\"g\"}}\n",
    "            \n",
    "            read_target_attributions(results_path = attributes[\"path\"] / \"expression_output\",\n",
    "                                figure_path=figure_path,\n",
    "                                track_dict=track_dict,\n",
    "                                N_BINS = attributes[\"N_BINS\"],\n",
    "                                l_in= 10001,\n",
    "                                n_in=3,\n",
    "                                TRAIN_ATTR = False,\n",
    "                                SPLIT = None)\n",
    "        else:\n",
    "            track_dict = {0:{\"name\":\"ATAC-seq\",\"function\":\"Chromatin accessibility\",\"color\":\"k\"},\n",
    "            1:{\"name\":\"H3K4me3\",\"function\":\"Promoter\",\"color\":\"r\"},\n",
    "            2:{\"name\":\"H3K27ac\",\"function\":\"Enhancer\",\"color\":\"b\"},\n",
    "            3:{\"name\":\"H3K27me3\",\"function\":\"Chromatin silencing\",\"color\":\"g\"}}\n",
    "            read_target_attributions(results_path = attributes[\"path\"] / \"expression_output\",\n",
    "                    figure_path=figure_path,\n",
    "                    track_dict=track_dict,\n",
    "                    N_BINS = attributes[\"N_BINS\"],\n",
    "                    l_in= 10001,\n",
    "                    n_in=4,\n",
    "                    TRAIN_ATTR = False,\n",
    "                    SPLIT = None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V) Performance evaluations:\n",
    "\n",
    "The following section is aimed to provide a more comprehensive and rigorous overview of CLASTER's predictive performance. \n",
    "\n",
    "- Performance is evaluated using $R^2$, $MSE$, $RMSE$, $MAE$, $Pearson$ and $Spearman$ for all CLASTER models. \n",
    "- Performance is reported before and after zero clipping using the ReLU activation.\n",
    "- Perfomance is reported with and without log2(1+x) transforming the clipped predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Showing that unclipped profiles show values close to zero.**\n",
    "\n",
    "Predictions below zero are not too negative. Therefore, the difference between clipped and unclipped profiles in terms of standard performance metrics is not that pronounced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_path = Path(\"../runs/test_runs/gene_expression_only_chrom_pure_conv/\")\n",
    "figures_path = Path(\"../figures/Figures_revisions/\")\n",
    "\n",
    "EXAMPLE_GENE = \"ENSMUSG00000003032.8_forward.npy\" #<- KLF4 # Akirin2: \"ENSMUSG00000028291.7_forward.npy\"\n",
    "\n",
    "input_id_list = [\"ENSMUSG00000028291.7_forward\"]\n",
    "\n",
    "# Outputs:\n",
    "ids, predicted, actual = _get_predictions(outputs_path, N_BINS=200, condition_list = [\"_ctrl\"], CLIP= False)\n",
    "\n",
    "for ID,line_p,line_a in zip(ids,predicted.values,actual.values):   \n",
    "    if ID in input_id_list:\n",
    "        line_p = predicted.loc[ID]\n",
    "        line_a = actual.loc[ID]\n",
    "        fig = plot_target_predictions(line_p, line_a)\n",
    "        fig.savefig(figures_path / 'Outputs' / f\"{ID}_prediction_unclipped.png\", dpi=200)\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculating new performance metrics:**\n",
    "\n",
    "- We will use parallel processing to speed up computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile create_evaluation_metrics_table.py\n",
    "#!/usr/bin/env python\n",
    "import multiprocessing as mp\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "def _get_predictions(\n",
    "    results_path: Path,\n",
    "    N_BINS: int,\n",
    "    condition_list: List[str],\n",
    "    is_training: bool = False,\n",
    "    batch_num: Optional[int] = None,\n",
    "    CLIP: bool = True\n",
    ") -> Tuple[List[str], pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Reads prediction files from EIR for each target bin condition.\n",
    "    \"\"\"\n",
    "    predicted_dfs = []\n",
    "    actual_dfs = []\n",
    "    \n",
    "    for condition in condition_list:\n",
    "        for i in range(-N_BINS, N_BINS + 1):\n",
    "            if is_training:\n",
    "                if batch_num is None:\n",
    "                    raise ValueError(\"batch_num must be specified for training predictions\")\n",
    "                file_path = results_path / f\"expression_output/{i}{condition}/samples/{batch_num}/regression_predictions.csv\"\n",
    "            else:\n",
    "                file_path = results_path / f\"expression_output/{i}{condition}/predictions.csv\"\n",
    "            \n",
    "            if not file_path.exists():\n",
    "                raise FileNotFoundError(f\"Prediction file not found: {file_path}\")\n",
    "                \n",
    "            predictions = pd.read_csv(file_path).set_index('ID')\n",
    "\n",
    "            # Apply ReLU to model predictions if specified\n",
    "            if CLIP:\n",
    "                predictions = predictions.clip(lower=0)\n",
    "\n",
    "            # Handle different column names for training vs test\n",
    "            if is_training:\n",
    "                predicted_column = predictions[\"Predicted\"].rename(f\"{i}{condition}\")\n",
    "                actual_column = predictions[\"Actual\"].rename(f\"{i}{condition}\")\n",
    "            else:\n",
    "                predicted_column = predictions[f\"{i}{condition} Untransformed\"].rename(f\"{i}{condition}\")\n",
    "                actual_column = predictions[\"True Label Untransformed\"].rename(f\"{i}{condition}\")\n",
    "\n",
    "            predicted_dfs.append(predicted_column)\n",
    "            actual_dfs.append(actual_column)\n",
    "\n",
    "    # Concatenate all DataFrames horizontally\n",
    "    predicted = pd.concat(predicted_dfs, axis=1)\n",
    "    actual = pd.concat(actual_dfs, axis=1)\n",
    "    ids = list(predicted.index)\n",
    "\n",
    "    return ids, predicted, actual\n",
    "\n",
    "def evaluate_model(args: Tuple[str, Dict[str, Any], bool]) -> Tuple[str, str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Evaluate a model using R2, MSE, RMSE, and MAE metrics\n",
    "    \"\"\"\n",
    "    title, model_config, is_clipping = args\n",
    "    state = \"clipped\" if is_clipping else \"linear\"\n",
    "    \n",
    "    # Extract parameters from model config\n",
    "    path = model_config[\"path\"]\n",
    "    is_training = model_config[\"is_training\"]\n",
    "    n_bins = model_config[\"N_BINS\"]\n",
    "    batch_num = model_config.get(\"batch_num\")\n",
    "    \n",
    "    try:\n",
    "        # Get predictions with the appropriate parameters\n",
    "        ids, predicted, actual = _get_predictions(\n",
    "            path, \n",
    "            N_BINS=n_bins, \n",
    "            condition_list=[\"_ctrl\"], \n",
    "            is_training=is_training, \n",
    "            batch_num=batch_num, \n",
    "            CLIP=is_clipping\n",
    "        )\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            f'R2_{state}': round(r2_score(actual.values, predicted.values), 4),\n",
    "            f'MSE_{state}': round(mean_squared_error(actual.values, predicted.values), 4),\n",
    "            f'RMSE_{state}': round(np.sqrt(mean_squared_error(actual.values, predicted.values)), 4),\n",
    "            f'MAE_{state}': round(mean_absolute_error(actual.values, predicted.values), 4)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating {title} ({state}): {str(e)}\")\n",
    "        metrics = {\n",
    "            f'R2_{state}': np.nan,\n",
    "            f'MSE_{state}': np.nan,\n",
    "            f'RMSE_{state}': np.nan,\n",
    "            f'MAE_{state}': np.nan\n",
    "        }\n",
    "    \n",
    "    return (title, state, metrics)\n",
    "\n",
    "def parallel_model_evaluation(models_dict: Dict[str, Dict[str, Any]], \n",
    "                           output_path: Path, \n",
    "                           n_processes: int = mp.cpu_count()-1) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluate multiple models in parallel\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    results_path = Path(output_path)\n",
    "    results_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Prepare arguments for parallel processing\n",
    "    tasks = []\n",
    "    for title, config in models_dict.items():\n",
    "        # Add task for linear evaluation\n",
    "        tasks.append((title, config, False))\n",
    "        # Add task for clipped evaluation\n",
    "        tasks.append((title, config, True))\n",
    "    \n",
    "    # Initialize DataFrame for results\n",
    "    r2_df = pd.DataFrame(\n",
    "        columns=['R2_linear', 'R2_clipped', 'MSE_linear', 'MSE_clipped', \n",
    "                'RMSE_linear', 'RMSE_clipped', 'MAE_linear', 'MAE_clipped'], \n",
    "        index=models_dict.keys()\n",
    "    )\n",
    "    \n",
    "    # Run evaluations in parallel\n",
    "    with mp.Pool(processes=n_processes) as pool:\n",
    "        results = pool.map(evaluate_model, tasks)\n",
    "    \n",
    "    # Process results\n",
    "    for title, state, metrics in results:\n",
    "        for metric_name, value in metrics.items():\n",
    "            r2_df.loc[title, metric_name] = value\n",
    "    \n",
    "    # Save results to file\n",
    "    r2_df.to_csv(results_path / \"performance_metrics_comparison.tsv\", sep=\"\\t\")\n",
    "    \n",
    "    return r2_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define paths\n",
    "    results_path = Path(\"../benchmarks/Revised_performance_metrics/\")\n",
    "    \n",
    "    # Define models to evaluate\n",
    "    results_path_dict = {\n",
    "        \"Only Chromatin Pure Convolutions training\": {\"path\": Path(\"../runs/gene_expression_only_chrom_pure_conv/results/\"),\n",
    "                                                      \"is_training\": True,\n",
    "                                                      \"N_BINS\": 200,\n",
    "                                                      \"batch_num\": 60600, \n",
    "                                                      \"attributions\": False,\n",
    "                                                      \"label\": \"Gaussian\",\n",
    "                                                      \"color\": \"blue\"},\n",
    "        \"Only Chromatin Pure Convolution test\": {\"path\": Path(\"../runs/test_runs/gene_expression_only_chrom_pure_conv/\"),\n",
    "                                                 \"is_training\": False,\n",
    "                                                 \"N_BINS\": 200,\n",
    "                                                 \"batch_num\": None,\n",
    "                                                 \"attributions\": True,\n",
    "                                                 \"label\": \"CLASTER test\",\n",
    "                                                 \"color\": \"blue\"},\n",
    "        \"Only Chromatin with Attention training\": {\"path\": Path(\"../runs/gene_expression_only_chrom_attention/results/\"),\n",
    "                                                  \"is_training\": True,\n",
    "                                                  \"N_BINS\": 200,\n",
    "                                                  \"batch_num\": 60600,\n",
    "                                                  \"attributions\": False,\n",
    "                                                  \"label\": \"Only chrom attention\",\n",
    "                                                  \"color\": \"blue\"},\n",
    "        \"Only Chromatin with Attention test\": {\"path\": Path(\"../runs/test_runs/gene_expression_only_chrom_attention/\"),\n",
    "                                              \"is_training\": False,\n",
    "                                              \"N_BINS\": 200,\n",
    "                                              \"batch_num\": None,\n",
    "                                              \"attributions\": True,\n",
    "                                              \"label\": \"Only chrom attention\",\n",
    "                                              \"color\": \"blue\"},\n",
    "        \"Chromatin and Structure training\": {\"path\": Path(\"../runs/gene_expression_microc_pure_conv/results/\"),\n",
    "                                            \"is_training\": True,\n",
    "                                            \"N_BINS\": 200,\n",
    "                                            \"batch_num\": 60600,\n",
    "                                            \"attributions\": False,\n",
    "                                            \"label\": \"Chrom and structure\",\n",
    "                                            \"color\": \"blue\"},\n",
    "        \"Chromatin and Structure test\": {\"path\": Path(\"../runs/test_runs/gene_expression_microc_pure_conv/\"),\n",
    "                                        \"is_training\": False,\n",
    "                                        \"N_BINS\": 200,\n",
    "                                        \"batch_num\": None,\n",
    "                                        \"attributions\": True,\n",
    "                                        \"label\": \"Chrom and Structure\",\n",
    "                                        \"color\": \"blue\"},\n",
    "        \"Chromatin and Structure rotated training\": {\"path\": Path(\"../runs/gene_expression_microc_rotated_pure_conv/results/\"),\n",
    "                                                   \"is_training\": True,\n",
    "                                                   \"N_BINS\": 200,\n",
    "                                                   \"batch_num\": 60600,\n",
    "                                                   \"attributions\": False,\n",
    "                                                   \"label\": \"Chrom and Structure rotated\",\n",
    "                                                   \"color\": \"blue\"},\n",
    "        \"Chromatin and Structure rotated test\": {\"path\": Path(\"../runs/test_runs/gene_expression_microc_rotated_pure_conv/\"),\n",
    "                                               \"is_training\": False,\n",
    "                                               \"N_BINS\": 200,\n",
    "                                               \"batch_num\": None,\n",
    "                                               \"attributions\": True,\n",
    "                                               \"label\": \"Chrom and Structure rotated\",\n",
    "                                               \"color\": \"blue\"},\n",
    "        \"Only Chromatin Pure Convolutions Softplus SmoothL1 training\": {\"path\": Path(\"../runs/gene_expression_only_chrom_pure_conv_softplus_no_bias/results/\"),\n",
    "                                                                       \"is_training\": True,\n",
    "                                                                       \"N_BINS\": 200,\n",
    "                                                                       \"batch_num\": 60600,\n",
    "                                                                       \"attributions\": False,\n",
    "                                                                       \"label\": \"train Softplus SmoothL1\",\n",
    "                                                                       \"color\": \"purple\"},\n",
    "        \"Only Chromatin Pure Convolutions Softplus Poisson training\": {\"path\": Path(\"../runs/gene_expression_only_chrom_pure_conv_softplus_poisson_no_bias/results/\"),\n",
    "                                                                     \"is_training\": True,\n",
    "                                                                     \"N_BINS\": 200,\n",
    "                                                                     \"batch_num\": 60600,\n",
    "                                                                     \"attributions\": False,\n",
    "                                                                     \"label\": \"train Softplus Poisson\",\n",
    "                                                                     \"color\": \"orange\"},\n",
    "        \"Only Chromatin Pure Convolutions Softplus Poisson log_input=False training\": {\"path\": Path(\"../runs/gene_expression_only_chrom_pure_conv_softplus_poisson_no_bias_log_input_false/results/\"),\n",
    "                                                                                      \"is_training\": True,\n",
    "                                                                                      \"N_BINS\": 200,\n",
    "                                                                                      \"batch_num\": 60600,\n",
    "                                                                                      \"attributions\": False,\n",
    "                                                                                      \"label\": \"train Softplus Poisson log input false\",\n",
    "                                                                                      \"color\": \"darkred\"},\n",
    "        \"Only Chromatin Pure Convolutions 20kbp context training\": {\"path\": Path(\"../runs/gene_expression_only_chrom_pure_conv_20kbp_context/results/\"),\n",
    "                                                                  \"is_training\": True,\n",
    "                                                                  \"N_BINS\": 9,\n",
    "                                                                  \"batch_num\": 30300,\n",
    "                                                                  \"attributions\": False,\n",
    "                                                                  \"label\": \"train 20kb context\",\n",
    "                                                                  \"color\": \"yellow\"},\n",
    "        \"Only Chromatin Pure Convolutions 20kbp context test\": {\"path\": Path(\"../runs/test_runs/gene_expression_only_chrom_pure_conv_20kbp_context_test\"),\n",
    "                                                              \"is_training\": False,\n",
    "                                                              \"N_BINS\": 9,\n",
    "                                                              \"batch_num\": None,\n",
    "                                                              \"attributions\": False,\n",
    "                                                              \"label\": \"test 20kb context\",\n",
    "                                                              \"color\": \"green\"},\n",
    "        \"Only Chromatin Pure Convolutions chr19 holdout training\": {\"path\": Path(\"../runs/gene_expression_only_chrom_pure_conv_chr19_holdout/results/\"),\n",
    "                                                                  \"is_training\": True,\n",
    "                                                                  \"N_BINS\": 200,\n",
    "                                                                  \"batch_num\": 30300,\n",
    "                                                                  \"attributions\": False,\n",
    "                                                                  \"label\": \"train chr19 holdout\",\n",
    "                                                                  \"color\": \"lightblue\"},\n",
    "        \"Only Chromatin Pure Convolutions chr19 holdout test\": {\"path\": Path(\"../runs/test_runs/gene_expression_only_chrom_pure_conv_chr19_holdout_test/\"),\n",
    "                                                              \"is_training\": False,\n",
    "                                                              \"N_BINS\": 200,\n",
    "                                                              \"batch_num\": None,\n",
    "                                                              \"attributions\": True,\n",
    "                                                              \"label\": \"test chr19 holdout\",\n",
    "                                                              \"color\": \"lightblue\"},\n",
    "        \"Only Chromatin Pure Convolutions no H3K27ac training\": {\"path\": Path(\"../runs/gene_expression_only_chrom_pure_conv_no_H3K27ac/results/\"),\n",
    "                                                               \"is_training\": True,\n",
    "                                                               \"N_BINS\": 200,\n",
    "                                                               \"batch_num\": 60600,\n",
    "                                                               \"attributions\": False,\n",
    "                                                               \"label\": \"train no H3K27ac\",\n",
    "                                                               \"color\": \"darkgreen\"},\n",
    "        \"Only Chromatin Pure Convolutions no H3K27ac test\": {\"path\": Path(\"../runs/test_runs/gene_expression_only_chrom_pure_conv_no_H3K27ac_test/\"),\n",
    "                                                           \"is_training\": False,\n",
    "                                                           \"N_BINS\": 200,\n",
    "                                                           \"batch_num\": None,\n",
    "                                                           \"attributions\": True,\n",
    "                                                           \"label\": \"test no H3K27ac\",\n",
    "                                                           \"color\": \"darkgreen\"},\n",
    "        \"Only Chromatin Pure Convolutions 1kbp binning training\": {\"path\": Path(\"../runs/gene_expression_only_chrom_pure_conv_1kbp_binning/results/\"),\n",
    "                                                                 \"is_training\": True,\n",
    "                                                                 \"N_BINS\": 200,\n",
    "                                                                 \"batch_num\": 60600,\n",
    "                                                                 \"attributions\": False,\n",
    "                                                                 \"label\": \"1kbp binning\",\n",
    "                                                                 \"color\": \"darkorange\"},\n",
    "        \"Only Chromatin Pure Convolutions 1kbp binning test\": {\"path\": Path(\"../runs/test_runs/gene_expression_only_chrom_pure_conv_1kbp_binning_test/\"),\n",
    "                                                             \"is_training\": False,\n",
    "                                                             \"N_BINS\": 200,\n",
    "                                                             \"batch_num\": None,\n",
    "                                                             \"attributions\": True,\n",
    "                                                             \"label\": \"test 1kbp binning\",\n",
    "                                                             \"color\": \"darkblue\"},\n",
    "        \"Only Chromatin Pure Convolutions K562 RNA-seq training\": {\"path\": Path(\"../runs/gene_expression_only_chrom_K562_train/results/\"),\n",
    "                                                                 \"is_training\": True,\n",
    "                                                                 \"N_BINS\": 200,\n",
    "                                                                 \"batch_num\": 60600,\n",
    "                                                                 \"attributions\": False,\n",
    "                                                                 \"label\": \"K562 RNA-seq\",\n",
    "                                                                 \"color\": \"darkorange\"},\n",
    "        \"Only Chromatin Pure Convolutions K562 POLR2A training\": {\"path\": Path(\"../runs/gene_expression_only_chrom_K562_POLR2A_train/results/\"),\n",
    "                                                                \"is_training\": True,\n",
    "                                                                \"N_BINS\": 200,\n",
    "                                                                \"batch_num\": 30300,\n",
    "                                                                \"attributions\": False,\n",
    "                                                                \"label\": \"K562 POLR2A\",\n",
    "                                                                \"color\": \"darkblue\"},\n",
    "        \"Chromatin and Prom-CHiC training\": {\"path\": Path(\"../runs/gene_expression_prom_CHiC_pure_conv/results/\"),\n",
    "                                            \"is_training\": True,\n",
    "                                            \"N_BINS\": 200,\n",
    "                                            \"batch_num\": 60600,\n",
    "                                            \"attributions\": False,\n",
    "                                            \"label\": \"Prom-CHiC\",\n",
    "                                            \"color\": \"blue\"},\n",
    "        \"Chromatin and Prom-CHiC test\": {\"path\": Path(\"../runs/test_runs/gene_expression_prom_CHiC_pure_conv_test/\"),\n",
    "                                        \"is_training\": False,\n",
    "                                        \"N_BINS\": 200,\n",
    "                                        \"batch_num\": None,\n",
    "                                        \"attributions\": True,\n",
    "                                        \"label\": \"Prom CHiC test\",\n",
    "                                        \"color\": \"darkblue\"}\n",
    "    }\n",
    "    \n",
    "    # Run evaluation with parallel processing\n",
    "    results = parallel_model_evaluation(results_path_dict, results_path)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding Pearson and Spearman correlations:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile create_evaluation_metrics_table.py\n",
    "#!/usr/bin/env python\n",
    "import multiprocessing as mp\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "def _get_predictions(\n",
    "    results_path: Path,\n",
    "    N_BINS: int,\n",
    "    condition_list: List[str],\n",
    "    is_training: bool = False,\n",
    "    batch_num: Optional[int] = None,\n",
    "    CLIP: bool = True,\n",
    "    include_rev: bool = False\n",
    ") -> Tuple[List[str], pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Reads prediction files from EIR for each target bin condition.\n",
    "    \n",
    "    Args:\n",
    "        results_path: Path to the results directory\n",
    "        N_BINS: Number of bins to read\n",
    "        condition_list: List of conditions to read\n",
    "        is_training: Whether to read training or test predictions\n",
    "        batch_num: Batch number for training predictions\n",
    "        CLIP: Whether to clip predictions to non-negative values\n",
    "        include_rev: Whether to include reversed samples (for data augmentation)\n",
    "    \"\"\"\n",
    "    predicted_dfs = []\n",
    "    actual_dfs = []\n",
    "    \n",
    "    for condition in condition_list:\n",
    "        for i in range(-N_BINS, N_BINS + 1):\n",
    "            if is_training:\n",
    "                if batch_num is None:\n",
    "                    raise ValueError(\"batch_num must be specified for training predictions\")\n",
    "                file_path = results_path / f\"expression_output/{i}{condition}/samples/{batch_num}/regression_predictions.csv\"\n",
    "            else:\n",
    "                file_path = results_path / f\"expression_output/{i}{condition}/predictions.csv\"\n",
    "            \n",
    "            if not file_path.exists():\n",
    "                raise FileNotFoundError(f\"Prediction file not found: {file_path}\")\n",
    "                \n",
    "            predictions = pd.read_csv(file_path).set_index('ID')\n",
    "            \n",
    "            # Filter out samples with IDs containing \"_rev\" if requested\n",
    "            if not include_rev:\n",
    "                predictions = predictions[~predictions.index.str.contains(\"_rev\")]\n",
    "\n",
    "            # Apply ReLU to model predictions if specified\n",
    "            if CLIP:\n",
    "                predictions = predictions.clip(lower=0)\n",
    "\n",
    "            # Handle different column names for training vs test\n",
    "            if is_training:\n",
    "                predicted_column = predictions[\"Predicted\"].rename(f\"{i}{condition}\")\n",
    "                actual_column = predictions[\"Actual\"].rename(f\"{i}{condition}\")\n",
    "            else:\n",
    "                predicted_column = predictions[f\"{i}{condition} Untransformed\"].rename(f\"{i}{condition}\")\n",
    "                actual_column = predictions[\"True Label Untransformed\"].rename(f\"{i}{condition}\")\n",
    "\n",
    "            predicted_dfs.append(predicted_column)\n",
    "            actual_dfs.append(actual_column)\n",
    "\n",
    "    # Concatenate all DataFrames horizontally\n",
    "    predicted = pd.concat(predicted_dfs, axis=1)\n",
    "    actual = pd.concat(actual_dfs, axis=1)\n",
    "    ids = list(predicted.index)\n",
    "\n",
    "    return ids, predicted, actual\n",
    "\n",
    "def evaluate_model(args: Tuple[str, Dict[str, Any], bool]) -> Tuple[str, str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Evaluate a model using R2, MSE, RMSE, MAE, Spearman correlation, and Pearson correlation metrics\n",
    "    \"\"\"\n",
    "    title, model_config, is_clipping = args\n",
    "    state = \"clipped\" if is_clipping else \"linear\"\n",
    "    \n",
    "    # Extract parameters from model config\n",
    "    path = model_config[\"path\"]\n",
    "    is_training = model_config[\"is_training\"]\n",
    "    n_bins = model_config[\"N_BINS\"]\n",
    "    batch_num = model_config.get(\"batch_num\")\n",
    "    \n",
    "    try:\n",
    "        # Get predictions with the appropriate parameters\n",
    "        ids, predicted, actual = _get_predictions(\n",
    "            path, \n",
    "            N_BINS=n_bins, \n",
    "            condition_list=[\"_ctrl\"], \n",
    "            is_training=is_training, \n",
    "            batch_num=batch_num, \n",
    "            CLIP=is_clipping\n",
    "        )\n",
    "        \n",
    "        # Flatten matrices for correlation calculations\n",
    "        actual_flat = actual.values.flatten()\n",
    "        predicted_flat = predicted.values.flatten()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            f'R2_{state}': round(r2_score(actual.values, predicted.values), 4),\n",
    "            f'MSE_{state}': round(mean_squared_error(actual.values, predicted.values), 4),\n",
    "            f'RMSE_{state}': round(np.sqrt(mean_squared_error(actual.values, predicted.values)), 4),\n",
    "            f'MAE_{state}': round(mean_absolute_error(actual.values, predicted.values), 4),\n",
    "            f'Spearman_{state}': round(spearmanr(actual_flat, predicted_flat)[0], 4),\n",
    "            f'Pearson_{state}': round(pearsonr(actual_flat, predicted_flat)[0], 4)\n",
    "        }\n",
    "\n",
    "        # Calculate metrics log2(x+1): \n",
    "        # metrics = {\n",
    "        #     f'R2_{state}': round(r2_score(np.log2(actual.values+1), np.log2(predicted.values+1)), 4),\n",
    "        #     f'MSE_{state}': round(mean_squared_error(np.log2(actual.values+1), np.log2(predicted.values+1)), 4),\n",
    "        #     f'RMSE_{state}': round(np.sqrt(mean_squared_error(np.log2(actual.values+1), np.log2(predicted.values+1))), 4),\n",
    "        #     f'MAE_{state}': round(mean_absolute_error(np.log2(actual.values+1), np.log2(predicted.values+1)), 4),\n",
    "        #     f'Spearman_{state}': round(spearmanr(np.log2(actual_flat+1), np.log2(predicted_flat+1))[0], 4),\n",
    "        #     f'Pearson_{state}': round(pearsonr(np.log2(actual_flat+1), np.log2(predicted_flat+1))[0], 4)\n",
    "        # }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating {title} ({state}): {str(e)}\")\n",
    "        metrics = {\n",
    "            f'R2_{state}': np.nan,\n",
    "            f'MSE_{state}': np.nan,\n",
    "            f'RMSE_{state}': np.nan,\n",
    "            f'MAE_{state}': np.nan,\n",
    "            f'Spearman_{state}': np.nan,\n",
    "            f'Pearson_{state}': np.nan\n",
    "        }\n",
    "    \n",
    "    return (title, state, metrics)\n",
    "\n",
    "def parallel_model_evaluation(models_dict: Dict[str, Dict[str, Any]], \n",
    "                           output_path: Path, \n",
    "                           n_processes: int = mp.cpu_count()-1) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluate multiple models in parallel\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    results_path = Path(output_path)\n",
    "    results_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Prepare arguments for parallel processing\n",
    "    tasks = []\n",
    "    for title, config in models_dict.items():\n",
    "        # Add task for linear evaluation\n",
    "        tasks.append((title, config, False))\n",
    "        # Add task for clipped evaluation\n",
    "        tasks.append((title, config, True))\n",
    "    \n",
    "    # Initialize DataFrame for results\n",
    "    r2_df = pd.DataFrame(\n",
    "        columns=['R2_linear', 'R2_clipped', 'MSE_linear', 'MSE_clipped', \n",
    "                'RMSE_linear', 'RMSE_clipped', 'MAE_linear', 'MAE_clipped',\n",
    "                'Spearman_linear', 'Spearman_clipped', 'Pearson_linear', 'Pearson_clipped'], \n",
    "        index=models_dict.keys()\n",
    "    )\n",
    "    \n",
    "    # Run evaluations in parallel\n",
    "    with mp.Pool(processes=n_processes) as pool:\n",
    "        results = pool.map(evaluate_model, tasks)\n",
    "    \n",
    "    # Process results\n",
    "    for title, state, metrics in results:\n",
    "        for metric_name, value in metrics.items():\n",
    "            r2_df.loc[title, metric_name] = value\n",
    "    \n",
    "    # Save results to file\n",
    "    r2_df.to_csv(results_path / \"performance_metrics_comparison.tsv\", sep=\"\\t\")\n",
    "    \n",
    "    return r2_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define paths\n",
    "    results_path = Path(\"../benchmarks/Revised_performance_metrics/\")\n",
    "    \n",
    "    # Define models to evaluate\n",
    "    results_path_dict = {\n",
    "        \"Only Chromatin Pure Convolutions training\": {\"path\": Path(\"../runs/gene_expression_only_chrom_pure_conv/results/\"),\n",
    "                                                      \"is_training\": True,\n",
    "                                                      \"N_BINS\": 200,\n",
    "                                                      \"batch_num\": 60600, \n",
    "                                                      \"attributions\": False,\n",
    "                                                      \"label\": \"Gaussian\",\n",
    "                                                      \"color\": \"blue\"},\n",
    "        \"Only Chromatin Pure Convolution test\": {\"path\": Path(\"../runs/test_runs/gene_expression_only_chrom_pure_conv/\"),\n",
    "                                                 \"is_training\": False,\n",
    "                                                 \"N_BINS\": 200,\n",
    "                                                 \"batch_num\": None,\n",
    "                                                 \"attributions\": True,\n",
    "                                                 \"label\": \"CLASTER test\",\n",
    "                                                 \"color\": \"blue\"},\n",
    "        \"Only Chromatin with Attention training\": {\"path\": Path(\"../runs/gene_expression_only_chrom_attention/results/\"),\n",
    "                                                  \"is_training\": True,\n",
    "                                                  \"N_BINS\": 200,\n",
    "                                                  \"batch_num\": 60600,\n",
    "                                                  \"attributions\": False,\n",
    "                                                  \"label\": \"Only chrom attention\",\n",
    "                                                  \"color\": \"blue\"},\n",
    "        \"Only Chromatin with Attention test\": {\"path\": Path(\"../runs/test_runs/gene_expression_only_chrom_attention/\"),\n",
    "                                              \"is_training\": False,\n",
    "                                              \"N_BINS\": 200,\n",
    "                                              \"batch_num\": None,\n",
    "                                              \"attributions\": True,\n",
    "                                              \"label\": \"Only chrom attention\",\n",
    "                                              \"color\": \"blue\"},\n",
    "        \"Chromatin and Structure training\": {\"path\": Path(\"../runs/gene_expression_microc_pure_conv/results/\"),\n",
    "                                            \"is_training\": True,\n",
    "                                            \"N_BINS\": 200,\n",
    "                                            \"batch_num\": 60600,\n",
    "                                            \"attributions\": False,\n",
    "                                            \"label\": \"Chrom and structure\",\n",
    "                                            \"color\": \"blue\"},\n",
    "        \"Chromatin and Structure test\": {\"path\": Path(\"../runs/test_runs/gene_expression_microc_pure_conv/\"),\n",
    "                                        \"is_training\": False,\n",
    "                                        \"N_BINS\": 200,\n",
    "                                        \"batch_num\": None,\n",
    "                                        \"attributions\": True,\n",
    "                                        \"label\": \"Chrom and Structure\",\n",
    "                                        \"color\": \"blue\"},\n",
    "        \"Chromatin and Structure rotated training\": {\"path\": Path(\"../runs/gene_expression_microc_rotated_pure_conv/results/\"),\n",
    "                                                   \"is_training\": True,\n",
    "                                                   \"N_BINS\": 200,\n",
    "                                                   \"batch_num\": 60600,\n",
    "                                                   \"attributions\": False,\n",
    "                                                   \"label\": \"Chrom and Structure rotated\",\n",
    "                                                   \"color\": \"blue\"},\n",
    "        \"Chromatin and Structure rotated test\": {\"path\": Path(\"../runs/test_runs/gene_expression_microc_rotated_pure_conv/\"),\n",
    "                                               \"is_training\": False,\n",
    "                                               \"N_BINS\": 200,\n",
    "                                               \"batch_num\": None,\n",
    "                                               \"attributions\": True,\n",
    "                                               \"label\": \"Chrom and Structure rotated\",\n",
    "                                               \"color\": \"blue\"},\n",
    "        \"Only Chromatin Pure Convolutions Softplus SmoothL1 training\": {\"path\": Path(\"../runs/gene_expression_only_chrom_pure_conv_softplus_no_bias/results/\"),\n",
    "                                                                       \"is_training\": True,\n",
    "                                                                       \"N_BINS\": 200,\n",
    "                                                                       \"batch_num\": 60600,\n",
    "                                                                       \"attributions\": False,\n",
    "                                                                       \"label\": \"train Softplus SmoothL1\",\n",
    "                                                                       \"color\": \"purple\"},\n",
    "        \"Only Chromatin Pure Convolutions Softplus Poisson training\": {\"path\": Path(\"../runs/gene_expression_only_chrom_pure_conv_softplus_poisson_no_bias/results/\"),\n",
    "                                                                     \"is_training\": True,\n",
    "                                                                     \"N_BINS\": 200,\n",
    "                                                                     \"batch_num\": 60600,\n",
    "                                                                     \"attributions\": False,\n",
    "                                                                     \"label\": \"train Softplus Poisson\",\n",
    "                                                                     \"color\": \"orange\"},\n",
    "        \"Only Chromatin Pure Convolutions Softplus Poisson log_input=False training\": {\"path\": Path(\"../runs/gene_expression_only_chrom_pure_conv_softplus_poisson_no_bias_log_input_false/results/\"),\n",
    "                                                                                      \"is_training\": True,\n",
    "                                                                                      \"N_BINS\": 200,\n",
    "                                                                                      \"batch_num\": 60600,\n",
    "                                                                                      \"attributions\": False,\n",
    "                                                                                      \"label\": \"train Softplus Poisson log input false\",\n",
    "                                                                                      \"color\": \"darkred\"},\n",
    "        \"Only Chromatin Pure Convolutions 20kbp context training\": {\"path\": Path(\"../runs/gene_expression_only_chrom_pure_conv_20kbp_context/results/\"),\n",
    "                                                                  \"is_training\": True,\n",
    "                                                                  \"N_BINS\": 9,\n",
    "                                                                  \"batch_num\": 30300,\n",
    "                                                                  \"attributions\": False,\n",
    "                                                                  \"label\": \"train 20kb context\",\n",
    "                                                                  \"color\": \"yellow\"},\n",
    "        \"Only Chromatin Pure Convolutions 20kbp context test\": {\"path\": Path(\"../runs/test_runs/gene_expression_only_chrom_pure_conv_20kbp_context_test\"),\n",
    "                                                              \"is_training\": False,\n",
    "                                                              \"N_BINS\": 9,\n",
    "                                                              \"batch_num\": None,\n",
    "                                                              \"attributions\": False,\n",
    "                                                              \"label\": \"test 20kb context\",\n",
    "                                                              \"color\": \"green\"},\n",
    "        \"Only Chromatin Pure Convolutions chr19 holdout training\": {\"path\": Path(\"../runs/gene_expression_only_chrom_pure_conv_chr19_holdout/results/\"),\n",
    "                                                                  \"is_training\": True,\n",
    "                                                                  \"N_BINS\": 200,\n",
    "                                                                  \"batch_num\": 30300,\n",
    "                                                                  \"attributions\": False,\n",
    "                                                                  \"label\": \"train chr19 holdout\",\n",
    "                                                                  \"color\": \"lightblue\"},\n",
    "        \"Only Chromatin Pure Convolutions chr19 holdout test\": {\"path\": Path(\"../runs/test_runs/gene_expression_only_chrom_pure_conv_chr19_holdout_test/\"),\n",
    "                                                              \"is_training\": False,\n",
    "                                                              \"N_BINS\": 200,\n",
    "                                                              \"batch_num\": None,\n",
    "                                                              \"attributions\": True,\n",
    "                                                              \"label\": \"test chr19 holdout\",\n",
    "                                                              \"color\": \"lightblue\"},\n",
    "        \"Only Chromatin Pure Convolutions no H3K27ac training\": {\"path\": Path(\"../runs/gene_expression_only_chrom_pure_conv_no_H3K27ac/results/\"),\n",
    "                                                               \"is_training\": True,\n",
    "                                                               \"N_BINS\": 200,\n",
    "                                                               \"batch_num\": 60600,\n",
    "                                                               \"attributions\": False,\n",
    "                                                               \"label\": \"train no H3K27ac\",\n",
    "                                                               \"color\": \"darkgreen\"},\n",
    "        \"Only Chromatin Pure Convolutions no H3K27ac test\": {\"path\": Path(\"../runs/test_runs/gene_expression_only_chrom_pure_conv_no_H3K27ac_test/\"),\n",
    "                                                           \"is_training\": False,\n",
    "                                                           \"N_BINS\": 200,\n",
    "                                                           \"batch_num\": None,\n",
    "                                                           \"attributions\": True,\n",
    "                                                           \"label\": \"test no H3K27ac\",\n",
    "                                                           \"color\": \"darkgreen\"},\n",
    "        \"Only Chromatin Pure Convolutions 1kbp binning training\": {\"path\": Path(\"../runs/gene_expression_only_chrom_pure_conv_1kbp_binning/results/\"),\n",
    "                                                                 \"is_training\": True,\n",
    "                                                                 \"N_BINS\": 200,\n",
    "                                                                 \"batch_num\": 60600,\n",
    "                                                                 \"attributions\": False,\n",
    "                                                                 \"label\": \"1kbp binning\",\n",
    "                                                                 \"color\": \"darkorange\"},\n",
    "        \"Only Chromatin Pure Convolutions 1kbp binning test\": {\"path\": Path(\"../runs/test_runs/gene_expression_only_chrom_pure_conv_1kbp_binning_test/\"),\n",
    "                                                             \"is_training\": False,\n",
    "                                                             \"N_BINS\": 200,\n",
    "                                                             \"batch_num\": None,\n",
    "                                                             \"attributions\": True,\n",
    "                                                             \"label\": \"test 1kbp binning\",\n",
    "                                                             \"color\": \"darkblue\"},\n",
    "        \"Only Chromatin Pure Convolutions K562 RNA-seq training\": {\"path\": Path(\"../runs/gene_expression_only_chrom_K562_train/results/\"),\n",
    "                                                                 \"is_training\": True,\n",
    "                                                                 \"N_BINS\": 200,\n",
    "                                                                 \"batch_num\": 60600,\n",
    "                                                                 \"attributions\": False,\n",
    "                                                                 \"label\": \"K562 RNA-seq\",\n",
    "                                                                 \"color\": \"darkorange\"},\n",
    "        \"Only Chromatin Pure Convolutions K562 POLR2A training\": {\"path\": Path(\"../runs/gene_expression_only_chrom_K562_POLR2A_train/results/\"),\n",
    "                                                                \"is_training\": True,\n",
    "                                                                \"N_BINS\": 200,\n",
    "                                                                \"batch_num\": 30300,\n",
    "                                                                \"attributions\": False,\n",
    "                                                                \"label\": \"K562 POLR2A\",\n",
    "                                                                \"color\": \"darkblue\"},\n",
    "        \"Chromatin and Prom-CHiC training\": {\"path\": Path(\"../runs/gene_expression_prom_CHiC_pure_conv/results/\"),\n",
    "                                            \"is_training\": True,\n",
    "                                            \"N_BINS\": 200,\n",
    "                                            \"batch_num\": 60600,\n",
    "                                            \"attributions\": False,\n",
    "                                            \"label\": \"Prom-CHiC\",\n",
    "                                            \"color\": \"blue\"},\n",
    "        \"Chromatin and Prom-CHiC test\": {\"path\": Path(\"../runs/test_runs/gene_expression_prom_CHiC_pure_conv_test/\"),\n",
    "                                        \"is_training\": False,\n",
    "                                        \"N_BINS\": 200,\n",
    "                                        \"batch_num\": None,\n",
    "                                        \"attributions\": True,\n",
    "                                        \"label\": \"Prom CHiC test\",\n",
    "                                        \"color\": \"darkblue\"}\n",
    "    }\n",
    "    \n",
    "    # Run evaluation with parallel processing\n",
    "    results = parallel_model_evaluation(results_path_dict, results_path)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  VI) Data distributions \n",
    "\n",
    "**Visualizing data distributions:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile create_data_distribution_plot.py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "plt.rcParams['font.family'] = 'Nimbus Roman'\n",
    "\n",
    "savepath = Path(\"../figures/Figures_revisions/data_distributions_K562/\")\n",
    "savepath.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "def load_single_file(filename, path, marks):\n",
    "    a = np.load(path / filename)\n",
    "    if True in np.isnan(a):\n",
    "        print(filename)\n",
    "    return {mark: a[i,:] for i, mark in enumerate(marks)}\n",
    "\n",
    "def load_data(path):\n",
    "    marks = [\"ATAC\", \"H3K4me3\", \"H3K27ac\", \"H3K27me3\"]\n",
    "    filenames = [array.name for array in os.scandir(path)]\n",
    "    \n",
    "    load_func = partial(load_single_file, path=path, marks=marks)\n",
    "    \n",
    "    n_cores = cpu_count() - 1\n",
    "    with Pool(processes=n_cores) as pool:\n",
    "        results = pool.map(load_func, filenames)\n",
    "    \n",
    "    combined = {mark: [] for mark in marks}\n",
    "    for result in results:\n",
    "        for mark in marks:\n",
    "            combined[mark].append(result[mark])\n",
    "            \n",
    "    return {mark: np.concatenate(arrays) for mark, arrays in combined.items()}\n",
    "\n",
    "# Load Micro-C data - using same loading function but with single mark\n",
    "def load_microc_data(path):\n",
    "    filenames = [array.name for array in os.scandir(path)]\n",
    "    \n",
    "    load_func = partial(load_single_file, path=path, marks=[\"prom-CHiC\"])\n",
    "    \n",
    "    n_cores = cpu_count() - 1\n",
    "    with Pool(processes=n_cores) as pool:\n",
    "        results = pool.map(load_func, filenames)\n",
    "    \n",
    "    combined = {\"prom-CHiC\": []}\n",
    "    for result in results:\n",
    "        combined[\"prom-CHiC\"].append(result[\"prom-CHiC\"])\n",
    "            \n",
    "    return {\"prom-CHiC\": np.concatenate(arrays) for mark, arrays in combined.items()}\n",
    "\n",
    "# Define paths\n",
    "training_inputs_path = Path(\"../inputs/landscape_arrays/K562/training/\")\n",
    "test_inputs_path = Path(\"../inputs/landscape_arrays/K562/test/\")\n",
    "#training_microc_path = Path(\"../inputs/microC/training/\")\n",
    "#test_microc_path = Path(\"../inputs/microC/test/\")\n",
    "training_microc_path = Path(\"../inputs/Promoter-CHiC/training/\")\n",
    "test_microc_path = Path(\"../inputs/Promoter-CHiC/test/\")\n",
    "\n",
    "# Load data in parallel\n",
    "train = load_data(training_inputs_path)\n",
    "test = load_data(test_inputs_path)\n",
    "\n",
    "# Load EU-seq data from CSV files\n",
    "train_euseq = pd.read_csv(\"../targets/K562/training_targets.csv\",  index_col='ID').values.flatten()\n",
    "test_euseq = pd.read_csv(\"../targets/K562/test_targets.csv\",  index_col='ID').values.flatten()\n",
    "# Load Micro-C data\n",
    "train_microc = load_microc_data(training_microc_path)\n",
    "test_microc = load_microc_data(test_microc_path)\n",
    "\n",
    "# Create and plot subplots (2x3 grid to accommodate the EU-seq plot)\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "marks = [\"ATAC\", \"H3K4me3\", \"H3K27ac\", \"H3K27me3\"]\n",
    "\n",
    "# Define custom bounds for each mark\n",
    "bounds = {\n",
    "    \"ATAC\": (0, 1000),\n",
    "    \"H3K4me3\": (0, 1000),\n",
    "    \"H3K27ac\": (0, 1000),\n",
    "    \"H3K27me3\": (0, 1000),\n",
    "    #\"EU-seq\": (0, 400),\n",
    "    'RNA-seq':(0,1000),\n",
    "    #\"POLR2A\": (0,1000),\n",
    "    #\"Micro-C\": (-15,2),\n",
    "    \"prom-CHiC\":(-15,2)\n",
    "}\n",
    "\n",
    "N_BINS = 200\n",
    "\n",
    "eu_seq_ax = axes.flat[4]  # Move EU-seq to position 5\n",
    "\n",
    "for (mark, ax) in zip(marks, axes.flat):\n",
    "    min_bound, max_bound = bounds[mark]\n",
    "    bins = np.linspace(min_bound, max_bound, N_BINS+1)  # Wider bins (step of 2 instead of 0.5)\n",
    "    \n",
    "    ax.hist(train[mark], bins=bins, alpha=0.5, \n",
    "            label=f'Train\\nMin: {int(np.min(train[mark]))}\\nMax: {int(np.max(train[mark]))}\\nMedian: {int(np.median(train[mark]))}', \n",
    "            density=True)\n",
    "    ax.hist(test[mark], bins=bins, alpha=0.5, \n",
    "            label=f'Test\\nMin: {int(np.min(test[mark]))}\\nMax: {int(np.max(test[mark]))}\\nMedian: {int(np.median(test[mark]))}', \n",
    "            density=True)\n",
    "\n",
    "    ax.set_xlim(min_bound, max_bound)\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_aspect('auto', adjustable='box')\n",
    "    \n",
    "    ax.set_title(mark)\n",
    "    ax.legend(bbox_to_anchor=(.8, .999), loc='upper center', ncol=1)\n",
    "\n",
    "# Plot EU-seq distributions\n",
    "min_bound, max_bound = bounds[\"RNA-seq\"]\n",
    "bins_euseq = np.linspace(min_bound, max_bound,  N_BINS+1)  # Same wider bins for consistency\n",
    "\n",
    "eu_seq_ax.hist(train_euseq, bins=bins_euseq, alpha=0.5,\n",
    "               label=f'Train\\nMin: {int(np.min(train_euseq))}\\nMax: {int(np.max(train_euseq))}\\nMedian: {int(np.median(train_euseq))}',\n",
    "               density=True)\n",
    "eu_seq_ax.hist(test_euseq, bins=bins_euseq, alpha=0.5,\n",
    "               label=f'Test\\nMin: {int(np.min(test_euseq))}\\nMax: {int(np.max(test_euseq))}\\nMedian: {int(np.median(test_euseq))}',\n",
    "               density=True)\n",
    "\n",
    "eu_seq_ax.set_xlim(min_bound, max_bound)\n",
    "eu_seq_ax.set_yscale('log')\n",
    "eu_seq_ax.set_title('RNA-seq')\n",
    "eu_seq_ax.set_aspect('auto', adjustable='box')\n",
    "eu_seq_ax.legend(bbox_to_anchor=(.8, .99), loc='upper center', ncol=1)\n",
    "fig.savefig(savepath / \"Data_distribution-RNA.png\", dpi=200, bbox_inches='tight')\n",
    "\n",
    "# Micro-C\n",
    "# Before creating the EU-seq subplot, add Micro-C plotting:\n",
    "fig_microC, microc_ax = plt.subplots(1,1, figsize=(5,4)) # Use position 4 for Micro-C\n",
    "min_bound, max_bound = bounds[\"prom-CHiC\"]\n",
    "bins_microc = np.linspace(min_bound, max_bound,  N_BINS+1)\n",
    "\n",
    "microc_ax.hist(train_microc[\"prom-CHiC\"], bins=bins_microc, alpha=0.5,\n",
    "               label=f'Train\\nMin: {int(np.min(train_microc[\"prom-CHiC\"]))}\\nMax: {int(np.max(train_microc[\"prom-CHiC\"]))}\\nMedian: {int(np.median(train_microc[\"prom-CHiC\"]))}',\n",
    "               density=True)\n",
    "microc_ax.hist(test_microc[\"prom-CHiC\"], bins=bins_microc, alpha=0.5,\n",
    "               label=f'Test\\nMin: {int(np.min(test_microc[\"prom-CHiC\"]))}\\nMax: {int(np.max(test_microc[\"prom-CHiC\"]))}\\nMedian: {int(np.median(test_microc[\"prom-CHiC\"]))}',\n",
    "               density=True)\n",
    "\n",
    "microc_ax.set_ylim(1e-6,2e1)\n",
    "microc_ax.set_yscale('log')\n",
    "microc_ax.set_title('prom-CHiC')\n",
    "microc_ax.legend(bbox_to_anchor=(.3, .99), loc='upper center', ncol=1)\n",
    "microc_ax.set_aspect('auto', adjustable='box')\n",
    "\n",
    "plt.tight_layout()\n",
    "fig_microC.savefig(savepath / \"prom_CHiC_distribution.png\", dpi=200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before creating the EU-seq subplot, add Micro-C plotting:\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "plt.rcParams['font.family'] = 'Nimbus Roman'\n",
    "\n",
    "training_microc_path = Path(\"../inputs/Promoter-CHiC/training/\")\n",
    "test_microc_path = Path(\"../inputs/Promoter-CHiC/test/\")\n",
    "savepath = Path(\"../figures/Figures_revisions/data_distributions_K562/\")\n",
    "savepath.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "def load_single_file(filename, path, marks):\n",
    "    a = np.load(path / filename)\n",
    "    if True in np.isnan(a):\n",
    "        print(filename)\n",
    "    return {mark: a[i,:] for i, mark in enumerate(marks)}\n",
    "\n",
    "def load_data(path):\n",
    "    marks = [\"ATAC\", \"H3K4me3\", \"H3K27ac\", \"H3K27me3\"]\n",
    "    filenames = [array.name for array in os.scandir(path)]\n",
    "    \n",
    "    load_func = partial(load_single_file, path=path, marks=marks)\n",
    "    \n",
    "    n_cores = cpu_count() - 1\n",
    "    with Pool(processes=n_cores) as pool:\n",
    "        results = pool.map(load_func, filenames)\n",
    "    \n",
    "    combined = {mark: [] for mark in marks}\n",
    "    for result in results:\n",
    "        for mark in marks:\n",
    "            combined[mark].append(result[mark])\n",
    "            \n",
    "    return {mark: np.concatenate(arrays) for mark, arrays in combined.items()}\n",
    "\n",
    "# Define custom bounds for each mark\n",
    "bounds = {\n",
    "    \"ATAC\": (0, 1000),\n",
    "    \"H3K4me3\": (0, 1000),\n",
    "    \"H3K27ac\": (0, 1000),\n",
    "    \"H3K27me3\": (0, 1000),\n",
    "    #\"EU-seq\": (0, 400),\n",
    "    'RNA-seq':(0,1000),\n",
    "    #\"POLR2A\": (0,1000),\n",
    "    #\"Micro-C\": (-15,2),\n",
    "    \"prom-CHiC\":(-15,2)\n",
    "}\n",
    "\n",
    "# Load Micro-C data - using same loading function but with single mark\n",
    "def load_microc_data(path):\n",
    "    filenames = [array.name for array in os.scandir(path)]\n",
    "    \n",
    "    load_func = partial(load_single_file, path=path, marks=[\"prom-CHiC\"])\n",
    "    \n",
    "    n_cores = cpu_count() - 1\n",
    "    with Pool(processes=n_cores) as pool:\n",
    "        results = pool.map(load_func, filenames)\n",
    "    \n",
    "    combined = {\"prom-CHiC\": []}\n",
    "    for result in results:\n",
    "        combined[\"prom-CHiC\"].append(result[\"prom-CHiC\"])\n",
    "            \n",
    "    return {\"prom-CHiC\": np.concatenate(arrays) for mark, arrays in combined.items()}\n",
    "\n",
    "# Load Micro-C data\n",
    "train_microc = load_microc_data(training_microc_path)\n",
    "test_microc = load_microc_data(test_microc_path)\n",
    "\n",
    "N_BINS = 200\n",
    "\n",
    "training_microc_path = Path(\"../inputs/Promoter-CHiC/training/\")\n",
    "test_microc_path = Path(\"../inputs/Promoter-CHiC/test/\")\n",
    "\n",
    "fig_microC, microc_ax = plt.subplots(1,1, figsize=(5,4)) # Use position 4 for Micro-C\n",
    "min_bound, max_bound = bounds[\"prom-CHiC\"]\n",
    "bins_microc = np.linspace(min_bound, max_bound,  N_BINS+1)\n",
    "\n",
    "microc_ax.hist(train_microc[\"prom-CHiC\"], bins=bins_microc, alpha=0.5,\n",
    "               label=f'Train\\nMin: {int(np.min(train_microc[\"prom-CHiC\"]))}\\nMax: {int(np.max(train_microc[\"prom-CHiC\"]))}\\nMedian: {int(np.median(train_microc[\"prom-CHiC\"]))}',\n",
    "               density=True)\n",
    "microc_ax.hist(test_microc[\"prom-CHiC\"], bins=bins_microc, alpha=0.5,\n",
    "               label=f'Test\\nMin: {int(np.min(test_microc[\"prom-CHiC\"]))}\\nMax: {int(np.max(test_microc[\"prom-CHiC\"]))}\\nMedian: {int(np.median(test_microc[\"prom-CHiC\"]))}',\n",
    "               density=True)\n",
    "\n",
    "microc_ax.set_ylim(1e-6,2e1)\n",
    "microc_ax.set_yscale('log')\n",
    "microc_ax.set_title('prom-CHiC')\n",
    "microc_ax.legend(bbox_to_anchor=(.3, .99), loc='upper center', ncol=1)\n",
    "microc_ax.set_aspect('auto', adjustable='box')\n",
    "\n",
    "plt.tight_layout()\n",
    "fig_microC.savefig(savepath / \"prom_CHiC_distribution.png\", dpi=200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complementary analysis of Micro-C data distribution:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile create_average_microc_matrix.py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import os\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "plt.rcParams['font.family'] = 'Nimbus Roman'\n",
    "\n",
    "savepath = Path(\"../figures/Figures_revisions/\")\n",
    "savepath.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "def process_single_file(filename, path):\n",
    "    array = np.load(path / filename)\n",
    "    return array\n",
    "\n",
    "def main():\n",
    "    path = Path(\"../inputs/microC/test/\")\n",
    "    \n",
    "    # Get list of files\n",
    "    filenames = [f.name for f in os.scandir(path)]\n",
    "    scaling_factor = len(filenames)\n",
    "    \n",
    "    # Setup parallel processing\n",
    "    n_cores = cpu_count() - 1\n",
    "    process_file = partial(process_single_file, path=path)\n",
    "    \n",
    "    # Process files in parallel\n",
    "    with Pool(processes=n_cores) as pool:\n",
    "        results = pool.map(process_file, filenames)\n",
    "    \n",
    "    # Sum all arrays and apply scaling\n",
    "    avg_array = np.zeros((626, 626))\n",
    "    median_array = np.zeros((scaling_factor, 626, 626))\n",
    "    for j,array in enumerate(results):\n",
    "        avg_array += array / scaling_factor\n",
    "        median_array[j] = array\n",
    "\n",
    "    # Plot mean array\n",
    "    fig = plt.figure(figsize=(6,5))\n",
    "    plt.imshow(avg_array, cmap='jet')\n",
    "    plt.colorbar()\n",
    "    fig.savefig(savepath / 'Average_test_MicroC.png', dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    # Plot median array\n",
    "    fig = plt.figure(figsize=(6,5))\n",
    "    plt.imshow(np.median(median_array, axis=0), cmap='jet')\n",
    "    plt.colorbar()\n",
    "    fig.savefig(savepath / 'Median_test_MicroC.png', dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    # Plot histogram of values\n",
    "    fig = plt.figure(figsize=(6,5))\n",
    "    plt.hist(avg_array.flatten(), bins=100, density=True)\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Distribution of Values in Average Micro-C Matrix')\n",
    "    fig.savefig(savepath / 'Average_test_MicroC_histogram.png', dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VII) Promoter-Capture Hi-C analyses\n",
    "\n",
    "**Process the raw data to obtain cooler contact matrices.**\n",
    "- Obtain the SRA file with the raw reads corresponding to mESC Promoter-Capture HiC data:\n",
    "[SRA file](https://trace.ncbi.nlm.nih.gov/Traces/?view=run_browser&acc=SRR5972844&display=data-access)\n",
    "- Dump these into two separate fastq files (to build the 2D histograms).\n",
    "\n",
    "The original microC matrices were obtained at a 1600bp resolution. Promoter Capture Hi-C has lower sequencing depth, and therefore we lowered the resolution to 5kbp in order to increase the signal intensity. The scripts to obtain cooler files can be found inside the ```prom_CHiC_processing``` folder.\n",
    "\n",
    "Minimum and maximum values in the Promoter-Capture Hi-C dataset (at a 5kbp resolution) were 8e-4 and 8.47(log10 transformed were -3.1 and 0.92). In a similar fashion as for micro-C data, we imputed with 1e-14 to keep the imputation consistent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cooler\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def find_dataset_min_max(input_path: str) -> float:\n",
    "    # Open contact file (mcool) as a cooler object\n",
    "    c = cooler.Cooler(input_path)\n",
    "    bin_weights = c.bins()['weight'][:]\n",
    "    return (np.min(bin_weights), np.max(bin_weights))\n",
    "\n",
    "def plot_histogram_dataset(input_path: str, bins = 1000):\n",
    "    # Open contact file (mcool) as a cooler object\n",
    "    c = cooler.Cooler(input_path)\n",
    "    bin_weights = c.bins()['weight'][:]\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.hist(np.log10(bin_weights), bins=bins)\n",
    "    plt.yscale('log')\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "def mcool_summary(microc_path):\n",
    "    c = cooler.Cooler(microc_path)\n",
    "\n",
    "    # Inspecting what is inside the cooler object:\n",
    "    metadata = c.info\n",
    "    chromosomes = c.chromnames\n",
    "    extent = c.extent('chr2')\n",
    "\n",
    "    print(\"Metadata:\\n\", metadata)\n",
    "    print(\"Chromosomes:\\n\", chromosomes)\n",
    "    print(\"Extent:\\n\", extent)\n",
    "\n",
    "input_path = \"../GEO_files/SRR5972844.mm10.mapq_30.100.mcool::/resolutions/5000\"\n",
    "window_start = 87668723\n",
    "window_end = window_start + 1000000\n",
    "chrom = \"chr1\"\n",
    "imputation_value = 1e-14\n",
    "\n",
    "minimum, maximum = find_dataset_min_max(input_path)\n",
    "\n",
    "print(np.log10(minimum), np.log10(maximum))\n",
    "fig = plot_histogram_dataset(input_path, bins = 1000)\n",
    "fig.show()\n",
    "\n",
    "c = cooler.Cooler(input_path)\n",
    "# Fetch and process the matrix\n",
    "c_matrix = c.matrix(balance=True).fetch(f'{chrom}:{window_start}-{window_end}')\n",
    "c_matrix = np.nan_to_num(c_matrix, nan=imputation_value)\n",
    "c_matrix[c_matrix == 0.] = imputation_value\n",
    "c_matrix = np.log10(c_matrix)\n",
    "\n",
    "print(c_matrix.shape)\n",
    "print(np.min(c_matrix), np.max(c_matrix))\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.imshow(c_matrix, cmap='YlOrRd')\n",
    "plt.colorbar()\n",
    "fig.show\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.hist(np.log10(c_matrix.flatten()), bins=1000)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile create_prom_CHiC_arrays.py\n",
    "\n",
    "import multiprocessing\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cooler\n",
    "import logging\n",
    "from scipy.ndimage import rotate\n",
    "\n",
    "########### Functions #####################################\n",
    "def find_dataset_min_max(input_path: str) -> float:\n",
    "    # Open contact file (mcool) as a cooler object\n",
    "    c = cooler.Cooler(input_path)\n",
    "    bin_weights = c.bins()['weight'][:]\n",
    "    return (np.min(bin_weights), np.max(bin_weights))\n",
    "\n",
    "def plot_histogram_dataset(input_path: str, bins = 1000):\n",
    "    # Open contact file (mcool) as a cooler object\n",
    "    c = cooler.Cooler(input_path)\n",
    "    bin_weights = c.bins()['weight'][:]\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.hist(bin_weights, bins=bins)\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "def mcool_summary(microc_path):\n",
    "    c = cooler.Cooler(microc_path)\n",
    "\n",
    "    # Inspecting what is inside the cooler object:\n",
    "    metadata = c.info\n",
    "    chromosomes = c.chromnames\n",
    "    extent = c.extent('chr2')\n",
    "\n",
    "    print(\"Metadata:\\n\", metadata)\n",
    "    print(\"Chromosomes:\\n\", chromosomes)\n",
    "    print(\"Extent:\\n\", extent)\n",
    "\n",
    "def process_prom_CHiC_data(args):\n",
    "    (gene_annotations_path, microc_path, savepath, shift, imputation_value, row) = args\n",
    "    ID, chrom, Start, End, Strand, Name, CRE_type = row\n",
    "    split = \"test\" if chrom == \"chr4\" else \"training\"\n",
    "    TSS = int(Start) if Strand == '+' else int(End)\n",
    "\n",
    "    c = cooler.Cooler(microc_path)\n",
    "    window_start = TSS - shift\n",
    "    window_end = TSS + shift\n",
    "\n",
    "    try:\n",
    "        # Fetch and process the matrix\n",
    "        c_matrix = c.matrix(balance=True).fetch(f'{chrom}:{window_start}-{window_end}')\n",
    "        c_matrix = np.nan_to_num(c_matrix, nan=imputation_value)\n",
    "        c_matrix[c_matrix == 0.] = imputation_value\n",
    "        c_matrix = np.log10(c_matrix)\n",
    "\n",
    "        if c_matrix.shape == (201, 201):\n",
    "            # Save direct contact matrices\n",
    "            np.save(savepath / \"Promoter-CHiC\" / split / f\"{ID}_forward.npy\", c_matrix)\n",
    "            anti_c_matrix = np.fliplr(np.flipud(c_matrix))\n",
    "            np.save(savepath / \"Promoter-CHiC\" / split / f\"{ID}_rev.npy\", anti_c_matrix)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.info(f\"{ID} raised an error: {e}\")\n",
    "\n",
    "def create_promoter_CHiC_arrays_parallel(gene_annotations_path, microc_path, savepath, shift=500000, imputation_value=1e-14):\n",
    "    \"\"\"\n",
    "    Parallel version for creating Micro C arrays.\n",
    "    \"\"\"\n",
    "    gene_annotations_df = pd.read_csv(gene_annotations_path, sep=\"\\t\")\n",
    "    \n",
    "    args = [(gene_annotations_path, microc_path, savepath, shift, imputation_value, row) for index, row in gene_annotations_df.iterrows()]\n",
    "\n",
    "    with multiprocessing.Pool(processes=multiprocessing.cpu_count() - 1) as pool:\n",
    "        pool.map(process_prom_CHiC_data, args)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Define paths\n",
    "    path = Path(\"../\")\n",
    "    prom_CHiC_path = path /  \"GEO_files/SRR5972844.mm10.mapq_30.100.mcool::/resolutions/5000\"\n",
    "    gene_annotations_path = path / \"annotations/Final_gene_annotations.tsv\"\n",
    "    savepath = path / \"inputs\"\n",
    "    savepath.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Setup logging\n",
    "    LOG_FILENAME = \"Prom_CHiC_data_obtention.log\"\n",
    "    logging.basicConfig(filename=savepath / LOG_FILENAME, level=logging.INFO, format='%(asctime)s %(message)s')\n",
    "\n",
    "    # Create directories for output\n",
    "    for split in [\"training\", \"test\"]:\n",
    "        (savepath / \"Promoter-CHiC\" / split).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Run the processing in parallel\n",
    "    create_promoter_CHiC_arrays_parallel(str(gene_annotations_path), str(prom_CHiC_path), savepath, imputation_value=1e-14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the samples for which we do not have landscape arrays in training or test folders:\n",
    "# Get list of files from both reference folders\n",
    "landscape_training = [file.name for file in os.scandir(\"../inputs/landscape_arrays/training/\")]\n",
    "landscape_test = [file.name for file in os.scandir(\"../inputs/landscape_arrays/test/\")]\n",
    "old_samples = landscape_training + landscape_test\n",
    "\n",
    "# Check current directory files\n",
    "for folder in os.scandir('../inputs/Promoter-CHiC/'):\n",
    "    for file in os.scandir(folder):\n",
    "        if file.name not in old_samples:\n",
    "            print(f\"Removing: {file.name}\")\n",
    "            os.remove(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training eir with promoter capture HiC added:**\n",
    "\n",
    "```bash\n",
    "\n",
    "eirtrain \\\n",
    "--global_configs ./configurations/conf_pure_conv_prom_CHiC_train/globals.yaml \\\n",
    "--input_configs ./configurations/conf_pure_conv_prom_CHiC_train/input_cnn.yaml ./configurations/conf_pure_conv_prom_CHiC_train/input_cnn_microc.yaml \\\n",
    "--fusion_configs ./configurations/conf_pure_conv_prom_CHiC_train/fusion.yaml \\\n",
    "--output_configs ./configurations/conf_pure_conv_prom_CHiC_train/outputs_2_cond.yaml\n",
    "```\n",
    "**Testing prom-CHiC:**\n",
    "\n",
    "```bash\n",
    "\n",
    "eirpredict \\\n",
    "--global_configs ./configurations/conf_pure_conv_prom_CHiC_test/globals.yaml \\\n",
    "--input_configs ./configurations/conf_pure_conv_prom_CHiC_test/input_cnn.yaml ./configurations/conf_pure_conv_prom_CHiC_test/input_cnn_microc.yaml \\\n",
    "--fusion_configs ./configurations/conf_pure_conv_prom_CHiC_test/fusion.yaml \\\n",
    "--output_configs ./configurations/conf_pure_conv_prom_CHiC_test/outputs_2_cond.yaml \\\n",
    "--evaluate \\\n",
    "--model_path ./runs/gene_expression_prom_CHiC_pure_conv/saved_models/gene_expression_prom_CHiC_pure_conv_model_60600_perf-average=0.8159.pt \\\n",
    "--output_folder ./runs/test_runs/gene_expression_prom_CHiC_pure_conv_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing contact maps and attributions for prom CHiC data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_prom_CHiC(path):\n",
    "    prom_CHiC_map = np.load(path)\n",
    "\n",
    "    fig = plt.figure()#(11.5, 5))\n",
    "    gs = gridspec.GridSpec(1, 2, hspace=.2, wspace=.1, width_ratios=(5,0.1))\n",
    "\n",
    "    # Main image\n",
    "    ax = fig.add_subplot(gs[0, 0])\n",
    "    # Create an axis for the colorbar\n",
    "    cax = fig.add_subplot(gs[0, 1])\n",
    "\n",
    "    img_display = ax.imshow(prom_CHiC_map, aspect='equal', cmap=\"YlOrRd\")\n",
    "    cbar = plt.colorbar(img_display, cax=cax)\n",
    "\n",
    "    ax.set_xlabel(\"Bins (5kbp resolution)\", fontsize= 12)\n",
    "    ax.set_ylabel(\"Bins (5kbp resolution)\", fontsize= 12)\n",
    "\n",
    "    return fig\n",
    "\n",
    "def plot_prom_CHiC_attributions(figure_path, sample_ids, num_bins, window_of_observation, ATTR_type_list, conditions, sigma):\n",
    "    \"\"\" \n",
    "    Plot the attribution maps for micro-c matrices.\n",
    "    \"\"\"\n",
    "    bin_per_kbp = num_bins/window_of_observation\n",
    "    print(bin_per_kbp)\n",
    "    max_shift = int(200*bin_per_kbp) # Distance to crop because it was rolled from the other side\n",
    "    print(max_shift)\n",
    "\n",
    "    avg_att_mat_dict = {}\n",
    "    for condition in conditions:\n",
    "        avg_att_mat_dict[condition] = {}\n",
    "        for ATTR_type in ATTR_type_list:\n",
    "            avg_att_mat_dict[condition][ATTR_type] = np.zeros((201,201))\n",
    "\n",
    "\n",
    "    for condition in conditions:\n",
    "        for i in sample_ids:\n",
    "            #path = Path(f\"../runs/training_runs/gene_expression_exformer_unlimited_all_rotated/results/expression_output/{i}{condition}/samples/17880/attributions/contact_maps/{i}{condition}.npy\") \n",
    "            path = Path(f\"../runs/test_runs/gene_expression_prom_CHiC_pure_conv_test/expression_output/{i}{condition}/attributions/contact_maps/{i}{condition}.npy\") \n",
    "            att_mat = np.load(path)\n",
    "            att_mat_rolled = np.roll(np.roll(att_mat,-int(i*bin_per_kbp), axis = 1), -int(i*bin_per_kbp), axis = 0)\n",
    "\n",
    "            avg_att_mat_dict[condition][\"abs\"] += 1/(len(sample_ids))*abs(att_mat_rolled)\n",
    "            avg_att_mat_dict[condition][\"signed\"] += 1/(len(sample_ids))*att_mat_rolled\n",
    "\n",
    "    for CONDITION in conditions:\n",
    "        for ATTR_type in ATTR_type_list:\n",
    "            avg_att_mat = avg_att_mat_dict[condition][ATTR_type] \n",
    "            #avg_att_mat = gaussian_filter(avg_att_mat, sigma=sigma)\n",
    "\n",
    "            fig = plt.figure()#(11.5, 5))\n",
    "            gs = gridspec.GridSpec(1, 2, hspace=.2, wspace=.02, width_ratios=(5,0.1))\n",
    "\n",
    "            # Main image\n",
    "            ax = fig.add_subplot(gs[0, 0])\n",
    "            # Create an axis for the colorbar\n",
    "            cax = fig.add_subplot(gs[0, 1])\n",
    "\n",
    "            if ATTR_type == \"abs\":\n",
    "                img_display = ax.imshow(avg_att_mat, aspect='equal', cmap=\"inferno\", vmin=0, vmax=0.5e-4, origin=\"upper\", extent=[0, 201, 201, 0])\n",
    "                cbar = plt.colorbar(img_display, cax=cax)\n",
    "                cbar.set_ticks([0,5e-5])\n",
    "                cbar.set_ticklabels([0,1])\n",
    "            else:\n",
    "                img_display = ax.imshow(avg_att_mat, aspect='equal', cmap=\"seismic\", vmin=-0.5e-4, vmax=0.5e-4, origin=\"upper\", extent=[0, 201, 201, 0])\n",
    "                cbar = plt.colorbar(img_display, cax=cax)\n",
    "                cbar.set_ticks([-5e-5,0,5e-5])\n",
    "                cbar.set_ticklabels([-1,0,1])\n",
    "                ax.plot(np.arange(1,101), 200-np.arange(1,101), 'k--', lw=1)\n",
    "                ax.plot(np.arange(1,101), 100*np.ones(100), 'b--', lw=.8)\n",
    "                ax.plot(100*np.ones(100), np.arange(100,200), 'b--', lw=.8)\n",
    "                ax.add_patch(patches.Rectangle((0, 0), 40, 201, linewidth = 0, fill=True, alpha=0.2, color='grey'))\n",
    "                # Right edge\n",
    "                ax.add_patch(patches.Rectangle((201-40, 0), 40, 201, linewidth = 0, fill=True, alpha=0.2, color='grey'))\n",
    "                # Bottom edge (excluding corners already covered)\n",
    "                ax.add_patch(patches.Rectangle((40, 0), 201-80, 40, linewidth = 0, fill=True, alpha=0.2, color='grey'))\n",
    "                # Top edge (excluding corners already covered)\n",
    "                ax.add_patch(patches.Rectangle((40, 201-40), 201-80, 40, linewidth = 0, fill=True, alpha=0.2, color='grey'))\n",
    "\n",
    "            ax.set_xlabel(\"Bins (5kbp resolution)\", fontsize= 12)\n",
    "            ax.set_ylabel(\"Bins (5kbp resolution)\", fontsize= 12)\n",
    "            #ax.set_xlim(201,0)\n",
    "            #ax.set_ylim(201,0)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            fig.savefig(figure_path / f\"Structural_attributions_{ATTR_type}_{CONDITION}_{sigma}.png\", dpi=200)\n",
    "            plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"../inputs/Promoter-CHiC/test/\")\n",
    "figure_path = Path(\"../figures/Figures_revisions/attributions/\")\n",
    "sample_ids = range(-200,201)\n",
    "sample = \"ENSMUSG00000028287.4_forward\"\n",
    "num_bins = 201\n",
    "window_of_observation = 1000\n",
    "ATTR_type_list = [\"abs\",\"signed\"]\n",
    "conditions = [\"_ctrl\"]\n",
    "sigma = 1\n",
    "\n",
    "fig = visualize_prom_CHiC(path / f\"{sample}.npy\")\n",
    "fig.savefig(figure_path / f\"{sample}.png\", dpi=200)\n",
    "plot_prom_CHiC_attributions(figure_path, sample_ids, num_bins, window_of_observation, ATTR_type_list, conditions, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> VIII) Extending CLASTER to new cell types: K562 (human) <center>\n",
    "\n",
    "Reviewer 1 asked us to benchmark our _in silico_ perturbations with experimental data. Most experimental data on genome-wide enhancer KOs is obtained in K562 cells. We did not find nascent transcription data matching our protocol, and hence decided to predict two widespread transcriptional readouts: RNA-seq and POLR2A ChIP-seq.\n",
    "\n",
    "### Predicting mature RNA-seq counts & RNA-polII ChIP-seq\n",
    "- Obtain data (hg19).\n",
    "- Train models.\n",
    "- Evaluate performance.\n",
    "- Perform downstream perturbational analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mature RNA-seq predictions: hg19\n",
    "links = {#\"EU_Seq_Ctrl_plus.bw\":\"https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSM2072364&format=file&file=GSM2072364%5FENCFF001SEN%5Fplus%5Fstrand%5Fsignal%5Fhg19%2EbigWig\",\n",
    "         #\"EU_Seq_Ctrl_minus.bw\":\"https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSM2072364&format=file&file=GSM2072364%5FENCFF001SEO%5Fminus%5Fstrand%5Fsignal%5Fhg19%2EbigWig\",\n",
    "         \"POLR_IIA_Ctrl.bw\": \"https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE91426&format=file&file=GSE91426%5FENCFF541PYW%5Fsignal%5Fp%2Dvalue%5Fhg19%2EbigWig\", #Stanford ENCODE hg19\n",
    "         #\"ATAC_Seq.bw\": \"https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSM4971133&format=file&file=GSM4971133%5F11283%5F11142%5F109459%5FHKLV3BGXC%5FATAC%5FATAC%5F0h%5FTTCTGC%2Ebw\", #Danko\n",
    "         #\"H3K4me3.bw\":\"https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSM4971098&format=file&file=GSM4971098%5FmergedMasked%5FK4me3%5F0h%5Fbr1%2Eraw%2Ebw\", #Danko\n",
    "         #\"H3K27ac.bw\":\"https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSM4971064&format=file&file=GSM4971064%5FmergedMasked%5FK27ac%5F0h%5Fbr1%2Eraw%2Ebw\", #Danko\n",
    "         #\"H3K27me3.bw\": \"https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSM4971075&format=file&file=GSM4971075%5FmergedMasked%5FK27me3%5F0h%5Fbr1%2Eraw%2Ebw\"\n",
    "         } #Danko\n",
    "\n",
    "savepath = Path(\"../GEO_files/K562/\")\n",
    "savepath.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "download_files(links, savepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gene and Enhancer annotations:**\n",
    "- Gene annotations were obtained for hg19 from [gencode](https://www.gencodegenes.org/human/release_19.html) (Input data from Danko lab was mapped to hg19 originally).\n",
    "- Enhancer annotations could be obtained from:\n",
    "    - [Supplementary Table 10](https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-020-2493-4/MediaObjects/41586_2020_2493_MOESM12_ESM.txt). \n",
    "    - We however used the [annotations from the CRISPR enhancer KOs from the Engreitz lab](https://github.com/EngreitzLab/CRISPR_comparison/blob/main/resources/crispr_data/EPCrisprBenchmark_ensemble_data_GRCh38.tsv.gz) reported in the latest E2G paper to ease the comparison. Only enhancers whose epigenetic activity was above a certain threshold were kept.\n",
    "    - The coordinates of the regulatory elements were lifted using [the UCSC genome browser lifting tool](https://genome.ucsc.edu/cgi-bin/hgLiftOver) from hg38 to hg19 to match the input profiles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = {\"Gene_annotations_human.gtf.gz\":\"https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_19/gencode.v19.annotation.gtf.gz\"}\n",
    "savepath = Path(\"../annotations/human/\")\n",
    "savepath.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "download_files(links, savepath)\n",
    "\n",
    "! gunzip ../annotations/human/Gene_annotations_human.gtf.gz\n",
    "\n",
    "# Create a file with annotations that correspond only to genes\n",
    "! awk '$3 == \"gene\"' ../annotations/human/Gene_annotations_human.gtf > ../annotations/human/Filtered_gene_annotations_human.gtf\n",
    "\n",
    "# Further filter the number of fields/columns required\n",
    "\n",
    "file = Path(\"../annotations/human/Filtered_gene_annotations_human.gtf\")\n",
    "outfile = Path(\"../annotations/human/Final_gene_annotations_human.tsv\")\n",
    "gene_annotations = pd.read_csv(file, sep=\"\\t\", header=None)\n",
    "\n",
    "ids = []\n",
    "names = []\n",
    "\n",
    "final_df = pd.DataFrame(columns=[\"ID\",\"chr\",\"Start\",\"End\",\"Strand\",\"Name\",\"type\"]).set_index(\"ID\")\n",
    "for (chrom, _, _, start, end, _, strand, _, features) in gene_annotations.values:\n",
    "    splitline =  features.split(\";\")\n",
    "\n",
    "    id = splitline[0].split(\" \")[1]\n",
    "    gene_type = splitline[2].split(\" \")[2]\n",
    "    name = splitline[4].split(\" \")[2]\n",
    "    if gene_type == '\"protein_coding\"' and chrom not in ['chrM', 'chrX','chrY']: # or gene_type == '\"lincRNA\"':\n",
    "        final_df.loc[id[1:-1]] = [chrom,start,end,strand,name[1:-1], gene_type[1:-1]]\n",
    "\n",
    "final_df.to_csv(outfile, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Manual validation ids (chr17) in human samples:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_validation_ids(path: Path, val_chrom: str = \"chr17\"):\n",
    "    \"\"\" All genes encoded in chr17 will be used as a validation set.\"\"\"\n",
    "    gene_annotations_df = pd.read_csv(path / \"Final_gene_annotations_human.tsv\", sep=\"\\t\").set_index('ID')\n",
    "    indices = gene_annotations_df[gene_annotations_df[\"chr\"] == val_chrom].index\n",
    "\n",
    "    # Prepare the strings to write in the file\n",
    "    lines = [f\"{index}_forward\\n{index}_rev\\n\" for index in indices]\n",
    "\n",
    "    # Write to the text file\n",
    "    with open(path / f\"manual_validation_ids_{val_chrom}_human.txt\", 'w') as file:\n",
    "        file.writelines(lines)\n",
    "\n",
    "path = Path(\"../annotations/human/\")\n",
    "create_validation_ids(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We removed the following IDs from manual validation id file: \n",
    "- 'ENSG00000272636.1_forward', \n",
    "- 'ENSG00000272636.1_rev', \n",
    "- 'ENSG00000175711.4_forward', \n",
    "- 'ENSG00000175711.4_rev', \n",
    "- 'ENSG00000176845.8_forward', \n",
    "- 'ENSG00000176845.8_rev'. \n",
    "\n",
    "Causes explained in Landscape_data_obtention_human.log: the input samples are out of bounds since these genes lie at the beginning/end of the chromosome.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create CLASTER inputs and outputs from human (K562) samples:**\n",
    "\n",
    "Similar code as in I_Data_Obtention.ipynb, better handling of Nans in input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile create_landscape_arrays_and_targets_human.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyBigWig\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "from scipy.ndimage import gaussian_filter1d, rotate\n",
    "import numpy.ma as ma\n",
    "\n",
    "track_dict = {0:{\"name\":\"ATAC-seq\",\"function\":\"Chromatin accessibility\",\"color\":\"k\"},\n",
    "            1:{\"name\":\"H3K4me3\",\"function\":\"Promoter\",\"color\":\"r\"},\n",
    "            2:{\"name\":\"H3K27ac\",\"function\":\"Enhancer\",\"color\":\"b\"},\n",
    "            3:{\"name\":\"H3K27me3\",\"function\":\"Chromatin silencing\",\"color\":\"g\"}}\n",
    "\n",
    "\n",
    "def clean_array(stats, max_clip=1000):\n",
    "    \"\"\"\n",
    "    Comprehensive cleaning function for array data.\n",
    "    Handles NaN, None, inf, and invalid values.\n",
    "    \n",
    "    Args:\n",
    "        stats: Input array or list of values\n",
    "        max_clip: Maximum value to clip to\n",
    "    \n",
    "    Returns:\n",
    "        Cleaned numpy array\n",
    "    \"\"\"\n",
    "    # Convert to numpy array if not already\n",
    "    array = np.array(stats, dtype=np.float64)\n",
    "    \n",
    "    # Create mask for invalid values (None, NaN, inf)\n",
    "    invalid_mask = (\n",
    "        (array == None) | \n",
    "        np.isnan(array) | \n",
    "        np.isinf(array) |\n",
    "        (array < 0)  # Negative values\n",
    "    )\n",
    "    \n",
    "    # Replace invalid values with 0\n",
    "    array[invalid_mask] = 0.0\n",
    "    \n",
    "    # Clip values to valid range\n",
    "    array = np.clip(array, 0, max_clip)\n",
    "    \n",
    "    return array\n",
    "\n",
    "def create_input_array(data_path: Path,\n",
    "                      input_shift: int,\n",
    "                      n_input_tracks: int, \n",
    "                      n_input_bins: int,\n",
    "                      bw_input_track_list: list,\n",
    "                      ID: str,\n",
    "                      TSS: int,\n",
    "                      chrom: str,\n",
    "                      max_clip: int = 1000):\n",
    "    \"\"\"\n",
    "    Modified input array creation with improved error handling and data cleaning.\n",
    "    \"\"\"\n",
    "    input_sample = np.zeros((n_input_tracks, n_input_bins))  # Initialize with zeros\n",
    "    \n",
    "    for i, bw_path in enumerate(bw_input_track_list):\n",
    "        try:\n",
    "            bw = pyBigWig.open(str(data_path / bw_path), \"r\")\n",
    "            stats = bw.stats(chrom, TSS-input_shift, TSS+input_shift, type=\"mean\", nBins=n_input_bins)\n",
    "            bw.close()\n",
    "            \n",
    "            # Clean the data using our comprehensive function\n",
    "            input_sample[i] = clean_array(stats, max_clip)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.warning(f\"{ID} track {i} error: {str(e)}\")\n",
    "            # Row already initialized to zeros, so we can continue\n",
    "            continue\n",
    "    \n",
    "    # Final validation of the entire array\n",
    "    if np.any(np.isnan(input_sample)) or np.any(np.isinf(input_sample)):\n",
    "        logging.warning(f\"{ID} still contains invalid values after cleaning. Setting to zero.\")\n",
    "        input_sample[~np.isfinite(input_sample)] = 0.0\n",
    "    \n",
    "    return input_sample\n",
    "\n",
    "def create_single_target_array(data_path: Path,\n",
    "                             output_shift: int,\n",
    "                             n_output_bins: int,\n",
    "                             bw_target_path: str,  # Single bigWig file path\n",
    "                             ID: str,\n",
    "                             TSS: int,\n",
    "                             chrom: str,\n",
    "                             sigma: int,\n",
    "                             binsize: int,\n",
    "                             max_clip: int = 1000):\n",
    "    \"\"\"\n",
    "    Creates target array from a single bigWig file without summing strands.\n",
    "    Used for RNA-PolII data that's already processed.\n",
    "    \n",
    "    Args:\n",
    "        bw_target_path: String with the path to the bigWig file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the data\n",
    "        bw = pyBigWig.open(str(data_path / bw_target_path), \"r\")\n",
    "        stats = bw.stats(chrom, TSS-output_shift, TSS+output_shift, \n",
    "                        type=\"mean\", nBins=n_output_bins)\n",
    "        bw.close()\n",
    "        \n",
    "        # Clean the data\n",
    "        stats = clean_array(stats, max_clip)\n",
    "        \n",
    "        # Apply Gaussian smoothing\n",
    "        stats = gaussian_filter1d(stats, sigma=sigma)\n",
    "        \n",
    "        # Initialize target condition array\n",
    "        target_cond = np.zeros_like(stats)\n",
    "        \n",
    "        # Averaging over bins\n",
    "        for i in range(binsize):\n",
    "            target_cond += 1 / binsize * np.roll(stats, -i)\n",
    "        target_cond = target_cond[::binsize]\n",
    "        \n",
    "        # Final cleanup of target condition array\n",
    "        target_cond = clean_array(target_cond, max_clip)\n",
    "        \n",
    "        # Create forward and reverse arrays\n",
    "        target_sample_pos = target_cond\n",
    "        target_sample_neg = np.flip(target_cond)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.warning(f\"{ID} RNA-PolII target processing error: {str(e)}\")\n",
    "        # Return arrays of appropriate size filled with zeros in case of error\n",
    "        zeros = np.zeros(n_output_bins // binsize)\n",
    "        target_sample_pos = zeros\n",
    "        target_sample_neg = zeros\n",
    "    \n",
    "    # Final validation\n",
    "    if np.any(np.isnan(target_sample_pos)) or np.any(np.isnan(target_sample_neg)):\n",
    "        logging.warning(f\"{ID} RNA-PolII targets contain NaN values after processing. Setting to zero.\")\n",
    "        target_sample_pos = np.nan_to_num(target_sample_pos, 0.0)\n",
    "        target_sample_neg = np.nan_to_num(target_sample_neg, 0.0)\n",
    "    \n",
    "    return target_sample_pos.flatten(), target_sample_neg.flatten()\n",
    "\n",
    "def create_target_array(data_path: Path,\n",
    "                       output_shift: int,\n",
    "                       n_output_bins: int,\n",
    "                       bw_target_track_list: list,\n",
    "                       ID: str,\n",
    "                       TSS: int,\n",
    "                       chrom: str,\n",
    "                       sigma: int,\n",
    "                       binsize: int,\n",
    "                       max_clip: int = 1000):\n",
    "    \"\"\"\n",
    "    Modified target array creation with improved error handling and data cleaning.\n",
    "    \"\"\"\n",
    "    target_sample_pos = np.array([])\n",
    "    target_sample_neg = np.array([])\n",
    "    \n",
    "    for plus_bw_path, minus_bw_path in bw_target_track_list:\n",
    "        try:\n",
    "            # Read plus strand data\n",
    "            plus_bw = pyBigWig.open(str(data_path / plus_bw_path), \"r\")\n",
    "            plus_stats = plus_bw.stats(chrom, TSS-output_shift, TSS+output_shift, \n",
    "                                     type=\"mean\", nBins=n_output_bins)\n",
    "            plus_bw.close()\n",
    "            \n",
    "            # Read minus strand data\n",
    "            minus_bw = pyBigWig.open(str(data_path / minus_bw_path), \"r\")\n",
    "            minus_stats = minus_bw.stats(chrom, TSS-output_shift, TSS+output_shift, \n",
    "                                       type=\"mean\", nBins=n_output_bins)\n",
    "            minus_bw.close()\n",
    "            \n",
    "            # Clean both arrays\n",
    "            plus_stats = clean_array(plus_stats, max_clip)\n",
    "            minus_stats = clean_array(minus_stats, max_clip)\n",
    "            \n",
    "            # Combine plus and minus strand signals\n",
    "            combined_stats = plus_stats + minus_stats\n",
    "            \n",
    "            # Apply Gaussian smoothing\n",
    "            combined_stats = gaussian_filter1d(combined_stats, sigma=sigma)\n",
    "            \n",
    "            # Initialize target condition array\n",
    "            target_cond = np.zeros_like(combined_stats)\n",
    "            \n",
    "            # Averaging over bins\n",
    "            for i in range(binsize):\n",
    "                target_cond += 1 / binsize * np.roll(combined_stats, -i)\n",
    "            target_cond = target_cond[::binsize]\n",
    "            \n",
    "            # Final cleanup of target condition array\n",
    "            target_cond = clean_array(target_cond, max_clip)\n",
    "            \n",
    "            # Add to final arrays\n",
    "            target_sample_pos = np.concatenate([target_sample_pos, target_cond])\n",
    "            target_sample_neg = np.concatenate([target_sample_neg, np.flip(target_cond)])\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.warning(f\"{ID} target processing error: {str(e)}\")\n",
    "            # Return arrays of appropriate size filled with zeros in case of error\n",
    "            zeros = np.zeros(n_output_bins // binsize)\n",
    "            target_sample_pos = np.concatenate([target_sample_pos, zeros])\n",
    "            target_sample_neg = np.concatenate([target_sample_neg, zeros])\n",
    "    \n",
    "    # Final validation\n",
    "    if np.any(np.isnan(target_sample_pos)) or np.any(np.isnan(target_sample_neg)):\n",
    "        logging.warning(f\"{ID} targets contain NaN values after processing. Setting to zero.\")\n",
    "        target_sample_pos = np.nan_to_num(target_sample_pos, 0.0)\n",
    "        target_sample_neg = np.nan_to_num(target_sample_neg, 0.0)\n",
    "    \n",
    "    return target_sample_pos.flatten(), target_sample_neg.flatten()\n",
    "    \n",
    "def visualize_input_array(a,\n",
    "                     cropped_bins : int = 4400,\n",
    "                     scaling_factor: float = 1.,\n",
    "                     track_dict: dict = track_dict):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to visualize an input numpy array.\n",
    "    \"\"\"\n",
    "\n",
    "    fig, axs = plt.subplots(len(track_dict), 2, figsize=(20,8))\n",
    "\n",
    "    # Plot the scaled arrays\n",
    "    for j,line in enumerate(a):\n",
    "        # Left plots: full window\n",
    "        axs[j][0].plot(np.arange(-len(line)//2,len(line)//2),line*scaling_factor, color=track_dict[j][\"color\"], label=track_dict[j][\"name\"], lw=.2)\n",
    "        axs[j][0].fill_between(np.arange(-len(line)//2,len(line)//2),line*scaling_factor, color=track_dict[j][\"color\"], alpha=1)\n",
    "        #axs[j][0].set_yticks([]) #([0,int(np.max(line))], [0,int(np.max(line))], fontsize=16)\n",
    "        #axs[j][0].set_ylim(0,350)\n",
    "        axs[j][0].set_ylabel(track_dict[j][\"name\"])\n",
    "        axs[-1][0].set_xlabel(\"Distance to TSS (kbp)\", fontsize= 16)\n",
    "        # Right plots, centered window\n",
    "        x_values = np.arange(-len(line)//2,len(line)//2)[cropped_bins:-cropped_bins]\n",
    "        y_values = line[cropped_bins:-cropped_bins]*scaling_factor\n",
    "        axs[j][1].set_xlim(x_values[0],x_values[-1])\n",
    "        axs[j][1].plot(x_values,y_values, color=track_dict[j][\"color\"], label=track_dict[j][\"name\"], lw=.2)\n",
    "        axs[j][1].fill_between(x_values,y_values, color=track_dict[j][\"color\"], alpha=.6)\n",
    "\n",
    "    return fig\n",
    "        \n",
    "def visualize_target_array(a,\n",
    "                           scaling_factor: float = 1.):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to visualize target profiles.\n",
    "    \"\"\"\n",
    "    ctrl = a\n",
    "    fig = plt.figure(figsize=(20,8))\n",
    "    plt.plot(np.arange(-len(ctrl)//2,len(ctrl)//2),ctrl*scaling_factor,label=\"ctrl\",color=\"grey\",lw=.2)\n",
    "    plt.fill_between(np.arange(-len(ctrl)//2,len(ctrl)//2),ctrl*scaling_factor, color = \"silver\" , alpha=1)\n",
    "\n",
    "    return fig\n",
    "\n",
    "def process_gene_data(data_path, bw_input_track_list, bw_target_track_list, rna_polii_track, input_shift, output_shift, \n",
    "                     n_input_bins, n_input_tracks, n_output_bins, sigma, binsize, INPUT_SHAPE, OUTPUT_LENGTH, \n",
    "                     path, split_list, figure_path, PLOT_IDS, target_files, polii_target_files, row):\n",
    "    ID, chrom, Start, End, Strand, Name, CRE_type = row\n",
    "    split = \"test\" if chrom == \"chr4\" else \"training\"\n",
    "    TSS = int(Start) if Strand == '+' else int(End)\n",
    "\n",
    "    # Create input sample\n",
    "    input_sample = create_input_array(data_path, input_shift, n_input_tracks, n_input_bins, \n",
    "                                    bw_input_track_list, ID, TSS, chrom)\n",
    "    \n",
    "    # Create regular target samples\n",
    "    target_sample_pos, target_sample_neg = create_target_array(data_path, output_shift, n_output_bins, \n",
    "                                                             bw_target_track_list, ID, TSS, chrom, sigma, binsize)\n",
    "    \n",
    "    # Create RNA-PolII target samples\n",
    "    polii_sample_pos, polii_sample_neg = create_single_target_array(data_path, output_shift, n_output_bins,\n",
    "                                                                  rna_polii_track, ID, TSS, chrom, sigma, binsize)\n",
    "\n",
    "    if (input_sample.shape == INPUT_SHAPE) and (len(target_sample_pos) == OUTPUT_LENGTH):\n",
    "        # Save input arrays\n",
    "        np.save(path / \"inputs\" / \"landscape_arrays\" / \"K562\" / split / f\"{ID}_forward.npy\", input_sample)\n",
    "        np.save(path / \"inputs\" / \"landscape_arrays\" / \"K562\" / split / f\"{ID}_rev.npy\", np.flip(input_sample, axis=1))\n",
    "        \n",
    "        # Save regular targets\n",
    "        target_file_path = target_files[split]\n",
    "        with open(target_file_path, 'a') as file:\n",
    "            file.write(f\"{ID}_forward,\" + \",\".join(map(str, target_sample_pos)) + \"\\n\")\n",
    "            file.write(f\"{ID}_rev,\" + \",\".join(map(str, target_sample_neg)) + \"\\n\")\n",
    "        \n",
    "        # Save RNA-PolII targets\n",
    "        polii_file_path = polii_target_files[split]\n",
    "        with open(polii_file_path, 'a') as file:\n",
    "            file.write(f\"{ID}_forward,\" + \",\".join(map(str, polii_sample_pos)) + \"\\n\")\n",
    "            file.write(f\"{ID}_rev,\" + \",\".join(map(str, polii_sample_neg)) + \"\\n\")\n",
    "    else:\n",
    "        logging.info(f\"{ID} : Input or target did not match the expected shape.\")\n",
    "\n",
    "    if ID in PLOT_IDS:\n",
    "        # Regular input/target visualizations\n",
    "        fig = visualize_input_array(input_sample)\n",
    "        fig.savefig(figure_path / f\"{ID}_forward.png\")\n",
    "        fig = visualize_target_array(target_sample_pos)\n",
    "        fig.savefig(figure_path / f\"{ID}_forward_target.png\")\n",
    "        fig = visualize_input_array(np.flip(input_sample, axis=1))\n",
    "        fig.savefig(figure_path / f\"{ID}_rev.png\")\n",
    "        fig = visualize_target_array(target_sample_neg)\n",
    "        fig.savefig(figure_path / f\"{ID}_rev_target.png\")\n",
    "        \n",
    "        # RNA-PolII visualizations\n",
    "        fig = visualize_target_array(polii_sample_pos)\n",
    "        fig.savefig(figure_path / f\"{ID}_forward_polii_target.png\")\n",
    "        fig = visualize_target_array(polii_sample_neg)\n",
    "        fig.savefig(figure_path / f\"{ID}_rev_polii_target.png\")\n",
    "\n",
    "############################ Script ###################\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    path = Path(\"../\")\n",
    "    data_path = path / \"GEO_files\" / \"K562\"\n",
    "    figure_path = path / \"figures\" / \"Figures_revisions\" / \"Inputs_and_Outputs\"\n",
    "    figure_path.mkdir(parents=True, exist_ok=True)\n",
    "    target_path = path / \"targets\" / \"K562\"\n",
    "    target_path.mkdir(exist_ok=True, parents=True)\n",
    "    polii_target_path = path / \"targets\" / \"K562\" / \"polii\"\n",
    "    polii_target_path.mkdir(exist_ok=True, parents=True)\n",
    "    split_list = [\"training\",\"test\"]\n",
    "    output_dirs = [(path / \"inputs\" / \"landscape_arrays\" / \"K562\" / f\"{split}\").mkdir(parents=True, exist_ok = True) for split in split_list]\n",
    "\n",
    "    PLOT_IDS = [\"ENSG00000269308.1\", \"ENSG00000185619.13\", \"ENSG00000235478.1\"]\n",
    "\n",
    "    # Start logger\n",
    "    LOG_FILENAME = \"/Landscape_data_obtention_human.log\"\n",
    "    logging.basicConfig(filename=str(path) + LOG_FILENAME, level=logging.INFO)  \n",
    "\n",
    "    gene_annotations_path = path / \"annotations\" / \"human\" / \"Final_gene_annotations_human.tsv\"\n",
    "    gene_annotations_df = pd.read_csv(gene_annotations_path, sep=\"\\t\")\n",
    "\n",
    "    input_shift = 500050 # In basepairs, from TSS to one side + central bin (100bp res)\n",
    "    output_shift = 200010 # Output in 20 bp resolution (to be smoothed and downsampled)\n",
    "    n_input_bins = 10001\n",
    "    n_input_tracks = 4\n",
    "    n_output_bins = 20001\n",
    "    sigma = 50\n",
    "    binsize = 50\n",
    "    INPUT_SHAPE = (4, 10001)\n",
    "    OUTPUT_LENGTH = 401 #802\n",
    "    N_BINS = 200\n",
    "    bw_input_track_list = [\"ATAC_Seq.bw\", \"H3K4me3.bw\", \"H3K27ac.bw\", \"H3K27me3.bw\"]\n",
    "    bw_target_track_list = [(\"EU_Seq_Ctrl_minus.bw\",\"EU_Seq_Ctrl_plus.bw\")] \n",
    "    # Add RNA-PolII track\n",
    "    rna_polii_track = \"POLR_IIA_Ctrl.bw\" \n",
    "\n",
    "    target_files = {split: ( target_path / f\"{split}_targets.csv\") for split in [\"training\", \"validation\", \"test\"]}\n",
    "\n",
    "    # Create additional target files for RNA-PolII\n",
    "    polii_target_files = {split: (polii_target_path / f\"{split}_polii_targets.csv\") for split in [\"training\", \"validation\", \"test\"]}\n",
    "    # Initialize RNA-PolII target files\n",
    "    for file_path in polii_target_files.values():\n",
    "        with open(file_path, 'w') as file:\n",
    "            file.write(\"ID,\" + \",\".join([f\"{i}{cond}\" for cond in [\"_ctrl\"] for i in range(-N_BINS,N_BINS+1)]) + \"\\n\")\n",
    "\n",
    "    for file_path in target_files.values():\n",
    "        with open(file_path, 'w') as file:\n",
    "            file.write(\"ID,\" + \",\".join([f\"{i}{cond}\" for cond in [\"_ctrl\"] for i in range(-N_BINS,N_BINS+1)]) + \"\\n\")\n",
    "\n",
    "    # Update process arguments to include the RNA-PolII track\n",
    "    process_args = (data_path, bw_input_track_list, bw_target_track_list, rna_polii_track, input_shift, output_shift, \n",
    "                  n_input_bins, n_input_tracks, n_output_bins, sigma, binsize, INPUT_SHAPE, OUTPUT_LENGTH, \n",
    "                  path, [\"training\", \"validation\", \"test\"], figure_path, PLOT_IDS, target_files, polii_target_files)\n",
    "\n",
    "    # Process gene data with multiprocessing\n",
    "    with multiprocessing.Pool(processes=multiprocessing.cpu_count() - 1) as pool:\n",
    "        pool.starmap(process_gene_data, [(process_args + (row,)) for row in gene_annotations_df.to_numpy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the model on K562 to predict baseline RNA-seq**\n",
    "\n",
    "```bash\n",
    "eirtrain \\\n",
    "--global_configs ./configurations/conf_pure_conv_K562_train/globals.yaml \\\n",
    "--input_configs ./configurations/conf_pure_conv_K562_train/input_cnn.yaml \\\n",
    "--fusion_configs ./configurations/conf_pure_conv_K562_train/fusion.yaml \\\n",
    "--output_configs ./configurations/conf_pure_conv_K562_train/outputs_2_cond.yaml\n",
    "```\n",
    "\n",
    "**Testing the model on K562 to predict baseline RNA-seq**\n",
    "\n",
    "```bash\n",
    "eirpredict \\\n",
    "--global_configs ./configurations/conf_pure_conv_K562_test/globals.yaml \\\n",
    "--input_configs ./configurations/conf_pure_conv_K562_test/input_cnn.yaml \\\n",
    "--fusion_configs ./configurations/conf_pure_conv_K562_test/fusion.yaml \\\n",
    "--output_configs ./configurations/conf_pure_conv_K562_test/outputs_2_cond.yaml \\\n",
    "--evaluate \\\n",
    "--model_path ./runs/gene_expression_only_chrom_K562_train/saved_models/gene_expression_only_chrom_K562_train_model_60600_perf-average=0.5028.pt \\\n",
    "--output_folder ./runs/test_runs/gene_expression_only_chrom_K562_test\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the model on K562 to predict baseline POLR2A CHiP-seq**\n",
    "\n",
    "```bash\n",
    "eirtrain \\\n",
    "--global_configs ./configurations/conf_pure_conv_K562_POLR2A_train/globals.yaml \\\n",
    "--input_configs ./configurations/conf_pure_conv_K562_POLR2A_train/input_cnn.yaml \\\n",
    "--fusion_configs ./configurations/conf_pure_conv_K562_POLR2A_train/fusion.yaml \\\n",
    "--output_configs ./configurations/conf_pure_conv_K562_POLR2A_train/outputs_2_cond.yaml\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize K562 predictions (RNA-seq and POLR2A)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputs:\n",
    "figures_path = Path(\"../figures/Figures_revisions/\")\n",
    "#ID = ENSMUSG00000090115.7_rev ENSMUSG00000043592.15_rev ENSMUSG00000023266.11_rev # Nice examples 'ENSMUSG00000079707.10_forward'\n",
    "val_ids = pd.read_csv(Path('../annotations/human/manual_validation_ids_chr17_human.txt')).values.flatten()\n",
    "\n",
    "for ID in val_ids[5::200]: #[\"ENSG00000166685.7_forward\", \"ENSG00000136436.10_forward\"]: #val_ids[2::200]:#   [\"ENSMUSG00000090115.7_forward\", \"ENSMUSG00000043592.15_forward\", \"ENSMUSG00000023266.11_forward\"]: #val_ids[::8]:\n",
    "    predictions = []\n",
    "    actual_data = []\n",
    "    for k, (title, attributes) in  enumerate(results_path_dict.items()):\n",
    "        if \"training\" in title:\n",
    "            #print(title)\n",
    "            ids, predicted, actual = _get_predictions(attributes['path'],\n",
    "                                                    attributes['N_BINS'],\n",
    "                                                    [\"_ctrl\"], \n",
    "                                                    is_training=attributes['is_training'], \n",
    "                                                    batch_num=attributes['batch_num'])\n",
    "            \n",
    "            line_p = predicted.loc[ID]\n",
    "            predictions.append({'profile':line_p,\n",
    "                                'color': attributes['color'],\n",
    "                                'alpha': 0.2,\n",
    "                                'label': f\"Predicted {attributes['label']}\",\n",
    "                                'ls':'-'\n",
    "            })\n",
    "\n",
    "            EU_seq_colors = [\"k\",\"red\"]\n",
    "            # Actual baseline signal:\n",
    "            actual_data.append({\n",
    "                'profile': actual.loc[ID],\n",
    "                'color': EU_seq_colors[k],\n",
    "                'alpha': 0.2,\n",
    "                'label': f\"Target {attributes['label']}\",\n",
    "                'ls':'-'\n",
    "            })\n",
    "\n",
    "\n",
    "\n",
    "    # Create plot\n",
    "    fig = plot_generalized_predictions(predictions, actual_data, N_BINS=200)\n",
    "    fig.savefig(figures_path / 'Outputs' / f'Model_predictions_human_{ID}.png', dpi=200)\n",
    "\n",
    "\n",
    "# Visualize correlations:\n",
    "for k, (title, attributes) in  enumerate(results_path_dict.items()):\n",
    "    ids, predicted, actual = _get_predictions(attributes['path'],\n",
    "                                        attributes['N_BINS'],\n",
    "                                        [\"_ctrl\"], \n",
    "                                        is_training=attributes['is_training'], \n",
    "                                        batch_num=attributes['batch_num'])\n",
    "    \n",
    "    # Plot point-wise prediction correlations:\n",
    "    pred_list_values = np.log2(np.array(predicted.values.flatten())+1) \n",
    "    actual_list_values = np.log2(np.array(actual.values.flatten())+1)\n",
    "    pointwise = plot_correlations(figures_path, pred_list_values, actual_list_values, title + \"_pointwise_correlation_prediction\",cmap='binary', binlims=(0,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enhancer-centered analyses of RNA-seq and POLR2A binding differences after _in silico_ enhancer ablation:\n",
    "\n",
    ">*Pipeline:*\n",
    ">- Check enhancer coordinates in EPCrisprBenchmark\n",
    ">- Create samples centered at those enhancers and create silenced enhancer samples\n",
    ">   - We filtered out: samples with enhancer activity average (H3K27ac) below 10 reads.\n",
    ">   - Samples where either input or output crossed chromosome boundaries (enhancers at the ends of the chromosomes).\n",
    ">- Predict using models trained on K562 (human) data.\n",
    ">- Quantify POLR2A /RNA-seq changes:\n",
    ">   - For POLR2A: Integrate between 2 kbp upstream and 3 kbp downstream of all genes in predicted window. (-1,2) kbp yielded similar results.\n",
    ">   - For RNA-seq: Integrate inside gene boundaries of all genes in predicted window.\n",
    ">- Downstream analyses:\n",
    ">   - Precision-Recall and ROC curves for the following models:\n",
    ">      - Gene-enhancer distance: $Score = - Distance$\n",
    ">      - RNA and POLR2A models: $Score = abs($ Area difference $)$\n",
    ">      - Ratio to max models: area difference divided by max area difference found in predicted window.\n",
    ">   - Confusion matrices:\n",
    ">      - Primary target (most affected gene in a single prediction run): True / False\n",
    ">      - Closest gene: True / False\n",
    "\n",
    "**Create enhancer centered K562 samples and targets:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile create_enhancer_centered_inputs_human.py\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyBigWig\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "\n",
    "def create_input_array(data_path: Path,\n",
    "                      input_shift: int,\n",
    "                      n_input_tracks: int, \n",
    "                      n_input_bins: int,\n",
    "                      bw_input_track_list: list,\n",
    "                      enhancer_id: str,\n",
    "                      center: int,\n",
    "                      chrom: str):\n",
    "    \"\"\"\n",
    "    Creates input arrays centered at enhancer midpoint\n",
    "    \"\"\"\n",
    "    input_sample = np.empty((n_input_tracks, n_input_bins))\n",
    "    for i, bw_path in enumerate(bw_input_track_list):\n",
    "        try:\n",
    "            bw = pyBigWig.open(str(data_path / bw_path), \"r\")\n",
    "            stats = bw.stats(chrom, center-input_shift, center+input_shift, type=\"mean\", nBins=n_input_bins)\n",
    "            bw.close()\n",
    "            \n",
    "            stats = np.array([float(value) if value is not None else 0. for value in stats])\n",
    "            stats = np.clip(np.array(stats), 0, None)  # ReLU\n",
    "            input_sample[i] = stats\n",
    "        except Exception as e:\n",
    "            logging.info(f\"{enhancer_id} input landscape coordinates are out of bounds: {e}\")\n",
    "    \n",
    "    return input_sample\n",
    "\n",
    "def create_perturbed_array(input_array: np.ndarray, start_idx: int, end_idx: int):\n",
    "    \"\"\"\n",
    "    Creates perturbed version with all marks except H3K27me3 silenced within enhancer boundaries\n",
    "    \"\"\"\n",
    "    perturbed = input_array.copy()\n",
    "    # Silence ATAC-seq, H3K4me3, and H3K27ac (indices 0, 1, 2) only within enhancer boundaries\n",
    "    perturbed[[0, 1, 2], start_idx:end_idx] = 0\n",
    "    return perturbed\n",
    "\n",
    "def process_enhancer(data_path, bw_input_track_list, input_shift, n_input_tracks, n_input_bins, \n",
    "                    INPUT_SHAPE, path, row_data, resolution=100, activity_threshold=10):\n",
    "    \"\"\"\n",
    "    Process a single enhancer and create its input arrays if enhancer is active\n",
    "    \"\"\"\n",
    "    # Extract data from row\n",
    "    enhancer_id = row_data['accession']\n",
    "    chrom_hg19 = row_data['chrom(hg19)']\n",
    "    start_hg19 = int(row_data['chromStart(hg19)'])\n",
    "    end_hg19 = int(row_data['chromEnd(hg19)'])\n",
    "    chrom = row_data['chrom']  # Current assembly chromosome\n",
    "    \n",
    "    # Use hg19 coordinates for centering\n",
    "    center_hg19 = (start_hg19 + end_hg19) // 2\n",
    "\n",
    "    # Create regular input array centered on hg19 coordinates\n",
    "    input_sample = create_input_array(data_path, input_shift, n_input_tracks, n_input_bins, \n",
    "                                     bw_input_track_list, enhancer_id, center_hg19, chrom_hg19)\n",
    "\n",
    "    if input_sample.shape == INPUT_SHAPE:\n",
    "        # Calculate enhancer boundaries in array coordinates\n",
    "        central_idx = n_input_bins // 2\n",
    "        start_idx = central_idx + (start_hg19 - center_hg19) // resolution\n",
    "        end_idx = central_idx + (end_hg19 - center_hg19) // resolution\n",
    "        \n",
    "        # Ensure indices are within bounds\n",
    "        start_idx = max(0, start_idx)\n",
    "        end_idx = min(n_input_bins, end_idx)\n",
    "\n",
    "        # Check H3K27ac activity within enhancer boundaries\n",
    "        h3k27ac_idx = 2  # Index for H3K27ac track\n",
    "        enhancer_mean = np.mean(input_sample[h3k27ac_idx, start_idx:end_idx])\n",
    "\n",
    "        if enhancer_mean > activity_threshold:\n",
    "            # Create output filename based on enhancer coordinates to handle duplicates\n",
    "            enhancer_coord_id = f\"{chrom_hg19}_{start_hg19}_{end_hg19}\"\n",
    "            \n",
    "            # Save regular array\n",
    "            enhancer_array_path = path / \"inputs\" / \"perturbed_landscape_arrays\" / \"human_enhancer_arrays\" / f\"{enhancer_coord_id}.npy\"\n",
    "            np.save(enhancer_array_path, input_sample)\n",
    "            \n",
    "            # Create and save perturbed array\n",
    "            perturbed_sample = create_perturbed_array(input_sample, start_idx, end_idx)\n",
    "            perturbed_array_path = path / \"inputs\" / \"perturbed_landscape_arrays\" / \"human_enhancer_arrays\" / f\"{enhancer_coord_id}_perturbed.npy\"\n",
    "            np.save(perturbed_array_path, perturbed_sample)\n",
    "            \n",
    "            # Record the mapping between accession ID and enhancer coordinate ID\n",
    "            return {\n",
    "                'accession_id': enhancer_id,\n",
    "                'enhancer_coord_id': enhancer_coord_id,\n",
    "                'gene': row_data['measuredGeneSymbol'],\n",
    "                'h3k27ac_mean': enhancer_mean\n",
    "            }\n",
    "        else:\n",
    "            logging.info(f\"{enhancer_id}: Enhancer activity below threshold ({enhancer_mean:.2f} ≤ {activity_threshold})\")\n",
    "            return None\n",
    "    else:\n",
    "        logging.info(f\"{enhancer_id}: Input did not match the expected shape.\")\n",
    "        return None\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Setup paths\n",
    "    path = Path(\"../\")\n",
    "    data_path = path / \"GEO_files\" / \"K562\"\n",
    "\n",
    "    # Create output directories\n",
    "    (path / \"inputs\" / \"perturbed_landscape_arrays\" / \"human_enhancer_arrays\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Start logger\n",
    "    LOG_FILENAME = \"/Enhancer_data_creation_human.log\"\n",
    "    logging.basicConfig(filename=str(path) + LOG_FILENAME, level=logging.INFO)\n",
    "\n",
    "    # Load enhancer-promoter pair annotations\n",
    "    ep_annotations_path = path / \"annotations\" / \"human\" / \"lifted_hg38_to_hg19\" / \"EPCrisprBenchmark_ensemble_data_GRCh38_hg19.txt\"\n",
    "    ep_df = pd.read_csv(ep_annotations_path, sep=\"\\t\")\n",
    "    \n",
    "    # Make sure numeric columns are properly formatted\n",
    "    # Handle potential comma issues in numeric columns\n",
    "    numeric_cols = ['chromStart(hg19)', 'chromEnd(hg19)', 'chromStart', 'chromEnd', 'startTSS', 'endTSS']\n",
    "    for col in numeric_cols:\n",
    "        if col in ep_df.columns:\n",
    "            # First convert any comma-formatted numbers to proper format\n",
    "            if ep_df[col].dtype == 'object':\n",
    "                ep_df[col] = ep_df[col].astype(str).str.replace(',', '.').astype(float)\n",
    "    \n",
    "    # Filter for specific chromosome if needed\n",
    "    # Uncomment and modify the line below to filter for a specific chromosome\n",
    "    # ep_df = ep_df[ep_df['chrom(hg19)'] == 'chr19']  \n",
    "\n",
    "    # Setup parameters\n",
    "    input_shift = 500050\n",
    "    n_input_bins = 10001\n",
    "    n_input_tracks = 4\n",
    "    INPUT_SHAPE = (4, 10001)\n",
    "    bw_input_track_list = [\"ATAC_Seq.bw\", \"H3K4me3.bw\", \"H3K27ac.bw\", \"H3K27me3.bw\"]\n",
    "    activity_threshold = 10\n",
    "\n",
    "    # Process enhancers in parallel\n",
    "    process_args = (data_path, bw_input_track_list, input_shift, n_input_tracks, n_input_bins, \n",
    "                    INPUT_SHAPE, path)\n",
    "    \n",
    "    # Use multiprocessing to process enhancers\n",
    "    with multiprocessing.Pool(processes=multiprocessing.cpu_count() - 1) as pool:\n",
    "        results = pool.starmap(process_enhancer, \n",
    "                    [(process_args + (row, 100, activity_threshold)) for _, row in ep_df.iterrows()])\n",
    "        \n",
    "    # Filter out None results and create results DataFrame\n",
    "    valid_results = [r for r in results if r is not None]\n",
    "    \n",
    "    # Create a mapping DataFrame\n",
    "    mapping_df = pd.DataFrame(valid_results)\n",
    "    \n",
    "    # Save the mapping between accession IDs and enhancer coordinate IDs\n",
    "    mapping_df.to_csv(path / \"annotations\" / \"human\" / \"enhancer_mapping.tsv\", sep=\"\\t\", index=False)\n",
    "    \n",
    "    # Count unique enhancers processed\n",
    "    unique_enhancers = mapping_df['enhancer_coord_id'].nunique()\n",
    "    logging.info(f\"Processed {len(valid_results)} enhancer-gene pairs, representing {unique_enhancers} unique enhancers\")\n",
    "    print(f\"Processed {len(valid_results)} enhancer-gene pairs, representing {unique_enhancers} unique enhancers\")\n",
    "    \n",
    "    # Create dummy target files\n",
    "    def get_file_ids(directory):\n",
    "        \"\"\"Get all file IDs from a directory without the .npy extension\"\"\"\n",
    "        return [f.stem for f in Path(directory).glob('*.npy')]\n",
    "\n",
    "    def create_unified_target_file(baseline_ids, output_path, n_bins=200):\n",
    "        \"\"\"Create a unified target file with zeros for both regular and perturbed IDs\"\"\"\n",
    "        # Create column names\n",
    "        columns = ['ID'] + [f'{i}_ctrl' for i in range(-n_bins, n_bins+1)]\n",
    "        \n",
    "        # Create DataFrame with zeros\n",
    "        df = pd.DataFrame(0, index=range(len(baseline_ids)), columns=columns)\n",
    "        \n",
    "        # Fill ID column\n",
    "        df['ID'] = baseline_ids\n",
    "        \n",
    "        # Save to CSV\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"Created {output_path} with {len(baseline_ids)} entries\")\n",
    "\n",
    "    # Setup paths for target creation\n",
    "    baseline_dir = path / \"inputs\" / \"perturbed_landscape_arrays\" / \"human_enhancer_arrays\"\n",
    "    output_dir = path / \"targets\" / \"K562\"\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    # Get IDs from both directories\n",
    "    baseline_ids = get_file_ids(baseline_dir)\n",
    "\n",
    "    # Create unified target file\n",
    "    create_unified_target_file(baseline_ids,\n",
    "                              output_dir / \"Enhancer_centered_targets_human.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing the model on K562 to predict POLR2A on enhancer centered, perturbed samples**\n",
    "\n",
    "```bash\n",
    "eirpredict \\\n",
    "--global_configs ./configurations/conf_pure_conv_K562_POLR2A_enhancer_centric/globals.yaml \\\n",
    "--input_configs ./configurations/conf_pure_conv_K562_POLR2A_enhancer_centric/input_cnn.yaml \\\n",
    "--fusion_configs ./configurations/conf_pure_conv_K562_POLR2A_enhancer_centric/fusion.yaml \\\n",
    "--output_configs ./configurations/conf_pure_conv_K562_POLR2A_enhancer_centric/outputs_2_cond.yaml \\\n",
    "--evaluate \\\n",
    "--model_path ./runs/gene_expression_only_chrom_K562_POLR2A_train/saved_models/gene_expression_only_chrom_K562_POLR2A_train_model_30300_perf-average=0.5586.pt \\\n",
    "--output_folder ./runs/perturbation_runs/gene_expression_only_chrom_K562_POLR2A_enhancer_centric\n",
    "```\n",
    "\n",
    "**Testing the model on K562 to predict RNA-seq of enhancer-centered, perturbed samples**\n",
    "\n",
    ">Nothing changes on globals, the only change is the model to use for the predictions, i.e. the one aimed to predict mature RNA-seq profiles, and the output folder name:\n",
    "\n",
    "```bash\n",
    "eirpredict \\\n",
    "--global_configs ./configurations/conf_pure_conv_K562_POLR2A_enhancer_centric/globals.yaml \\\n",
    "--input_configs ./configurations/conf_pure_conv_K562_POLR2A_enhancer_centric/input_cnn.yaml \\\n",
    "--fusion_configs ./configurations/conf_pure_conv_K562_POLR2A_enhancer_centric/fusion.yaml \\\n",
    "--output_configs ./configurations/conf_pure_conv_K562_POLR2A_enhancer_centric/outputs_2_cond.yaml \\\n",
    "--evaluate \\\n",
    "--model_path ./runs/gene_expression_only_chrom_K562_train/saved_models/gene_expression_only_chrom_K562_train_model_60600_perf-average=0.5028.pt \\\n",
    "--output_folder ./runs/perturbation_runs/gene_expression_only_chrom_K562_RNA_enhancer_centric\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enhancer-gene pairing analysis on predicted RNA-seq and POLR2A Chip-seq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from scipy.integrate import simps\n",
    "import re\n",
    "from typing import List, Tuple, Dict, Any, Callable, Optional, Union\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "def load_annotations(gene_path):\n",
    "    \"\"\"Load gene annotations.\"\"\"\n",
    "    return pd.read_csv(gene_path, sep='\\t')\n",
    "\n",
    "def _get_predictions(\n",
    "    results_path: Path,\n",
    "    N_BINS: int,\n",
    "    condition_list: List[str],\n",
    "    is_training: bool = False,\n",
    "    batch_num: str = None,\n",
    "    CLIP: bool = True\n",
    ") -> Tuple[List[str], pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Reads prediction files from EIR for each target bin condition.\n",
    "    \n",
    "    Args:\n",
    "        results_path: Path where the predictions are stored\n",
    "        N_BINS: Number of bins to one side of the central bin (e.g., 200 or 57 for Enformer)\n",
    "        condition_list: List of conditions (e.g., [\"_ctrl\"])\n",
    "        is_training: If True, uses training file structure, else uses test structure\n",
    "        batch_num: Batch number for training predictions (e.g., \"30300\")\n",
    "        CLIP: If True, applies ReLU to predictions (clips at 0)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "            - ids: List of sample names\n",
    "            - predicted: DataFrame of predicted untransformed values\n",
    "            - actual: DataFrame of actual values\n",
    "    \"\"\"\n",
    "    predicted_dfs = []\n",
    "    actual_dfs = []\n",
    "    \n",
    "    for condition in condition_list:\n",
    "        for i in range(-N_BINS, N_BINS + 1):\n",
    "            if is_training:\n",
    "                if batch_num is None:\n",
    "                    raise ValueError(\"batch_num must be specified for training predictions\")\n",
    "                file_path = results_path / f\"expression_output/{i}{condition}/samples/{batch_num}/regression_predictions.csv\"\n",
    "            else:\n",
    "                file_path = results_path / f\"expression_output/{i}{condition}/predictions.csv\"\n",
    "            \n",
    "            if not file_path.exists():\n",
    "                raise FileNotFoundError(f\"Prediction file not found: {file_path}\")\n",
    "                \n",
    "            predictions = pd.read_csv(file_path).set_index('ID')\n",
    "\n",
    "            # Apply ReLU to model predictions if specified\n",
    "            if CLIP:\n",
    "                predictions = predictions.clip(lower=0)\n",
    "\n",
    "            # Handle different column names for training vs test\n",
    "            if is_training:\n",
    "                predicted_column = predictions[\"Predicted\"].rename(f\"{i}{condition}\")\n",
    "                actual_column = predictions[\"Actual\"].rename(f\"{i}{condition}\")\n",
    "            else:\n",
    "                predicted_column = predictions[f\"{i}{condition} Untransformed\"].rename(f\"{i}{condition}\")\n",
    "                actual_column = predictions[\"True Label Untransformed\"].rename(f\"{i}{condition}\")\n",
    "\n",
    "            predicted_dfs.append(predicted_column)\n",
    "            actual_dfs.append(actual_column)\n",
    "\n",
    "    # Concatenate all DataFrames horizontally\n",
    "    predicted = pd.concat(predicted_dfs, axis=1)\n",
    "    actual = pd.concat(actual_dfs, axis=1)\n",
    "    ids = list(predicted.index)\n",
    "\n",
    "    return ids, predicted, actual\n",
    "\n",
    "def get_window_boundaries(enhancer_center, window_size=200500):\n",
    "    \"\"\"Calculate window boundaries.\"\"\"\n",
    "    return enhancer_center - window_size, enhancer_center + window_size\n",
    "\n",
    "def find_genes_in_window(genes_df, chrom, window_start, window_end):\n",
    "    \"\"\"Find genes fully contained within window.\"\"\"\n",
    "    return genes_df[\n",
    "        (genes_df['chr'] == chrom) & \n",
    "        (genes_df['Start'] >= window_start) & \n",
    "        (genes_df['End'] <= window_end)\n",
    "    ].copy()\n",
    "\n",
    "def get_gene_tss(gene_row):\n",
    "    \"\"\"Get TSS position based on strand.\"\"\"\n",
    "    return gene_row['Start'] if gene_row['Strand'] == '+' else gene_row['End']\n",
    "\n",
    "def calculate_distance_and_order(window_genes, enhancer_center):\n",
    "    \"\"\"Calculate distances to enhancer and determine gene order.\"\"\"\n",
    "    distances = []\n",
    "    for _, gene in window_genes.iterrows():\n",
    "        tss = get_gene_tss(gene)\n",
    "        distance = abs(enhancer_center - tss)\n",
    "        distances.append({\n",
    "            'gene_id': gene['ID'],\n",
    "            'distance': distance,\n",
    "            'tss': tss\n",
    "        })\n",
    "    \n",
    "    sorted_distances = sorted(distances, key=lambda x: x['distance'])\n",
    "    \n",
    "    return {d['gene_id']: {\n",
    "        'distance': d['distance'],\n",
    "        'order': idx + 1,\n",
    "        'tss': d['tss']\n",
    "    } for idx, d in enumerate(sorted_distances)}\n",
    "\n",
    "def check_gene_adjacency(window_genes, distance_map, gene_id):\n",
    "    \"\"\"Check if a gene is adjacent to the enhancer.\"\"\"\n",
    "    gene_tss = distance_map[gene_id]['tss']\n",
    "    gene_distance = distance_map[gene_id]['distance']\n",
    "    \n",
    "    for _, other_gene in window_genes.iterrows():\n",
    "        if other_gene['ID'] == gene_id:\n",
    "            continue\n",
    "            \n",
    "        other_tss = get_gene_tss(other_gene)\n",
    "        other_distance = abs(other_tss - gene_tss)\n",
    "        \n",
    "        if other_distance < gene_distance:\n",
    "            return False\n",
    "            \n",
    "    return True\n",
    "\n",
    "def get_bin_coordinates(pos, enhancer_center, window_size, resolution):\n",
    "    \"\"\"Convert genomic position to bin coordinate.\"\"\"\n",
    "    rel_pos = pos - enhancer_center\n",
    "    bin_pos = (rel_pos + window_size) // resolution\n",
    "    return int(bin_pos)\n",
    "\n",
    "def get_gene_bin_coordinates(gene, enhancer_center, window_size, resolution):\n",
    "    \"\"\"Get bin coordinates for TSS, gene start, and gene end.\"\"\"\n",
    "    tss = get_gene_tss(gene)\n",
    "    tss_bin = get_bin_coordinates(tss, enhancer_center, window_size, resolution)\n",
    "    \n",
    "    start_bin = get_bin_coordinates(gene['Start'], enhancer_center, window_size, resolution)\n",
    "    end_bin = get_bin_coordinates(gene['End'], enhancer_center, window_size, resolution)\n",
    "    \n",
    "    return tss_bin, start_bin, end_bin\n",
    "\n",
    "# Abstract integration method for different prediction types\n",
    "def calculate_area(\n",
    "    predictions: np.ndarray, \n",
    "    integration_type: str,\n",
    "    gene_strand: str,\n",
    "    tss_bin: int = None,\n",
    "    gene_start_bin: int = None,\n",
    "    gene_end_bin: int = None,\n",
    "    upstream_bins: int = 2,\n",
    "    downstream_bins: int = 3\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate area under the curve using different integration methods.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Array of prediction values\n",
    "        integration_type: Type of integration ('polr2a_tss' or 'rnaseq_gene_body')\n",
    "        gene_strand: Strand of the gene ('+' or '-')\n",
    "        tss_bin: Bin index of TSS (required for 'polr2a_tss')\n",
    "        gene_start_bin: Start bin of gene (required for 'rnaseq_gene_body')\n",
    "        gene_end_bin: End bin of gene (required for 'rnaseq_gene_body')\n",
    "        upstream_bins: Number of bins to include upstream of TSS\n",
    "        downstream_bins: Number of bins to include downstream of TSS\n",
    "        \n",
    "    Returns:\n",
    "        Normalized area under the curve\n",
    "    \"\"\"\n",
    "    total_bins = len(predictions)\n",
    "    \n",
    "    if integration_type == 'polr2a_tss':\n",
    "        if tss_bin is None:\n",
    "            raise ValueError(\"tss_bin must be provided for polr2a_tss integration\")\n",
    "            \n",
    "        # For negative strand genes, swap upstream and downstream\n",
    "        if gene_strand == '-':\n",
    "            upstream_bins, downstream_bins = downstream_bins, upstream_bins\n",
    "            \n",
    "        # Define the region of interest around TSS\n",
    "        start_bin = max(0, tss_bin - upstream_bins)\n",
    "        end_bin = min(total_bins - 1, tss_bin + downstream_bins)\n",
    "        \n",
    "    elif integration_type == 'rnaseq_gene_body':\n",
    "        if gene_start_bin is None or gene_end_bin is None:\n",
    "            raise ValueError(\"gene_start_bin and gene_end_bin must be provided for rnaseq_gene_body integration\")\n",
    "            \n",
    "        # Get bins within gene body, clipped to prediction boundaries\n",
    "        start_bin = max(0, gene_start_bin)\n",
    "        end_bin = min(total_bins - 1, gene_end_bin)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown integration_type: {integration_type}\")\n",
    "        \n",
    "    # Extract values in the selected region\n",
    "    region_values = predictions[start_bin:end_bin + 1]\n",
    "    \n",
    "    if len(region_values) == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Calculate area using Simpson's rule\n",
    "    x = np.arange(len(region_values))\n",
    "    \n",
    "    # Ensure x and y have the same dimensions before using Simpson's rule\n",
    "    if len(x) < 2:\n",
    "        return np.sum(region_values)  # For single point, just return the value\n",
    "    \n",
    "    # For arrays with only 2 points, use trapezoidal rule instead\n",
    "    if len(x) == 2:\n",
    "        return np.trapz(region_values, x)\n",
    "    \n",
    "    area = simps(region_values, x)\n",
    "    \n",
    "    # Normalize by length\n",
    "    length = end_bin - start_bin + 1\n",
    "    #return area / length if length > 0 else 0\n",
    "    return area if length > 0 else 0\n",
    "\n",
    "def process_enhancer_predictions(\n",
    "    enhancer_id: str, \n",
    "    predictions_df: pd.DataFrame, \n",
    "    genes_df: pd.DataFrame, \n",
    "    integration_type: str,\n",
    "    window_size: int = 200500, \n",
    "    resolution: int = 1000,\n",
    "    upstream_bins: int = 2, \n",
    "    downstream_bins: int = 3\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Process predictions for one enhancer and its genes.\n",
    "    \n",
    "    Args:\n",
    "        enhancer_id: ID of the enhancer\n",
    "        predictions_df: DataFrame with predictions for all enhancers\n",
    "        genes_df: DataFrame with gene annotations\n",
    "        integration_type: Type of integration ('polr2a_tss' or 'rnaseq_gene_body')\n",
    "        window_size: Size of the window around enhancer center\n",
    "        resolution: Resolution of the data in base pairs per bin\n",
    "        upstream_bins: Number of bins to include upstream of TSS (for polr2a_tss)\n",
    "        downstream_bins: Number of bins to include downstream of TSS (for polr2a_tss)\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries with results for each enhancer-gene pair\n",
    "    \"\"\"\n",
    "    # Extract enhancer information\n",
    "    match = re.match(r'(chr\\w+)_(\\d+)_(\\d+)', enhancer_id)\n",
    "    \n",
    "    if not match:\n",
    "        print(f\"Warning: Could not parse enhancer ID: {enhancer_id}\")\n",
    "        return []\n",
    "    \n",
    "    chrom, start, end = match.groups()\n",
    "    start, end = int(start), int(end)\n",
    "    enhancer_center = (start + end) // 2\n",
    "    \n",
    "    # Check if we have both regular and perturbed predictions\n",
    "    perturbed_id = f\"{enhancer_id}_perturbed\"\n",
    "    if perturbed_id not in predictions_df.index:\n",
    "        print(f\"Warning: Perturbed predictions not found for {enhancer_id}\")\n",
    "        return []\n",
    "    \n",
    "    # Get prediction data\n",
    "    baseline_series = predictions_df.loc[enhancer_id]\n",
    "    perturbed_series = predictions_df.loc[perturbed_id]\n",
    "    \n",
    "    # Convert series to numpy arrays for area calculation\n",
    "    baseline_array = baseline_series.values\n",
    "    perturbed_array = perturbed_series.values\n",
    "    \n",
    "    # Determine window boundaries and find genes in window\n",
    "    window_start, window_end = get_window_boundaries(enhancer_center, window_size)\n",
    "    window_genes = find_genes_in_window(genes_df, chrom, window_start, window_end)\n",
    "    \n",
    "    if len(window_genes) == 0:\n",
    "        return []\n",
    "    \n",
    "    distance_map = calculate_distance_and_order(window_genes, enhancer_center)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    try:\n",
    "        # Calculate areas for all genes in window\n",
    "        area_results = []\n",
    "        \n",
    "        for _, gene in window_genes.iterrows():\n",
    "            gene_strand = gene['Strand']\n",
    "            \n",
    "            # Get necessary bin coordinates\n",
    "            tss_bin, gene_start_bin, gene_end_bin = get_gene_bin_coordinates(\n",
    "                gene, enhancer_center, window_size, resolution\n",
    "            )\n",
    "            \n",
    "            # Calculate areas based on integration type\n",
    "            if integration_type == 'polr2a_tss':\n",
    "                baseline_area = calculate_area(\n",
    "                    baseline_array, integration_type, gene_strand, \n",
    "                    tss_bin=tss_bin, upstream_bins=upstream_bins, downstream_bins=downstream_bins\n",
    "                )\n",
    "                perturbed_area = calculate_area(\n",
    "                    perturbed_array, integration_type, gene_strand, \n",
    "                    tss_bin=tss_bin, upstream_bins=upstream_bins, downstream_bins=downstream_bins\n",
    "                )\n",
    "            elif integration_type == 'rnaseq_gene_body':\n",
    "                baseline_area = calculate_area(\n",
    "                    baseline_array, integration_type, gene_strand,\n",
    "                    gene_start_bin=gene_start_bin, gene_end_bin=gene_end_bin\n",
    "                )\n",
    "                perturbed_area = calculate_area(\n",
    "                    perturbed_array, integration_type, gene_strand,\n",
    "                    gene_start_bin=gene_start_bin, gene_end_bin=gene_end_bin\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown integration_type: {integration_type}\")\n",
    "            \n",
    "            # Calculate gene length in bp\n",
    "            gene_length_bp = gene['End'] - gene['Start']\n",
    "            \n",
    "            # Check if gene is adjacent to enhancer\n",
    "            is_adjacent = check_gene_adjacency(window_genes, distance_map, gene['ID'])\n",
    "            \n",
    "            # Store area calculations\n",
    "            area_result = {\n",
    "                'gene_id': gene['ID'],\n",
    "                'gene_name': gene['Name'],\n",
    "                'gene_type': gene['type'],\n",
    "                'gene_strand': gene_strand,\n",
    "                'gene_length': gene_length_bp,\n",
    "                'distance_to_enhancer': distance_map[gene['ID']]['distance'],\n",
    "                'gene_order': distance_map[gene['ID']]['order'],\n",
    "                'is_adjacent': is_adjacent,\n",
    "                'baseline_area': baseline_area,\n",
    "                'perturbed_area': perturbed_area,\n",
    "                'area_difference': baseline_area - perturbed_area,\n",
    "                'abs_area_difference': abs(baseline_area - perturbed_area)\n",
    "            }\n",
    "            \n",
    "            area_results.append(area_result)\n",
    "        \n",
    "        # Calculate z-scores for area differences within this window\n",
    "        area_diffs = [r['abs_area_difference'] for r in area_results]\n",
    "        if len(area_diffs) > 1:  # Need at least 2 points for z-score\n",
    "            mean_diff = np.mean(area_diffs)\n",
    "            std_diff = np.std(area_diffs, ddof=1)  # Use sample standard deviation\n",
    "            \n",
    "            if std_diff > 0:  # Avoid division by zero\n",
    "                for result in area_results:\n",
    "                    result['z_score'] = (result['abs_area_difference'] - mean_diff) / std_diff\n",
    "            else:\n",
    "                for result in area_results:\n",
    "                    result['z_score'] = 0.0\n",
    "        else:\n",
    "            for result in area_results:\n",
    "                result['z_score'] = 0.0\n",
    "        \n",
    "        # Calculate ratio to maximum score\n",
    "        max_diff = max(area_diffs) if area_diffs else 1.0\n",
    "        if max_diff > 0:  # Avoid division by zero\n",
    "            for result in area_results:\n",
    "                result['ratio_to_max'] = result['abs_area_difference'] / max_diff\n",
    "        else:\n",
    "            for result in area_results:\n",
    "                result['ratio_to_max'] = 0.0\n",
    "        \n",
    "        # Create final result entries\n",
    "        for result in area_results:\n",
    "            # Create result entry\n",
    "            entry = {\n",
    "                'ID': f\"{enhancer_id}_{result['gene_id']}\",\n",
    "                'enhancer_id': enhancer_id,\n",
    "                'baseline_area': result['baseline_area'],\n",
    "                'perturbed_area': result['perturbed_area'],\n",
    "                'area_difference': result['area_difference'],\n",
    "                'abs_area_difference': result['abs_area_difference'] ,\n",
    "                'z_score': result['z_score'],\n",
    "                'ratio_to_max': result['ratio_to_max'],\n",
    "                'gene_id': result['gene_id'],\n",
    "                'gene_name': result['gene_name'],\n",
    "                'distance_to_enhancer': result['distance_to_enhancer'],\n",
    "                'gene_order': result['gene_order'],\n",
    "                'is_adjacent': result['is_adjacent'],\n",
    "                'gene_type': result['gene_type'],\n",
    "                'gene_strand': result['gene_strand'],\n",
    "                'gene_length': result['gene_length'],\n",
    "                'integration_type': integration_type\n",
    "            }\n",
    "            \n",
    "            results.append(entry)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing enhancer {enhancer_id}: {str(e)}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def identify_primary_target_genes(results_df, score_method='abs_area_difference'):\n",
    "    \"\"\"\n",
    "    Identify the primary target gene for each enhancer based on the specified scoring method.\n",
    "    \n",
    "    Args:\n",
    "        results_df: DataFrame with enhancer-gene pair results\n",
    "        score_method: Scoring method to use ('abs_area_difference', 'z_score', or 'ratio_to_max')\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with added 'is_primary_target' column\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    df = results_df.copy()\n",
    "    \n",
    "    if score_method not in df.columns:\n",
    "        print(f\"Warning: Score method '{score_method}' not found in results. Using 'abs_area_difference' instead.\")\n",
    "        score_method = 'abs_area_difference'\n",
    "    \n",
    "    # Group by enhancer and find gene with maximum score\n",
    "    primary_targets = df.loc[df.groupby('enhancer_id')[score_method].idxmax()]\n",
    "    \n",
    "    # Create a unique ID for each enhancer-gene pair\n",
    "    df['pair_id'] = df['enhancer_id'] + '_' + df['gene_id']\n",
    "    primary_targets['pair_id'] = primary_targets['enhancer_id'] + '_' + primary_targets['gene_id']\n",
    "    \n",
    "    # Mark primary target genes\n",
    "    df['is_primary_target'] = df['pair_id'].isin(primary_targets['pair_id'])\n",
    "    \n",
    "    # Create is_closest flag (for the closest gene by distance)\n",
    "    closest_genes = df.loc[df.groupby('enhancer_id')['distance_to_enhancer'].idxmin()]\n",
    "    closest_genes['pair_id'] = closest_genes['enhancer_id'] + '_' + closest_genes['gene_id']\n",
    "    df['is_closest'] = df['pair_id'].isin(closest_genes['pair_id'])\n",
    "    \n",
    "    # Remove temporary column\n",
    "    df = df.drop('pair_id', axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def plot_confusion_matrix(results_df, score_method, output_path):\n",
    "    \"\"\"\n",
    "    Create confusion matrix comparing primary target identification methods.\n",
    "    \n",
    "    Args:\n",
    "        results_df: DataFrame with results including is_primary_target and is_closest flags\n",
    "        score_method: Scoring method used to identify primary targets\n",
    "        output_path: Path to save the visualization\n",
    "    \"\"\"\n",
    "    # Create figure\n",
    "    plt.figure(figsize=(4.5, 4.5))\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(results_df['is_primary_target'], results_df['is_closest'])\n",
    "    \n",
    "    # Plot using seaborn\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "               xticklabels=['Not Closest', 'Closest'],\n",
    "               yticklabels=['Not Primary Target', 'Primary Target'])\n",
    "    \n",
    "    plt.xlabel('Proximity-based Prediction')\n",
    "    plt.ylabel(f'{score_method.replace(\"_\", \" \").title()}-based Prediction')\n",
    "    plt.title(f'Primary Target Gene Identification: {score_method.replace(\"_\", \" \").title()} vs. Proximity')\n",
    "    \n",
    "    # Add accuracy and other metrics\n",
    "    accuracy = (cm[0, 0] + cm[1, 1]) / cm.sum()\n",
    "    plt.figtext(0.5, 0.01, f'Accuracy: {accuracy:.3f}', ha='center')\n",
    "    \n",
    "    # Save figure\n",
    "    out_file = output_path / f'target_identification_confusion_{score_method}.png'\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Generate classification report\n",
    "    report = classification_report(\n",
    "        results_df['is_primary_target'], \n",
    "        results_df['is_closest'],\n",
    "        target_names=['Not Primary Target', 'Primary Target'],\n",
    "        output_dict=True\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'confusion_matrix': cm,\n",
    "        'accuracy': accuracy,\n",
    "        'classification_report': report\n",
    "    }\n",
    "\n",
    "def visualize_enhancer_predictions(\n",
    "    enhancer_id: str,\n",
    "    predictions_df: pd.DataFrame,\n",
    "    genes_df: pd.DataFrame,\n",
    "    integration_type: str,\n",
    "    window_size: int = 200500,\n",
    "    resolution: int = 1000,\n",
    "    upstream_bins: int = 2,\n",
    "    downstream_bins: int = 3,\n",
    "    save_path: Path = None,\n",
    "    show_plot: bool = True\n",
    ") -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Visualize predictions and integration domains for an enhancer.\n",
    "    \"\"\"\n",
    "    # Extract enhancer information\n",
    "    match = re.match(r'(chr\\w+)_(\\d+)_(\\d+)', enhancer_id)\n",
    "    \n",
    "    if not match:\n",
    "        print(f\"Warning: Could not parse enhancer ID: {enhancer_id}\")\n",
    "        return None\n",
    "    \n",
    "    chrom, start, end = match.groups()\n",
    "    start, end = int(start), int(end)\n",
    "    enhancer_center = (start + end) // 2\n",
    "    \n",
    "    # Check if we have both regular and perturbed predictions\n",
    "    perturbed_id = f\"{enhancer_id}_perturbed\"\n",
    "    if perturbed_id not in predictions_df.index:\n",
    "        print(f\"Warning: Perturbed predictions not found for {enhancer_id}\")\n",
    "        return None\n",
    "    \n",
    "    # Get prediction data\n",
    "    baseline_series = predictions_df.loc[enhancer_id]\n",
    "    perturbed_series = predictions_df.loc[perturbed_id]\n",
    "    \n",
    "    # Get positions for x-axis (relative to enhancer center)\n",
    "    bin_range = range(-window_size//resolution, window_size//resolution + 1)  # inclusive\n",
    "    positions = np.array(bin_range) * resolution\n",
    "    \n",
    "    # Ensure positions and values arrays have the same length\n",
    "    if len(positions) != len(baseline_series.values):\n",
    "        # Adjust the smaller array to match the larger one\n",
    "        min_length = min(len(positions), len(baseline_series.values))\n",
    "        positions = positions[:min_length]\n",
    "        baseline_values = baseline_series.values[:min_length]\n",
    "        perturbed_values = perturbed_series.values[:min_length]\n",
    "    else:\n",
    "        baseline_values = baseline_series.values\n",
    "        perturbed_values = perturbed_series.values\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    \n",
    "    # Plot baseline and perturbed predictions\n",
    "    ax.plot(positions, baseline_values, label='Baseline', color='blue', alpha=0.7)\n",
    "    ax.plot(positions, perturbed_values, label='Perturbed', color='red', alpha=0.7)\n",
    "    \n",
    "    # Determine window boundaries and find genes in window\n",
    "    window_start, window_end = get_window_boundaries(enhancer_center, window_size)\n",
    "    window_genes = find_genes_in_window(genes_df, chrom, window_start, window_end)\n",
    "    \n",
    "    # Mark enhancer position\n",
    "    ax.axvline(x=0, color='black', linestyle='--', alpha=0.5, label='Enhancer Center')\n",
    "    ax.axvspan(start - enhancer_center, end - enhancer_center, color='gray', alpha=0.2, label='Enhancer Region')\n",
    "    \n",
    "    # Add gene visualization\n",
    "    colors = plt.cm.tab10.colors\n",
    "    \n",
    "    # Calculate y-axis limits for proper positioning of gene visualizations\n",
    "    y_min, y_max = ax.get_ylim()\n",
    "    highlight_height = 0.1 * (y_max - y_min)  # Height for visualization elements\n",
    "    \n",
    "    # Add legend entries for genes\n",
    "    gene_legend_handles = []\n",
    "    \n",
    "    for i, (_, gene) in enumerate(window_genes.iterrows()):\n",
    "        gene_strand = gene['Strand']\n",
    "        gene_color = colors[i % len(colors)]\n",
    "        \n",
    "        # Get necessary bin coordinates\n",
    "        tss_bin, gene_start_bin, gene_end_bin = get_gene_bin_coordinates(\n",
    "            gene, enhancer_center, window_size, resolution\n",
    "        )\n",
    "        \n",
    "        # Convert bin positions to genomic coordinates relative to enhancer center\n",
    "        tss_rel = get_gene_tss(gene) - enhancer_center\n",
    "        start_rel = gene['Start'] - enhancer_center\n",
    "        end_rel = gene['End'] - enhancer_center\n",
    "        \n",
    "        # Plot TSS vertical line\n",
    "        tss_line = ax.axvline(x=tss_rel, color=gene_color, linestyle='-', alpha=0.7)\n",
    "        \n",
    "        # Highlight integration regions based on type\n",
    "        if integration_type == 'polr2a_tss':\n",
    "            # For POLR2A, highlight region around TSS\n",
    "            if gene_strand == '+':\n",
    "                domain_start = tss_rel - (upstream_bins * resolution)\n",
    "                domain_end = tss_rel + (downstream_bins * resolution)\n",
    "            else:  # '-' strand\n",
    "                domain_start = tss_rel - (downstream_bins * resolution)\n",
    "                domain_end = tss_rel + (upstream_bins * resolution)\n",
    "            \n",
    "            # Shade integration domain\n",
    "            ax.axvspan(domain_start, domain_end, color=gene_color, alpha=0.3)\n",
    "            \n",
    "        elif integration_type == 'rnaseq_gene_body':\n",
    "            # For RNA-seq, highlight entire gene body\n",
    "            rect_y_bottom = y_min\n",
    "            rect_y_top = y_min + highlight_height\n",
    "            \n",
    "            # Create rectangle for gene body\n",
    "            from matplotlib.patches import Rectangle\n",
    "            gene_rect = Rectangle((start_rel, rect_y_bottom), end_rel - start_rel, highlight_height,\n",
    "                                 facecolor=gene_color, edgecolor='none', alpha=0.3)\n",
    "            ax.add_patch(gene_rect)\n",
    "            \n",
    "            # Add arrow to indicate strand\n",
    "            arrow_y = rect_y_bottom + highlight_height / 2\n",
    "            arrow_length = min(0.1 * (end_rel - start_rel), 5000)  # 10% of gene length or max 5kb\n",
    "            \n",
    "            if gene_strand == '+':\n",
    "                arrow_x = end_rel - arrow_length\n",
    "                dx = arrow_length\n",
    "            else:  # '-' strand\n",
    "                arrow_x = start_rel\n",
    "                dx = -arrow_length\n",
    "                \n",
    "            ax.arrow(arrow_x, arrow_y, dx, 0, head_width=highlight_height*0.8, \n",
    "                    head_length=min(2000, arrow_length*0.3), fc=gene_color, ec=gene_color)\n",
    "        \n",
    "        # Create custom legend entry for this gene\n",
    "        from matplotlib.patches import Patch\n",
    "        gene_patch = Patch(color=gene_color, alpha=0.7, \n",
    "                         label=f\"{gene['Name']} ({gene['ID']}, {gene_strand})\")\n",
    "        gene_legend_handles.append(gene_patch)\n",
    "    \n",
    "    # Set x-axis limits to match the data range\n",
    "    ax.set_xlim(-window_size, window_size)\n",
    "    \n",
    "    # Set x-axis ticks in kbp\n",
    "    x_ticks = np.arange(-window_size, window_size + 1, 50000)  # Every 50 kbp\n",
    "    x_tick_labels = [f\"{x/1000:.0f}\" for x in x_ticks]  # Format as integer\n",
    "    ax.set_xticks(x_ticks)\n",
    "    ax.set_xticklabels(x_tick_labels)\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('Position relative to enhancer center (kbp)')\n",
    "    ax.set_ylabel('Prediction value')\n",
    "    method_name = \"POLR2A around TSS\" if integration_type == 'polr2a_tss' else \"RNA-seq across gene body\"\n",
    "    ax.set_title(f'Enhancer {enhancer_id} predictions ({method_name})')\n",
    "    \n",
    "    # Add vertical line legend entries\n",
    "    line_legend_handles, line_legend_labels = ax.get_legend_handles_labels()\n",
    "    \n",
    "    # Combine both legend entries\n",
    "    all_handles = line_legend_handles + gene_legend_handles\n",
    "    \n",
    "    # Add legend (positioned outside the plot)\n",
    "    ax.legend(handles=all_handles, bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure if path is provided\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "    \n",
    "    # Show plot if requested\n",
    "    if show_plot:\n",
    "        plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def visualize_primary_targets(results_df, score_method, output_path):\n",
    "    \"\"\"\n",
    "    Visualize statistics about primary targets vs. proximity-based targets.\n",
    "    \n",
    "    Args:\n",
    "        results_df: DataFrame with results including primary target flags\n",
    "        score_method: Scoring method used to identify primary targets\n",
    "        output_path: Path to save visualizations\n",
    "    \"\"\"\n",
    "    # Create figure for distance distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Create distance histograms for primary targets vs non-primary\n",
    "    sns.histplot(data=results_df, x='distance_to_enhancer', hue='is_primary_target',\n",
    "                multiple='dodge', bins=30, log_scale=(False, True))\n",
    "    \n",
    "    plt.xlabel('Distance to Enhancer (bp)')\n",
    "    plt.ylabel('Count (log scale)')\n",
    "    plt.title(f'Distance Distribution: {score_method.replace(\"_\", \" \").title()} Primary Target Genes vs. Others')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path / f'primary_target_distance_distribution_{score_method}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create figure for gene order comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Count occurrences of each gene order for primary targets\n",
    "    order_counts = results_df[results_df['is_primary_target']]['gene_order'].value_counts().sort_index()\n",
    "    \n",
    "    # Plot gene order distribution\n",
    "    sns.barplot(x=order_counts.index, y=order_counts.values)\n",
    "    \n",
    "    plt.xlabel('Gene Order (by Distance)')\n",
    "    plt.ylabel('Count of Primary Targets')\n",
    "    plt.title(f'Distribution of {score_method.replace(\"_\", \" \").title()} Primary Target Genes by Distance Order')\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path / f'primary_target_order_distribution_{score_method}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def batch_process_enhancers(\n",
    "    enhancer_ids: List[str],\n",
    "    predictions_df: pd.DataFrame,\n",
    "    genes_df: pd.DataFrame,\n",
    "    integration_type: str,\n",
    "    output_path: Path,\n",
    "    visualize_top_n: int = 10,\n",
    "    score_methods: List[str] = ['abs_area_difference', 'z_score', 'ratio_to_max'],\n",
    "    **kwargs\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process multiple enhancers and generate visualizations.\n",
    "    \n",
    "    Args:\n",
    "        enhancer_ids: List of enhancer IDs to process\n",
    "        predictions_df: DataFrame with predictions\n",
    "        genes_df: DataFrame with gene annotations\n",
    "        integration_type: Type of integration ('polr2a_tss' or 'rnaseq_gene_body')\n",
    "        output_path: Path to save results\n",
    "        visualize_top_n: Number of top enhancers to visualize\n",
    "        score_methods: List of scoring methods to use\n",
    "        **kwargs: Additional arguments for process_enhancer_predictions\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with combined results\n",
    "    \"\"\"\n",
    "    output_path.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Create visualization output directory\n",
    "    vis_output_path = output_path / f\"{integration_type}_visualizations\"\n",
    "    vis_output_path.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Process each enhancer\n",
    "    all_results = []\n",
    "    for enhancer_id in enhancer_ids:\n",
    "        results = process_enhancer_predictions(\n",
    "            enhancer_id, \n",
    "            predictions_df, \n",
    "            genes_df, \n",
    "            integration_type,\n",
    "            **kwargs\n",
    "        )\n",
    "        if results:\n",
    "            all_results.extend(results)\n",
    "    \n",
    "    # Save results\n",
    "    if all_results:\n",
    "        # Create results DataFrame\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        \n",
    "        # Process for each scoring method\n",
    "        all_results_with_targets = {}\n",
    "        \n",
    "        for score_method in score_methods:\n",
    "            # Add primary target flags based on current scoring method\n",
    "            results_with_targets = identify_primary_target_genes(results_df, score_method)\n",
    "            all_results_with_targets[score_method] = results_with_targets\n",
    "            \n",
    "            # Save method-specific results\n",
    "            results_with_targets.to_csv(output_path / f\"{integration_type}_{score_method}_results_with_targets.csv\", index=False)\n",
    "            \n",
    "            # Create visualization for primary target stats\n",
    "            visualize_primary_targets(results_with_targets, score_method, output_path)\n",
    "            \n",
    "            # Create confusion matrix for primary vs. closest identification\n",
    "            cm_results = plot_confusion_matrix(results_with_targets, score_method, output_path)\n",
    "            \n",
    "            print(f\"\\nProcessing complete for {integration_type} using {score_method}:\")\n",
    "            print(f\"Processed {len(enhancer_ids)} enhancers\")\n",
    "            print(f\"Found {len(results_df)} enhancer-gene pairs\")\n",
    "            print(f\"Primary target identification accuracy: {cm_results['accuracy']:.3f}\")\n",
    "        \n",
    "        # Save base results\n",
    "        results_df.to_csv(output_path / f\"{integration_type}_base_results.csv\", index=False)\n",
    "        \n",
    "        # Use the standard abs_area_difference for further operations\n",
    "        results_with_targets = all_results_with_targets['abs_area_difference']\n",
    "        \n",
    "        # Generate visualizations for top enhancers by effect size\n",
    "        if visualize_top_n > 0:\n",
    "            top_enhancers = results_df.groupby('enhancer_id')['abs_area_difference'].mean().nlargest(visualize_top_n).index.tolist()\n",
    "            print(f\"Visualizing top {len(top_enhancers)} enhancers by effect size...\")\n",
    "            \n",
    "            for enhancer_id in top_enhancers:\n",
    "                print(f\"  Visualizing {enhancer_id}...\")\n",
    "                save_path = vis_output_path / f\"{enhancer_id}_{integration_type}_visualization.png\"\n",
    "                \n",
    "                fig = visualize_enhancer_predictions(\n",
    "                    enhancer_id=enhancer_id,\n",
    "                    predictions_df=predictions_df,\n",
    "                    genes_df=genes_df,\n",
    "                    integration_type=integration_type,\n",
    "                    save_path=save_path,\n",
    "                    show_plot=False,\n",
    "                    **kwargs\n",
    "                )\n",
    "                \n",
    "                if fig:\n",
    "                    plt.close(fig)\n",
    "            \n",
    "            print(f\"Visualizations saved to {vis_output_path}\")\n",
    "        \n",
    "        return all_results_with_targets\n",
    "    else:\n",
    "        print(f\"\\nNo results were generated for {integration_type}. Please check the error messages above.\")\n",
    "        return {}\n",
    "\n",
    "def run_analysis(\n",
    "    base_path: Path,\n",
    "    gene_path: Path = None,\n",
    "    polr2a_path: Path = None,\n",
    "    rna_path: Path = None,\n",
    "    output_path: Path = None,\n",
    "    window_size: int = 200500,\n",
    "    resolution: int = 1000,\n",
    "    upstream_bins: int = 2,\n",
    "    downstream_bins: int = 3,\n",
    "    N_BINS: int = 200,\n",
    "    visualize_top_n: int = 10,\n",
    "    score_methods: List[str] = ['abs_area_difference', 'z_score', 'ratio_to_max']\n",
    "):\n",
    "    \"\"\"\n",
    "    Run the full analysis pipeline.\n",
    "    \n",
    "    Args:\n",
    "        base_path: Base directory path\n",
    "        gene_path: Path to gene annotations (default: base_path/annotations/human/Final_gene_annotations_human.tsv)\n",
    "        polr2a_path: Path to POLR2A results (default: base_path/runs/perturbation_runs/gene_expression_only_chrom_K562_POLR2A_enhancer_centric)\n",
    "        rna_path: Path to RNA-seq results (default: base_path/runs/perturbation_runs/gene_expression_only_chrom_K562_RNA_enhancer_centric)\n",
    "        output_path: Path to save output (default: base_path/tables)\n",
    "        window_size: Size of the window around enhancer center\n",
    "        resolution: Resolution of the data in base pairs per bin\n",
    "        upstream_bins: Number of bins to include upstream of TSS (for polr2a_tss)\n",
    "        downstream_bins: Number of bins to include downstream of TSS (for polr2a_tss)\n",
    "        N_BINS: Number of bins to one side of the central bin\n",
    "        visualize_top_n: Number of top enhancers to visualize (0 to skip visualization)\n",
    "        score_methods: List of scoring methods to use\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing DataFrames with results for each analysis type\n",
    "    \"\"\"\n",
    "    # Set default paths if not provided\n",
    "    if gene_path is None:\n",
    "        gene_path = base_path / \"annotations\" / \"human\" / \"Final_gene_annotations_human.tsv\"\n",
    "    \n",
    "    if polr2a_path is None:\n",
    "        polr2a_path = base_path / \"runs\" / \"perturbation_runs\" / \"gene_expression_only_chrom_K562_POLR2A_enhancer_centric\"\n",
    "    \n",
    "    if rna_path is None:\n",
    "        rna_path = base_path / \"runs\" / \"perturbation_runs\" / \"gene_expression_only_chrom_K562_RNA_enhancer_centric\"\n",
    "    \n",
    "    if output_path is None:\n",
    "        output_path = base_path / \"tables\"\n",
    "    \n",
    "    output_path.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Load gene annotations\n",
    "    genes_df = load_annotations(gene_path)\n",
    "    print(f\"Loaded {len(genes_df)} gene annotations\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Process POLR2A data if path exists\n",
    "    if polr2a_path.exists():\n",
    "        print(\"\\nProcessing POLR2A data...\")\n",
    "        print(f\"Loading predictions from {polr2a_path}...\")\n",
    "        \n",
    "        ids, predictions_df, _ = _get_predictions(\n",
    "            results_path=polr2a_path,\n",
    "            N_BINS=N_BINS,\n",
    "            condition_list=[\"_ctrl\"],\n",
    "            is_training=False\n",
    "        )\n",
    "        \n",
    "        print(f\"Loaded predictions for {len(ids)} samples\")\n",
    "        \n",
    "        # Get unique enhancer IDs (excluding perturbed ones)\n",
    "        enhancer_ids = [id for id in ids if not id.endswith('_perturbed')]\n",
    "        print(f\"Found {len(enhancer_ids)} unique enhancers\")\n",
    "        \n",
    "        # Process enhancers\n",
    "        polr2a_results = batch_process_enhancers(\n",
    "            enhancer_ids=enhancer_ids,\n",
    "            predictions_df=predictions_df,\n",
    "            genes_df=genes_df,\n",
    "            integration_type='polr2a_tss',\n",
    "            output_path=output_path,\n",
    "            window_size=window_size,\n",
    "            resolution=resolution,\n",
    "            upstream_bins=upstream_bins,\n",
    "            downstream_bins=downstream_bins,\n",
    "            visualize_top_n=visualize_top_n,\n",
    "            score_methods=score_methods\n",
    "        )\n",
    "        \n",
    "        results['polr2a_tss'] = polr2a_results\n",
    "    \n",
    "    # Process RNA-seq data if path exists\n",
    "    if rna_path.exists():\n",
    "        print(\"\\nProcessing RNA-seq data...\")\n",
    "        print(f\"Loading predictions from {rna_path}...\")\n",
    "        \n",
    "        ids, predictions_df, _ = _get_predictions(\n",
    "            results_path=rna_path,\n",
    "            N_BINS=N_BINS,\n",
    "            condition_list=[\"_ctrl\"],\n",
    "            is_training=False\n",
    "        )\n",
    "        \n",
    "        print(f\"Loaded predictions for {len(ids)} samples\")\n",
    "        \n",
    "        # Get unique enhancer IDs (excluding perturbed ones)\n",
    "        enhancer_ids = [id for id in ids if not id.endswith('_perturbed')]\n",
    "        print(f\"Found {len(enhancer_ids)} unique enhancers\")\n",
    "        \n",
    "        # Process enhancers\n",
    "        rna_results = batch_process_enhancers(\n",
    "            enhancer_ids=enhancer_ids,\n",
    "            predictions_df=predictions_df,\n",
    "            genes_df=genes_df,\n",
    "            integration_type='rnaseq_gene_body',\n",
    "            output_path=output_path,\n",
    "            window_size=window_size,\n",
    "            resolution=resolution,\n",
    "            visualize_top_n=visualize_top_n,\n",
    "            score_methods=score_methods\n",
    "        )\n",
    "        \n",
    "        results['rnaseq_gene_body'] = rna_results\n",
    "    \n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the analysis pipeline.\"\"\"\n",
    "    # Define base path\n",
    "    base_path = Path(\"../\")\n",
    "    \n",
    "    # Run analysis\n",
    "    print(\"Starting enhancer effect analysis pipeline...\")\n",
    "    results = run_analysis(base_path)\n",
    "    \n",
    "    print(\"\\nAnalysis complete!\")\n",
    "    \n",
    "    # If both POLR2A and RNA-seq results are available, run comparison\n",
    "    if 'polr2a_tss' in results and 'rnaseq_gene_body' in results:\n",
    "        print(\"\\nBoth POLR2A and RNA-seq results available. Running comparison...\")\n",
    "        \n",
    "        # Create comparison output directory\n",
    "        comparison_path = base_path / \"results_CRISPR\" / \"Integration_comparison\"\n",
    "        comparison_path.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "        # Save individual result files for reference\n",
    "        polr2a_file = comparison_path / \"polr2a_results_with_targets.csv\"\n",
    "        rna_file = comparison_path / \"rna_results_with_targets.csv\"\n",
    "        \n",
    "        results['polr2a_tss'].to_csv(polr2a_file, index=False)\n",
    "        results['rnaseq_gene_body'].to_csv(rna_file, index=False)\n",
    "        \n",
    "        print(f\"Saved individual result files to {comparison_path}\")\n",
    "        \n",
    "        # Call comparison function (to be implemented in comparison module)\n",
    "        print(\"To compare results, use the comparison module with the generated CSV files.\")\n",
    "        \n",
    "        return polr2a_file, rna_file\n",
    "    \n",
    "    return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define base path\n",
    "    base_path = Path(\"../\")\n",
    "    \n",
    "    # Define paths for analysis\n",
    "    gene_path = base_path / \"annotations\" / \"human\" / \"Final_gene_annotations_human.tsv\"\n",
    "    polr2a_path = base_path / \"runs\" / \"perturbation_runs\" / \"gene_expression_only_chrom_K562_POLR2A_enhancer_centric\"\n",
    "    rna_path = base_path / \"runs\" / \"perturbation_runs\" / \"gene_expression_only_chrom_K562_RNA_enhancer_centric\"\n",
    "    output_path = base_path / \"tables\"\n",
    "    \n",
    "    # Define scoring methods\n",
    "    score_methods = ['abs_area_difference', 'z_score', 'ratio_to_max']\n",
    "    \n",
    "    # Run both analyses with custom parameters\n",
    "    results = run_analysis(\n",
    "        base_path=base_path,\n",
    "        gene_path=gene_path,\n",
    "        polr2a_path=polr2a_path,\n",
    "        rna_path=rna_path,\n",
    "        output_path=output_path,\n",
    "        window_size=200500,\n",
    "        resolution=1000,\n",
    "        upstream_bins=2,\n",
    "        downstream_bins=3,\n",
    "        N_BINS=200,\n",
    "        visualize_top_n=10,\n",
    "        score_methods=score_methods\n",
    "    )\n",
    "    \n",
    "    # Check if comparison can be run\n",
    "    if 'polr2a_tss' in results and 'rnaseq_gene_body' in results:\n",
    "        print(\"\\nBoth analyses completed successfully. To run comparison:\")\n",
    "        \n",
    "        # Define comparison path and CRISPR benchmark file\n",
    "        crispr_file = base_path / \"annotations/human/lifted_hg38_to_hg19/EPCrisprBenchmark_ensemble_data_GRCh38_hg19.txt\"\n",
    "        comparison_path = base_path / \"results_CRISPR\" / \"Integration_comparison\"\n",
    "        \n",
    "        print(f\"CRISPR benchmark file: {crispr_file}\")\n",
    "        print(f\"Comparison output path: {comparison_path}\")\n",
    "        print(f\"Use the comparison module with the generated result files.\")\n",
    "    else:\n",
    "        print(\"\\nAnalysis completed. Not all result types are available for comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize Enhancer-Gene pairing results:**\n",
    "\n",
    "- Precision- Recall curves.\n",
    "- ROC curves.\n",
    "- Confusion matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc, average_precision_score\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import matplotlib.gridspec as gridspec\n",
    "try:\n",
    "    from matplotlib_venn import venn2\n",
    "except ImportError:\n",
    "    print(\"Warning: matplotlib_venn not installed. Venn diagrams will be skipped.\")\n",
    "\n",
    "# Set plotting style\n",
    "plt.rcParams['font.family'] = 'Nimbus Roman'\n",
    "np.random.seed(42)\n",
    "\n",
    "def calculate_binary_metrics(merged_crispr_df, output_path):\n",
    "    \"\"\"\n",
    "    Calculate and save comprehensive classification metrics for binary predictions.\n",
    "    \"\"\"\n",
    "    if 'Regulated' not in merged_crispr_df.columns:\n",
    "        print(\"Warning: 'Regulated' column not found in merged data.\")\n",
    "        return None\n",
    "    \n",
    "    from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, accuracy_score\n",
    "    \n",
    "    # Define prediction methods to evaluate\n",
    "    methods = {\n",
    "        'polr2a_effect': {\n",
    "            'name': 'POLR2A Effect-based Primary Target',\n",
    "            'predictions': merged_crispr_df['is_primary_target_polr2a'],\n",
    "            'color': '#1f77b4'  # blue\n",
    "        },\n",
    "        'rna_effect': {\n",
    "            'name': 'RNA-seq Effect-based Primary Target',\n",
    "            'predictions': merged_crispr_df['is_primary_target_rna'],\n",
    "            'color': '#2ca02c'  # green\n",
    "        },\n",
    "        'proximity_based': {\n",
    "            'name': 'Proximity-based Target',\n",
    "            'predictions': merged_crispr_df['is_closest_polr2a'],\n",
    "            'color': '#d62728'  # red\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # True labels\n",
    "    y_true = merged_crispr_df['Regulated']\n",
    "    \n",
    "    # Calculate metrics for each method\n",
    "    results = {}\n",
    "    for method_id, method_info in methods.items():\n",
    "        y_pred = method_info['predictions']\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred)\n",
    "        recall = recall_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        \n",
    "        # Store results\n",
    "        results[method_id] = {\n",
    "            'name': method_info['name'],\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'report': classification_report(y_true, y_pred, output_dict=True)\n",
    "        }\n",
    "    \n",
    "    # Create metrics table\n",
    "    metrics_table = pd.DataFrame({\n",
    "        'Method': [results[m]['name'] for m in methods],\n",
    "        'Accuracy': [results[m]['accuracy'] for m in methods],\n",
    "        'Precision': [results[m]['precision'] for m in methods],\n",
    "        'Recall': [results[m]['recall'] for m in methods],\n",
    "        'F1 Score': [results[m]['f1'] for m in methods]\n",
    "    })\n",
    "    \n",
    "    # Save metrics to CSV\n",
    "    metrics_table.to_csv(output_path / 'binary_classification_metrics.csv', index=False)\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot metrics as bar chart\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.25\n",
    "    \n",
    "    for i, method_id in enumerate(methods.keys()):\n",
    "        method_metrics = [results[method_id]['accuracy'], \n",
    "                         results[method_id]['precision'], \n",
    "                         results[method_id]['recall'], \n",
    "                         results[method_id]['f1']]\n",
    "        \n",
    "        plt.bar(x + width*(i-1), method_metrics, width, \n",
    "               label=results[method_id]['name'], \n",
    "               color=methods[method_id]['color'])\n",
    "    \n",
    "    plt.xlabel('Metric')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Classification Metrics Comparison')\n",
    "    plt.xticks(x, metrics)\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path / 'binary_classification_metrics.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Binary classification metrics:\")\n",
    "    print(metrics_table)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def calculate_threshold_metrics(merged_crispr_df, output_path):\n",
    "    \"\"\"\n",
    "    Calculate optimal thresholds and metrics for continuous score methods.\n",
    "    \"\"\"\n",
    "    if 'Regulated' not in merged_crispr_df.columns:\n",
    "        print(\"Warning: 'Regulated' column not found in merged data.\")\n",
    "        return None\n",
    "    \n",
    "    from sklearn.metrics import precision_recall_curve, f1_score, accuracy_score\n",
    "    \n",
    "    # Define methods to evaluate\n",
    "    methods = {\n",
    "        'polr2a_abs': {\n",
    "            'name': 'POLR2A Abs Area Difference',\n",
    "            'scores': merged_crispr_df['abs_area_difference_polr2a'].values,\n",
    "            'color': '#1f77b4'  # blue\n",
    "        },\n",
    "        'polr2a_zscore': {\n",
    "            'name': 'POLR2A Z-Score',\n",
    "            'scores': merged_crispr_df['z_score_polr2a'].values,\n",
    "            'color': '#ff7f0e'  # orange\n",
    "        },\n",
    "        'polr2a_ratio': {\n",
    "            'name': 'POLR2A Ratio to Max',\n",
    "            'scores': merged_crispr_df['ratio_to_max_polr2a'].values,\n",
    "            'color': '#9467bd'  # purple\n",
    "        },\n",
    "        'rna_abs': {\n",
    "            'name': 'RNA-seq Abs Area Difference',\n",
    "            'scores': merged_crispr_df['abs_area_difference_rna'].values,\n",
    "            'color': '#2ca02c'  # green\n",
    "        },\n",
    "        'rna_zscore': {\n",
    "            'name': 'RNA-seq Z-Score',\n",
    "            'scores': merged_crispr_df['z_score_rna'].values,\n",
    "            'color': '#d62728'  # red\n",
    "        },\n",
    "        'rna_ratio': {\n",
    "            'name': 'RNA-seq Ratio to Max',\n",
    "            'scores': merged_crispr_df['ratio_to_max_rna'].values,\n",
    "            'color': '#8c564b'  # brown\n",
    "        },\n",
    "        'proximity': {\n",
    "            'name': 'Proximity-based',\n",
    "            'scores': -merged_crispr_df['distance_to_enhancer_polr2a'].values,\n",
    "            'color': '#e377c2'  # pink\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # True labels\n",
    "    y_true = merged_crispr_df['Regulated'].astype(int).values\n",
    "    \n",
    "    # Calculate optimal thresholds and metrics\n",
    "    results = {}\n",
    "    metrics_rows = []\n",
    "    \n",
    "    for method_id, method_info in methods.items():\n",
    "        scores = method_info['scores']\n",
    "        \n",
    "        # Calculate precision-recall curve and find optimal threshold for F1\n",
    "        precision, recall, thresholds = precision_recall_curve(y_true, scores)\n",
    "        \n",
    "        # Calculate F1 score for each threshold\n",
    "        f1_scores = []\n",
    "        for threshold in thresholds:\n",
    "            y_pred = (scores >= threshold).astype(int)\n",
    "            f1 = f1_score(y_true, y_pred)\n",
    "            f1_scores.append(f1)\n",
    "        \n",
    "        # Find optimal threshold for F1 score\n",
    "        if len(f1_scores) > 0:\n",
    "            best_idx = np.argmax(f1_scores)\n",
    "            best_threshold = thresholds[best_idx]\n",
    "            best_f1 = f1_scores[best_idx]\n",
    "            \n",
    "            # Apply optimal threshold\n",
    "            y_pred = (scores >= best_threshold).astype(int)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_true, y_pred)\n",
    "            precision_at_threshold = precision[best_idx]\n",
    "            recall_at_threshold = recall[best_idx]\n",
    "            \n",
    "            # Store results\n",
    "            results[method_id] = {\n",
    "                'name': method_info['name'],\n",
    "                'optimal_threshold': best_threshold,\n",
    "                'accuracy': accuracy,\n",
    "                'precision': precision_at_threshold,\n",
    "                'recall': recall_at_threshold,\n",
    "                'f1': best_f1\n",
    "            }\n",
    "            \n",
    "            # Add to metrics table\n",
    "            metrics_rows.append({\n",
    "                'Method': method_info['name'],\n",
    "                'Optimal Threshold': best_threshold,\n",
    "                'Accuracy': accuracy,\n",
    "                'Precision': precision_at_threshold,\n",
    "                'Recall': recall_at_threshold,\n",
    "                'F1 Score': best_f1\n",
    "            })\n",
    "    \n",
    "    # Create metrics table\n",
    "    metrics_table = pd.DataFrame(metrics_rows)\n",
    "    \n",
    "    # Save metrics to CSV\n",
    "    metrics_table.to_csv(output_path / 'continuous_metrics_at_optimal_threshold.csv', index=False)\n",
    "    \n",
    "    # Plot metrics\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    # Plot metrics as bar chart\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.1\n",
    "    method_ids = list(methods.keys())\n",
    "    \n",
    "    for i, method_id in enumerate(method_ids):\n",
    "        if method_id in results:\n",
    "            method_result = results[method_id]\n",
    "            method_metrics = [method_result['accuracy'], \n",
    "                             method_result['precision'], \n",
    "                             method_result['recall'], \n",
    "                             method_result['f1']]\n",
    "            \n",
    "            plt.bar(x + width*(i-len(method_ids)/2), method_metrics, width, \n",
    "                   label=method_result['name'], \n",
    "                   color=methods[method_id]['color'])\n",
    "    \n",
    "    plt.xlabel('Metric')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Classification Metrics at Optimal Thresholds')\n",
    "    plt.xticks(x, metrics)\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.legend(loc='best', fontsize='small')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path / 'continuous_classification_metrics.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Continuous score metrics at optimal thresholds:\")\n",
    "    print(metrics_table)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def load_crispr_data(crispr_path):\n",
    "    \"\"\"\n",
    "    Load and preprocess CRISPR benchmark data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        crispr_df = pd.read_csv(crispr_path, sep='\\t', decimal=',')\n",
    "        \n",
    "        float_cols = ['EffectSize', 'pValueAdjusted', 'PowerAtEffectSize10', 'PowerAtEffectSize15', \n",
    "                     'PowerAtEffectSize20', 'PowerAtEffectSize25', 'PowerAtEffectSize50']\n",
    "        \n",
    "        for col in float_cols:\n",
    "            if col in crispr_df.columns:\n",
    "                crispr_df[col] = crispr_df[col].apply(lambda x: \n",
    "                                                   float(str(x).replace(',', '.')) \n",
    "                                                   if isinstance(x, str) else x)\n",
    "        \n",
    "        if 'Regulated' in crispr_df.columns:\n",
    "            crispr_df['Regulated'] = crispr_df['Regulated'].apply(\n",
    "                lambda x: x == True or x == 'TRUE' if isinstance(x, (str, bool)) else bool(x)\n",
    "            )\n",
    "        \n",
    "        crispr_df['enhancer_id'] = crispr_df.apply(\n",
    "            lambda row: f\"{row['chrom(hg19)']}_{row['chromStart(hg19)']}_{row['chromEnd(hg19)']}\",\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        if 'measuredGeneSymbol' in crispr_df.columns:\n",
    "            crispr_df['gene_name'] = crispr_df['measuredGeneSymbol']\n",
    "        \n",
    "        print(f\"Loaded CRISPR data: {len(crispr_df)} enhancer-gene pairs\")\n",
    "        return crispr_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CRISPR data: {e}\")\n",
    "        return None\n",
    "\n",
    "def plot_unified_curves(merged_crispr_df, output_path):\n",
    "    \"\"\"Create unified ROC and PR curves with shared legend.\"\"\"\n",
    "    if 'Regulated' not in merged_crispr_df.columns:\n",
    "        print(\"Warning: 'Regulated' column not found in merged data.\")\n",
    "        return None\n",
    "    \n",
    "    # Convert boolean Regulated to numeric (0/1)\n",
    "    y_true = merged_crispr_df['Regulated'].astype(int).values\n",
    "    \n",
    "    # For proximity, use negative distance as the score\n",
    "    merged_crispr_df['proximity_score'] = -merged_crispr_df['distance_to_enhancer_polr2a']\n",
    "    \n",
    "    # Define methods for comparison\n",
    "    methods = {\n",
    "        'polr2a_abs': {\n",
    "            'name': 'POLR2A Abs Area Difference',\n",
    "            'scores': merged_crispr_df['abs_area_difference_polr2a'].values,\n",
    "            'color': '#1f77b4'  # blue\n",
    "        },\n",
    "        #'polr2a_zscore': {\n",
    "        #    'name': 'POLR2A Z-Score',\n",
    "        #    'scores': merged_crispr_df['z_score_polr2a'].values,\n",
    "        #    'color': '#ff7f0e'  # orange\n",
    "        #},\n",
    "        'polr2a_ratio': {\n",
    "            'name': 'POLR2A Ratio to Max',\n",
    "            'scores': merged_crispr_df['ratio_to_max_polr2a'].values,\n",
    "            'color': '#9467bd'  # purple\n",
    "        },\n",
    "        'rna_abs': {\n",
    "            'name': 'RNA-seq Abs Area Difference',\n",
    "            'scores': merged_crispr_df['abs_area_difference_rna'].values,\n",
    "            'color': '#2ca02c'  # green\n",
    "        },\n",
    "        #'rna_zscore': {\n",
    "        #    'name': 'RNA-seq Z-Score',\n",
    "        #    'scores': merged_crispr_df['z_score_rna'].values,\n",
    "        #    'color': '#d62728'  # red\n",
    "        #},\n",
    "        'rna_ratio': {\n",
    "            'name': 'RNA-seq Ratio to Max',\n",
    "            'scores': merged_crispr_df['ratio_to_max_rna'].values,\n",
    "            'color': '#ff7f0e'  # orange '#8c564b'  # brown\n",
    "        },\n",
    "        'proximity': {\n",
    "            'name': 'Proximity-based',\n",
    "            'scores': merged_crispr_df['proximity_score'].values,\n",
    "            'color': '#d62728'  # red #'#e377c2'  # pink\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create figure with GridSpec for custom layout\n",
    "    fig = plt.figure(figsize=(12, 4))\n",
    "    gs = gridspec.GridSpec(1, 3, width_ratios=[1, 1, 1])\n",
    "    \n",
    "    # Store lines for legend\n",
    "    legend_lines = []\n",
    "    legend_labels = []\n",
    "\n",
    "    # Create subplots\n",
    "    ax_roc = fig.add_subplot(gs[0])\n",
    "    ax_pr = fig.add_subplot(gs[1], sharey=ax_roc)  # Share y-axis\n",
    "    ax_legend = fig.add_subplot(gs[2])\n",
    "\n",
    "    # Plot ROC curves\n",
    "    for method_id, method_info in methods.items():\n",
    "        fpr, tpr, _ = roc_curve(y_true, method_info['scores'])\n",
    "        roc_auc_value = auc(fpr, tpr)\n",
    "        \n",
    "        line, = ax_roc.plot(fpr, tpr, color=method_info['color'])\n",
    "        legend_lines.append(line)\n",
    "        legend_labels.append(f\"{method_info['name']} (AUROC = {roc_auc_value:.3f}\")\n",
    "\n",
    "    # ROC plot settings\n",
    "    ax_roc.plot([0, 1], [0, 1], 'k--', alpha=0.3)\n",
    "    ax_roc.set_xlabel('False Positive Rate')\n",
    "    ax_roc.set_ylabel('True Positive Rate')\n",
    "    ax_roc.set_xlim([0.0, 1.0])\n",
    "    ax_roc.set_ylim([0.0, 1.0])\n",
    "    ax_roc.set_title('ROC Curves')\n",
    "    ax_roc.set_aspect(1.0)\n",
    "\n",
    "    # Plot PR curves\n",
    "    for method_id, method_info in methods.items():\n",
    "        precision, recall, _ = precision_recall_curve(y_true, method_info['scores'])\n",
    "        average_precision = average_precision_score(y_true, method_info['scores'])\n",
    "        \n",
    "        ax_pr.plot(recall, precision, color=method_info['color'])\n",
    "        # Update labels with AUPRC\n",
    "        idx = list(methods.keys()).index(method_id)\n",
    "        legend_labels[idx] += f\", AUPRC = {average_precision:.3f})\"\n",
    "\n",
    "    # PR plot settings\n",
    "    no_skill = sum(y_true) / len(y_true)\n",
    "    print(no_skill)\n",
    "    ax_pr.plot([0, 1], [no_skill, no_skill], 'k--', alpha=0.3)\n",
    "    ax_pr.set_xlabel('Recall')\n",
    "    ax_pr.set_ylabel('Precision')  # No y-label since shared\n",
    "    ax_pr.set_xlim([0.0, 1.0])\n",
    "    ax_pr.set_ylim([0.0, 1.0])\n",
    "    ax_pr.set_title('Precision-Recall Curves')\n",
    "    ax_pr.set_aspect(1.0)\n",
    "\n",
    "    # Make legend subplot empty but add the legend\n",
    "    ax_legend.axis('off')\n",
    "    ax_legend.legend(handles=legend_lines, labels=legend_labels, loc='center', fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path / 'unified_curves.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "    return fig\n",
    "\n",
    "def plot_performance_summary(merged_crispr_df, output_path):\n",
    "    \"\"\"Create a summary plot comparing all methods' performance.\"\"\"\n",
    "    if 'Regulated' not in merged_crispr_df.columns:\n",
    "        print(\"Warning: 'Regulated' column not found in merged data.\")\n",
    "        return None\n",
    "\n",
    "    # Convert boolean Regulated to numeric (0/1)\n",
    "    y_true = merged_crispr_df['Regulated'].astype(int).values\n",
    "    \n",
    "    # For proximity, use negative distance as the score (closer = higher score)\n",
    "    merged_crispr_df['proximity_score'] = -merged_crispr_df['distance_to_enhancer_polr2a']\n",
    "    \n",
    "    # Define all methods to compare - same as in plot_roc_curves\n",
    "    methods = {\n",
    "        'polr2a_abs': {\n",
    "            'name': 'POLR2A Abs Area Difference',\n",
    "            'scores': merged_crispr_df['abs_area_difference_polr2a'].values,\n",
    "            'color': '#1f77b4'  # blue\n",
    "        },\n",
    "        'polr2a_zscore': {\n",
    "            'name': 'POLR2A Z-Score',\n",
    "            'scores': merged_crispr_df['z_score_polr2a'].values,\n",
    "            'color': '#ff7f0e'  # orange\n",
    "        },\n",
    "        'polr2a_ratio': {\n",
    "            'name': 'POLR2A Ratio to Max',\n",
    "            'scores': merged_crispr_df['ratio_to_max_polr2a'].values,\n",
    "            'color': '#9467bd'  # purple\n",
    "        },\n",
    "        'rna_abs': {\n",
    "            'name': 'RNA-seq Abs Area Difference',\n",
    "            'scores': merged_crispr_df['abs_area_difference_rna'].values,\n",
    "            'color': '#2ca02c'  # green\n",
    "        },\n",
    "        'rna_zscore': {\n",
    "            'name': 'RNA-seq Z-Score',\n",
    "            'scores': merged_crispr_df['z_score_rna'].values,\n",
    "            'color': '#d62728'  # red\n",
    "        },\n",
    "        'rna_ratio': {\n",
    "            'name': 'RNA-seq Ratio to Max',\n",
    "            'scores': merged_crispr_df['ratio_to_max_rna'].values,\n",
    "            'color': '#8c564b'  # brown\n",
    "        },\n",
    "        'proximity': {\n",
    "            'name': 'Proximity-based',\n",
    "            'scores': merged_crispr_df['proximity_score'].values,\n",
    "            'color': '#e377c2'  # pink\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Calculate performance metrics for each method\n",
    "    results = []\n",
    "    \n",
    "    for method_id, method_info in methods.items():\n",
    "        # Calculate AUC-ROC\n",
    "        fpr, tpr, _ = roc_curve(y_true, method_info['scores'])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        # Calculate Average Precision\n",
    "        ap = average_precision_score(y_true, method_info['scores'])\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'Method': method_info['name'],\n",
    "            'AUC-ROC': roc_auc,\n",
    "            'AP': ap,\n",
    "            'Color': method_info['color']\n",
    "        })\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Create summary plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot as grouped bar chart\n",
    "    metrics = ['AUC-ROC', 'AP']\n",
    "    x = np.arange(len(summary_df))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, summary_df['AUC-ROC'], width, label='AUC-ROC', color='navy', alpha=0.7)\n",
    "    plt.bar(x + width/2, summary_df['AP'], width, label='Average Precision', color='darkred', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Method')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Performance Comparison of Prediction Methods')\n",
    "    plt.xticks(x, summary_df['Method'], rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path / 'performance_summary.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    # Save summary to CSV\n",
    "    summary_df[['Method', 'AUC-ROC', 'AP']].to_csv(output_path / 'performance_summary.csv', index=False)\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "def plot_primary_target_comparison(merged_crispr_df, output_path):\n",
    "    \"\"\"Create confusion matrix comparing methods and CRISPR data with precision/recall metrics.\"\"\"\n",
    "    if 'Regulated' not in merged_crispr_df.columns:\n",
    "        print(\"Warning: 'Regulated' column not found in merged data.\")\n",
    "        return None\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    output_path.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(9, 4.3))\n",
    "    \n",
    "    # Calculate metrics for effect-based prediction\n",
    "    cm1 = confusion_matrix(merged_crispr_df['Regulated'], merged_crispr_df['is_primary_target_polr2a'])\n",
    "    tn1, fp1, fn1, tp1 = cm1.ravel()\n",
    "    acc1 = (tp1 + tn1) / cm1.sum()\n",
    "    precision1 = tp1 / (tp1 + fp1) if (tp1 + fp1) > 0 else 0\n",
    "    recall1 = tp1 / (tp1 + fn1) if (tp1 + fn1) > 0 else 0\n",
    "    \n",
    "    # Plot first confusion matrix\n",
    "    sns.heatmap(cm1, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Not Primary', 'Primary'],\n",
    "                yticklabels=['Not Regulated', 'Regulated'],\n",
    "                ax=axes[0])\n",
    "    \n",
    "    axes[0].set_xlabel('Effect-based Prediction')\n",
    "    axes[0].set_ylabel('CRISPR Regulated')\n",
    "    axes[0].set_title('Effect-based Primary Target vs CRISPR')\n",
    "    \n",
    "    # Calculate metrics for proximity-based prediction\n",
    "    cm2 = confusion_matrix(merged_crispr_df['Regulated'], merged_crispr_df['is_closest_polr2a'])\n",
    "    tn2, fp2, fn2, tp2 = cm2.ravel()\n",
    "    acc2 = (tp2 + tn2) / cm2.sum()\n",
    "    precision2 = tp2 / (tp2 + fp2) if (tp2 + fp2) > 0 else 0\n",
    "    recall2 = tp2 / (tp2 + fn2) if (tp2 + fn2) > 0 else 0\n",
    "    f1_score2 = 2 * precision2 * recall2 / (precision2 + recall2) if (precision2 + recall2) > 0 else 0\n",
    "    \n",
    "    # Plot second confusion matrix\n",
    "    sns.heatmap(cm2, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Not Closest', 'Closest'],\n",
    "                yticklabels=['Not Regulated', 'Regulated'],\n",
    "                ax=axes[1])\n",
    "    \n",
    "    axes[1].set_xlabel('Proximity-based Prediction')\n",
    "    axes[1].set_ylabel('CRISPR Regulated')\n",
    "    axes[1].set_title('Proximity-based Target vs CRISPR')\n",
    "    \n",
    "    # Add metrics below each matrix\n",
    "    y_pos = 0.02\n",
    "    plt.figtext(0.25, y_pos+0.06, f'Accuracy: {acc1:.3f}, Precision: {precision1:.3f}, Recall: {recall1:.3f}', ha='center')\n",
    "    plt.figtext(0.75, y_pos+0.06, f'Accuracy: {acc2:.3f}, Precision: {precision2:.3f}, Recall: {recall2:.3f}', ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.2)  # Make space for metrics\n",
    "    plt.savefig(output_path / 'crispr_prediction_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return {\n",
    "        'effect_cm': cm1,\n",
    "        'proximity_cm': cm2,\n",
    "        'effect_metrics': {\n",
    "            'accuracy': acc1,\n",
    "            'precision': precision1,\n",
    "            'recall': recall1,\n",
    "        },\n",
    "        'proximity_metrics': {\n",
    "            'accuracy': acc2,\n",
    "            'precision': precision2,\n",
    "            'recall': recall2,\n",
    "        }\n",
    "    }\n",
    "\n",
    "def plot_correlation_scatter(merged_df, output_path):\n",
    "    \"\"\"\n",
    "    Create scatter plot comparing POLR2A and RNA-seq scores with primary target highlighting.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Define colors based on primary target status\n",
    "    colors = np.where(merged_df['is_primary_target_polr2a'], '#ff7f0e', '#1f77b4')\n",
    "    \n",
    "    # Create legend elements\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='#ff7f0e', label='Primary Target (POLR2A)'),\n",
    "        Patch(facecolor='#1f77b4', label='Secondary Target')\n",
    "    ]\n",
    "    \n",
    "    # Create scatter plot\n",
    "    plt.scatter(\n",
    "        merged_df['abs_area_difference_polr2a'],\n",
    "        merged_df['abs_area_difference_rna'],\n",
    "        c=colors,\n",
    "        alpha=0.7,\n",
    "        s=50\n",
    "    )\n",
    "    \n",
    "    # Add regression line\n",
    "    sns.regplot(\n",
    "        x=merged_df['abs_area_difference_polr2a'],\n",
    "        y=merged_df['abs_area_difference_rna'],\n",
    "        scatter=False,\n",
    "        color='black',\n",
    "        line_kws={'linestyle': '--', 'alpha': 0.7}\n",
    "    )\n",
    "    \n",
    "    # Calculate correlation statistics\n",
    "    pearson_r, pearson_p = pearsonr(\n",
    "        merged_df['abs_area_difference_polr2a'],\n",
    "        merged_df['abs_area_difference_rna']\n",
    "    )\n",
    "    \n",
    "    spearman_r, spearman_p = spearmanr(\n",
    "        merged_df['abs_area_difference_polr2a'],\n",
    "        merged_df['abs_area_difference_rna']\n",
    "    )\n",
    "    \n",
    "    plt.xlabel('POLR2A Integration Score')\n",
    "    plt.ylabel('RNA-seq Integration Score')\n",
    "    plt.title(f'Score Correlation\\nPearson r = {pearson_r:.3f}, Spearman ρ = {spearman_r:.3f}')\n",
    "    \n",
    "    plt.legend(handles=legend_elements, loc='upper left')\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path / 'score_correlation_with_targets.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    return {\n",
    "        'pearson_r': pearson_r,\n",
    "        'pearson_p': pearson_p,\n",
    "        'spearman_r': spearman_r,\n",
    "        'spearman_p': spearman_p\n",
    "    }\n",
    "\n",
    "\n",
    "def compare_results(polr2a_path, rna_path, crispr_path, output_path, score_methods=None):\n",
    "    \"\"\"\n",
    "    Run comparison analysis between POLR2A and RNA-seq predictions with multiple scoring methods.\n",
    "    \n",
    "    Args:\n",
    "        polr2a_path: Path to POLR2A results CSV with primary target flags\n",
    "        rna_path: Path to RNA-seq results CSV with primary target flags\n",
    "        crispr_path: Path to CRISPR benchmark data\n",
    "        output_path: Path to save outputs\n",
    "        score_methods: List of scoring methods to compare\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with analysis results\n",
    "    \"\"\"\n",
    "    if score_methods is None:\n",
    "        score_methods = ['abs_area_difference', 'z_score', 'ratio_to_max']\n",
    "        \n",
    "    # Create output directory\n",
    "    output_path = Path(output_path)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"\\n=== Running Enhanced Enhancer-Gene Interaction Analysis ===\")\n",
    "    \n",
    "    # Load POLR2A and RNA-seq results\n",
    "    print(\"Loading prediction results...\")\n",
    "    polr2a_df = pd.read_csv(polr2a_path)\n",
    "    rna_df = pd.read_csv(rna_path)\n",
    "    \n",
    "    # Merge datasets on enhancer-gene pairs\n",
    "    print(\"Merging POLR2A and RNA-seq results...\")\n",
    "    merged_df = pd.merge(\n",
    "        polr2a_df, \n",
    "        rna_df, \n",
    "        on=['enhancer_id', 'gene_id', 'gene_name'],\n",
    "        suffixes=('_polr2a', '_rna')\n",
    "    )\n",
    "    \n",
    "    print(f\"Found {len(merged_df)} common enhancer-gene pairs\")\n",
    "    merged_df.to_csv(output_path / 'merged_results_with_targets.csv', index=False)\n",
    "    \n",
    "    # Load CRISPR benchmark data if available\n",
    "    crispr_df = None\n",
    "    if crispr_path and Path(crispr_path).exists():\n",
    "        print(\"Loading CRISPR benchmark data...\")\n",
    "        crispr_df = load_crispr_data(crispr_path)\n",
    "        \n",
    "        if crispr_df is not None:\n",
    "            # Merge with CRISPR ground truth\n",
    "            print(\"Merging with CRISPR benchmark data...\")\n",
    "            merge_keys = ['enhancer_id', 'gene_name']\n",
    "            \n",
    "            # Select relevant columns from CRISPR data\n",
    "            crispr_cols = merge_keys + ['Regulated', 'EffectSize', 'Significant']\n",
    "            available_cols = [col for col in crispr_cols if col in crispr_df.columns]\n",
    "            \n",
    "            # Perform merge\n",
    "            merged_crispr_df = pd.merge(\n",
    "                merged_df,\n",
    "                crispr_df[available_cols],\n",
    "                on=merge_keys,\n",
    "                how='inner'\n",
    "            )\n",
    "            \n",
    "            print(f\"Merged results: {len(merged_crispr_df)} enhancer-gene pairs with CRISPR data\")\n",
    "            \n",
    "            # Report on regulated status\n",
    "            if 'Regulated' in merged_crispr_df.columns:\n",
    "                regulated_count = merged_crispr_df['Regulated'].sum()\n",
    "                print(f\"Regulated genes: {regulated_count} ({regulated_count/len(merged_crispr_df)*100:.1f}%)\")\n",
    "                \n",
    "            merged_crispr_df.to_csv(output_path / 'merged_with_crispr_targets.csv', index=False)\n",
    "        else:\n",
    "            merged_crispr_df = merged_df\n",
    "            print(\"Warning: Could not load CRISPR data. Proceeding with basic comparison.\")\n",
    "    else:\n",
    "        merged_crispr_df = merged_df\n",
    "        print(\"No CRISPR benchmark data provided. Proceeding with basic comparison.\")\n",
    "    \n",
    "    # Generate plots\n",
    "    print(\"Generating plots...\")\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt\n",
    "    fig.show()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax0 = fig.add_subplot(2,1,1)\n",
    "    ax0.hist(merged_crispr_df['baseline_area_polr2a'], bins=np.arange(merged_crispr_df['baseline_area_polr2a'].min(), merged_crispr_df['baseline_area_polr2a'].max(),1))\n",
    "    #ax0.set_yscale('log')\n",
    "    ax0.set_xlim(0,200)\n",
    "    ax1 = fig.add_subplot(2,1,2)\n",
    "    ax1.hist(merged_crispr_df['baseline_area_rna'], bins=np.arange(merged_crispr_df['baseline_area_rna'].min(), merged_crispr_df['baseline_area_rna'].max(),1))\n",
    "    ax1.set_xlim(0,200)\n",
    "    fig.show()\n",
    "\n",
    "    #threshold_polr2a = 10\n",
    "    #merged_crispr_df = merged_crispr_df[(merged_crispr_df['baseline_area_polr2a'] > threshold_polr2a)]\n",
    "    \n",
    "    # 1. Plot correlation between methods\n",
    "    print(\"- Generating correlation scatter plot...\")\n",
    "    corr_results = plot_correlation_scatter(merged_df, output_path)\n",
    "    \n",
    "    # 3. Generate CRISPR comparison if available\n",
    "    crispr_results = None\n",
    "    roc_results = None\n",
    "    pr_results = None\n",
    "    performance_summary = None\n",
    "    \n",
    "    if 'Regulated' in merged_crispr_df.columns:\n",
    "        print(\"- Generating CRISPR comparison plots...\")\n",
    "        crispr_results = plot_primary_target_comparison(merged_crispr_df, output_path)\n",
    "        \n",
    "        # ... other plotting functions\n",
    "        print(\"- Generating unified ROC and PR curves...\")\n",
    "        unified_fig = plot_unified_curves(merged_crispr_df, output_path)\n",
    "        \n",
    "        print(\"- Generating performance summary...\")\n",
    "        performance_summary = plot_performance_summary(merged_crispr_df, output_path)\n",
    "        \n",
    "        print(\"- Calculating binary classification metrics...\")\n",
    "        binary_metrics = calculate_binary_metrics(merged_crispr_df, output_path)\n",
    "        \n",
    "        print(\"- Calculating continuous score metrics at optimal thresholds...\")\n",
    "        threshold_metrics = calculate_threshold_metrics(merged_crispr_df, output_path)\n",
    "    \n",
    "    # Compile results\n",
    "    results = {\n",
    "        'data': {\n",
    "            'merged': merged_df,\n",
    "            'with_crispr': merged_crispr_df if crispr_df is not None else None\n",
    "        },\n",
    "        'metrics': {\n",
    "            'correlation': corr_results,\n",
    "            'crispr_comparison': crispr_results,\n",
    "            'roc': roc_results,\n",
    "            'pr': pr_results,\n",
    "            'performance_summary': performance_summary,\n",
    "            'binary_metrics': binary_metrics,\n",
    "            'threshold_metrics': threshold_metrics\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== Analysis Complete ===\")\n",
    "    print(f\"All results and visualizations saved to: {output_path}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the analysis pipeline.\"\"\"\n",
    "    # Define base path\n",
    "    base_path = Path(\"../\")\n",
    "    \n",
    "    # Define paths for analysis and score methods\n",
    "    polr2a_file = base_path / \"tables\" / \"polr2a_tss_abs_area_difference_results_with_targets.csv\"\n",
    "    rna_file = base_path / \"tables\" / \"rnaseq_gene_body_abs_area_difference_results_with_targets.csv\"\n",
    "    crispr_file = base_path / \"annotations/human/lifted_hg38_to_hg19/EPCrisprBenchmark_ensemble_data_GRCh38_hg19.txt\"\n",
    "    comparison_path = base_path / \"results_CRISPR\" / \"Integration_comparison_targets\"\n",
    "    \n",
    "    score_methods = ['abs_area_difference', 'z_score', 'ratio_to_max']\n",
    "    \n",
    "    # Run comparison\n",
    "    results = compare_results(\n",
    "        polr2a_path=polr2a_file,\n",
    "        rna_path=rna_file,\n",
    "        crispr_path=crispr_file,\n",
    "        output_path=comparison_path,\n",
    "        score_methods=score_methods\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plotting ground truth Enhancer-Gene pairs**\n",
    "\n",
    "Here we will plot ground truth Enhancer-Gene pairs in K562 cells, obtained by CRISPR KO of enhancers and measuring the induced gene expression changes. This data was downloaded from the [Engreitz lab's github](https://github.com/EngreitzLab/CRISPR_comparison/tree/main/resources/crispr_data), referenced as a benchmarking dataset in [Gschwind et al.](https://doi.org/10.1101/2023.11.09.563812)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD = True\n",
    "DATA_DIR = Path(\"./\") \n",
    "FIGURE_DIR = Path(\"../figures/Figures_revisions/\")\n",
    "FILENAME = \"EPCrisprBenchmark_ensemble_data_GRCh38.tsv\"\n",
    "URL = \"https://github.com/EngreitzLab/CRISPR_comparison/raw/refs/heads/main/resources/crispr_data/EPCrisprBenchmark_ensemble_data_GRCh38.tsv.gz\"\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if DOWNLOAD:\n",
    "    # Download and process file\n",
    "    import urllib.request\n",
    "    import gzip\n",
    "    import shutil\n",
    "    \n",
    "    # Download the gzipped file\n",
    "    gz_path = DATA_DIR / f\"{FILENAME}.gz\"\n",
    "    urllib.request.urlretrieve(URL, gz_path)\n",
    "    \n",
    "    # Unzip the file\n",
    "    with gzip.open(gz_path, 'rb') as f_in:\n",
    "        with open(DATA_DIR / FILENAME, 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "    \n",
    "    # Remove the gzipped file\n",
    "    gz_path.unlink()\n",
    "\n",
    "# Read the data into DataFrame\n",
    "df_path = DATA_DIR / FILENAME\n",
    "df = pd.read_csv(df_path, sep='\\t')\n",
    "df = df[df['Regulated']==True] # Keep only true EP pairs, defined by Regulated=True\n",
    "\n",
    "# Calculate distances\n",
    "enh_pos = (df['chromStart'] + df['chromStart']) // 2 #Center of the regulatory element\n",
    "prom_pos = df['startTSS'] # Only one basepair difference between start and end TSS\n",
    "x = abs(enh_pos.values - prom_pos.values).astype(int)\n",
    "scale = 1000 #kbp\n",
    "median = np.median(x)\n",
    "n_EG_pairs = len(df)\n",
    "\n",
    "#Plot histogram:\n",
    "fig = plt.figure(figsize=(7,3))\n",
    "plt.hist(x/scale, bins=np.arange(0, 200,5), density=True, label = f\"# True EG pairs: {n_EG_pairs}\")\n",
    "plt.plot(median/scale*np.ones(10), np.linspace(0,0.04,10), color='red', ls='--', label=f'Median: {int(median)} bp')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Distance to TSS (kbp)')\n",
    "plt.ylim(0,0.04)\n",
    "plt.xlim(0,200)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURE_DIR / 'Ground_truth_EP_histogram.png', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get genomic region boundaries for fast UCSC query:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrom = 17\n",
    "start = 75082798\n",
    "SHIFT = 200050\n",
    "\n",
    "print(f\"chr{chrom}:{start-SHIFT}-{start+SHIFT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check which perturbations were included:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Load data\n",
    "df = pd.read_csv(\"../annotations/human/lifted_hg38_to_hg19/EPCrisprBenchmark_ensemble_data_GRCh38_hg19.txt\", sep=\"\\t\")\n",
    "included = pd.read_csv(\"../targets/K562/Enhancer_centered_targets_human.csv\", sep=\",\")\n",
    "\n",
    "# Function to parse IDs in the format chr_start_end or chr_start_end_perturbed\n",
    "def parse_id(id_str):\n",
    "    parts = id_str.split('_')\n",
    "    \n",
    "    # Check if it has the perturbed suffix\n",
    "    is_perturbed = parts[-1] == 'perturbed'\n",
    "    \n",
    "    # Extract chr, start, end\n",
    "    if is_perturbed and len(parts) >= 4:\n",
    "        chrom = parts[0]\n",
    "        try:\n",
    "            start = int(parts[1])\n",
    "            end = int(parts[2])\n",
    "            return {'chrom': chrom, 'start': start, 'end': end}\n",
    "        except ValueError:\n",
    "            return None\n",
    "    elif not is_perturbed and len(parts) >= 3:\n",
    "        chrom = parts[0]\n",
    "        try:\n",
    "            start = int(parts[1])\n",
    "            end = int(parts[2])\n",
    "            return {'chrom': chrom, 'start': start, 'end': end}\n",
    "        except ValueError:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Create a set of (chrom, start, end) tuples from included dataframe\n",
    "included_regions = set()\n",
    "for _, row in included.iterrows():\n",
    "    parsed = parse_id(row['ID'])\n",
    "    if parsed:\n",
    "        included_regions.add((parsed['chrom'], parsed['start'], parsed['end']))\n",
    "\n",
    "# Add \"included\" column to df based on whether the hg19 coordinates appear in included_regions\n",
    "df['included'] = df.apply(\n",
    "    lambda row: \"yes\" if (row['chrom(hg19)'], row['chromStart(hg19)'], row['chromEnd(hg19)']) in included_regions else \"no\", \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Save the updated dataframe\n",
    "df.to_csv(\"df_with_included_flag.csv\", index=False)\n",
    "\n",
    "# Print summary\n",
    "included_count = (df['included'] == \"yes\").sum()\n",
    "print(f\"Total regions in df: {len(df)}\")\n",
    "print(f\"Regions also in included dataframe: {included_count}\")\n",
    "print(f\"Percentage of df regions in included dataframe: {included_count/len(df)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the enhancers data (update the path to your enhancers file)\n",
    "enhancers = pd.read_csv(\"../annotations/CLASTER Supplementary Tables Revisited - ST3. Enhancer_Like_Signatures.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Load the targets data from paste.txt\n",
    "targets = pd.read_csv(\"../targets/Enhancer_centered_targets.csv\", sep=\",\")\n",
    "\n",
    "# Extract the target IDs from the first column\n",
    "target_ids = set(targets['ID'])\n",
    "\n",
    "# Add 'Included' column to enhancers dataframe\n",
    "enhancers['Included'] = enhancers['ID'].apply(lambda x: \"yes\" if x in target_ids else \"no\")\n",
    "\n",
    "# Save the result\n",
    "enhancers.to_csv(\"enhancers_with_included.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Print summary\n",
    "included_count = (enhancers['Included'] == \"yes\").sum()\n",
    "print(f\"Total enhancers: {len(enhancers)}\")\n",
    "print(f\"Enhancers included in targets file: {included_count}\")\n",
    "print(f\"Percentage of enhancers in targets: {included_count/len(enhancers)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Editing gene annotations table to add whether they were included or not in the analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "def process_annotation_file(species=\"mouse\"):\n",
    "    \"\"\"\n",
    "    Updates the gene annotations TSV file by adding columns that indicate:\n",
    "    1. If the gene is in training, test, or missing for targets\n",
    "    2. For each input folder type, whether the gene is in training, test, both, or missing\n",
    "    \n",
    "    Parameters:\n",
    "    species (str): Either \"mouse\" or \"human\" to determine which files to process\n",
    "    \"\"\"\n",
    "    if species.lower() == \"mouse\":\n",
    "        # Paths for mouse files\n",
    "        annotation_file = \"../annotations/Final_gene_annotations.tsv\"\n",
    "        output_file = \"../annotations/Final_gene_annotations_updated.tsv\"\n",
    "        \n",
    "        # Target files for mouse\n",
    "        target_paths = {\n",
    "            \"default\": {\n",
    "                \"training\": \"../targets/training_targets.csv\",\n",
    "                \"test\": \"../targets/test_targets.csv\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Input folders for mouse\n",
    "        input_types = [\n",
    "            \"landscape_arrays\",\n",
    "            \"microC\",\n",
    "            \"microC_rotated\",\n",
    "            \"Promoter-CHiC\"\n",
    "        ]\n",
    "        \n",
    "    elif species.lower() == \"human\":\n",
    "        # Paths for human files\n",
    "        annotation_file = \"../annotations/human/Final_gene_annotations_human.tsv\"\n",
    "        output_file = \"../annotations/human/Final_gene_annotations_human_updated.tsv\"\n",
    "        \n",
    "        # Target files for human\n",
    "        target_paths = {\n",
    "            \"K562\": {\n",
    "                \"training\": \"../targets/K562/training_targets.csv\",\n",
    "                \"test\": \"../targets/K562/test_targets.csv\"\n",
    "            },\n",
    "            \"K562_polii\": {\n",
    "                \"training\": \"../targets/K562/polii/training_polii_targets.csv\",\n",
    "                \"test\": \"../targets/K562/polii/test_polii_targets.csv\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Check for polii target files - FIX: Try multiple potential locations\n",
    "        polii_paths = {\n",
    "            \"training\": [\n",
    "                \"../targets/K562/polii/training_polii_targets.csv\",\n",
    "                \"../targets/K562/polii/training_targets.csv\"\n",
    "            ],\n",
    "            \"test\": [\n",
    "                \"../targets/K562/polii/test_polii_targets.csv\",\n",
    "                \"../targets/K562/polii/test_targets.csv\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Check all potential paths for polii files\n",
    "        for split, paths in polii_paths.items():\n",
    "            found = False\n",
    "            for path in paths:\n",
    "                if os.path.exists(path):\n",
    "                    target_paths[\"K562_polii\"][split] = path\n",
    "                    print(f\"Found polii {split} targets at: {path}\")\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                print(f\"WARNING: No polii {split} targets found. Tried paths: {paths}\")\n",
    "        \n",
    "        # Input folders for human\n",
    "        input_types = [\"K562\"]\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported species: {species}. Choose 'mouse' or 'human'.\")\n",
    "    \n",
    "    print(f\"\\n--- Processing {species.upper()} gene annotations ---\\n\")\n",
    "    \n",
    "    # Read the gene annotations file\n",
    "    try:\n",
    "        annotations = pd.read_csv(annotation_file, sep='\\t')\n",
    "        print(f\"Loaded {len(annotations)} gene annotations for {species}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Annotation file {annotation_file} not found.\")\n",
    "        return {\"species\": species, \"error\": f\"Annotation file not found: {annotation_file}\"}\n",
    "    \n",
    "    # Initialize dictionaries to store all target types\n",
    "    all_targets = {}\n",
    "    for target_type, paths in target_paths.items():\n",
    "        all_targets[target_type] = {\n",
    "            \"training\": set(),\n",
    "            \"test\": set()\n",
    "        }\n",
    "    \n",
    "    # Initialize dictionaries to store array file information by input type\n",
    "    array_data = {}\n",
    "    for input_type in input_types:\n",
    "        array_data[input_type] = {\n",
    "            \"training\": set(),\n",
    "            \"test\": set()\n",
    "        }\n",
    "    \n",
    "    # Extract IDs from all target files\n",
    "    missing_target_files = []\n",
    "    for target_type, paths in target_paths.items():\n",
    "        for split, file_path in paths.items():\n",
    "            if os.path.exists(file_path):\n",
    "                print(f\"Processing target file: {file_path}\")\n",
    "                with open(file_path, 'r') as f:\n",
    "                    reader = csv.reader(f)\n",
    "                    header = next(reader)  # Skip header\n",
    "                    for row in reader:\n",
    "                        if row and len(row) > 0:  # Make sure the row is not empty\n",
    "                            # For polii files, the ID might be in the second column if there's a dummy index\n",
    "                            if target_type == \"K562_polii\" and len(row) > 1:\n",
    "                                full_id = row[1] if row[1] else row[0]\n",
    "                            else:\n",
    "                                full_id = row[0]\n",
    "                            # Remove only the _forward or _rev suffix\n",
    "                            if \"_forward\" in full_id:\n",
    "                                base_id = full_id.replace(\"_forward\", \"\")\n",
    "                            elif \"_rev\" in full_id:\n",
    "                                base_id = full_id.replace(\"_rev\", \"\")\n",
    "                            else:\n",
    "                                base_id = full_id\n",
    "                            all_targets[target_type][split].add(base_id)\n",
    "                print(f\"Found {len(all_targets[target_type][split])} unique IDs in {target_type} {split} targets\")\n",
    "                # Print sample IDs for verification\n",
    "                sample_ids = list(all_targets[target_type][split])[:3] if all_targets[target_type][split] else []\n",
    "                if sample_ids:\n",
    "                    print(f\"  Sample IDs: {', '.join(sample_ids)}\")\n",
    "            else:\n",
    "                missing_target_files.append(file_path)\n",
    "                print(f\"Warning: Target file {file_path} not found\")\n",
    "    \n",
    "    if missing_target_files:\n",
    "        print(f\"\\nWARNING: The following target files could not be found:\")\n",
    "        for file_path in missing_target_files:\n",
    "            print(f\"  - {file_path}\")\n",
    "    \n",
    "    # Scan each input folder for array files - with better path checking and debugging\n",
    "    for input_type in input_types:\n",
    "        for split in [\"training\", \"test\"]:\n",
    "            # Construct folder path based on species\n",
    "            if species.lower() == \"mouse\":\n",
    "                # Try multiple possible folder structures for mouse\n",
    "                possible_paths = [\n",
    "                    f\"../inputs/{input_type}/{split}\",  # New structure\n",
    "                    f\"../{input_type}/{split}\"          # Original structure \n",
    "                ]\n",
    "            else:  # human\n",
    "                # Try multiple possible folder structures for human K562\n",
    "                if input_type == \"K562\":\n",
    "                    possible_paths = [\n",
    "                        f\"../inputs/landscape_arrays/K562/{split}\",      # Direct K562 folder\n",
    "                        f\"../inputs/landscape_arrays/{input_type}/{split}\",  # Standard path\n",
    "                        f\"../inputs/{input_type}/{split}\",  # Alternative path\n",
    "                        f\"../inputs/{input_type}_landscape/{split}\"  # Another alternative\n",
    "                    ]\n",
    "                else:\n",
    "                    possible_paths = [\n",
    "                        f\"../inputs/K562/{input_type}/{split}\"  # Nested under K562\n",
    "                    ]\n",
    "            \n",
    "            # Try each possible path\n",
    "            found_path = False\n",
    "            for folder_path in possible_paths:\n",
    "                if os.path.exists(folder_path):\n",
    "                    found_path = True\n",
    "                    print(f\"Found directory: {folder_path}\")\n",
    "                    \n",
    "                    # Look for .npy files\n",
    "                    array_files = glob.glob(f\"{folder_path}/*.npy\")\n",
    "                    if array_files:\n",
    "                        for file_path in array_files:\n",
    "                            file_name = os.path.basename(file_path)\n",
    "                            # Extract gene ID from file name (remove .npy extension)\n",
    "                            file_id = file_name.replace(\".npy\", \"\")\n",
    "                            # Remove orientation suffix\n",
    "                            if \"_forward\" in file_id:\n",
    "                                base_id = file_id.replace(\"_forward\", \"\")\n",
    "                            elif \"_rev\" in file_id:\n",
    "                                base_id = file_id.replace(\"_rev\", \"\")\n",
    "                            else:\n",
    "                                base_id = file_id\n",
    "                            array_data[input_type][split].add(base_id)\n",
    "                        print(f\"Found {len(array_files)} array files, extracted {len(array_data[input_type][split])} unique IDs in {folder_path}\")\n",
    "                        # Print a few sample file names for debugging\n",
    "                        if array_files:\n",
    "                            print(f\"  Sample files: {[os.path.basename(f) for f in array_files[:3]]}\")\n",
    "                    else:\n",
    "                        print(f\"Warning: No .npy files found in {folder_path}\")\n",
    "                    \n",
    "                    # Break after finding a valid path\n",
    "                    break\n",
    "            \n",
    "            if not found_path:\n",
    "                print(f\"Warning: Could not find any valid path for {input_type}/{split}\")\n",
    "                print(f\"  Tried the following paths: {', '.join(possible_paths)}\")\n",
    "    \n",
    "    # Add columns for each target type\n",
    "    for target_type in target_paths.keys():\n",
    "        column_name = f\"Included_{target_type}\" if target_type != \"default\" else \"Included\"\n",
    "        annotations[column_name] = \"Missing\"\n",
    "        \n",
    "        # Set the target status for each gene\n",
    "        for idx, row in annotations.iterrows():\n",
    "            gene_id = row['ID']\n",
    "            training_present = gene_id in all_targets[target_type][\"training\"]\n",
    "            test_present = gene_id in all_targets[target_type][\"test\"]\n",
    "            \n",
    "            if training_present and test_present:\n",
    "                annotations.at[idx, column_name] = \"Both\"\n",
    "            elif training_present:\n",
    "                annotations.at[idx, column_name] = \"Training\"\n",
    "            elif test_present:\n",
    "                annotations.at[idx, column_name] = \"Test\"\n",
    "    \n",
    "    # Add columns for each input type\n",
    "    for input_type in input_types:\n",
    "        # Create a clean column name from the input type\n",
    "        column_name = input_type.replace(\"-\", \"_\")\n",
    "        # Initialize the column with \"Missing\"\n",
    "        annotations[column_name] = \"Missing\"\n",
    "        \n",
    "        # Check if we found any arrays for this input type\n",
    "        any_arrays_found = False\n",
    "        for split in [\"training\", \"test\"]:\n",
    "            if array_data[input_type][split]:\n",
    "                any_arrays_found = True\n",
    "                break\n",
    "                \n",
    "        if not any_arrays_found:\n",
    "            print(f\"WARNING: No arrays found for input type {input_type}. All genes will be marked as 'Missing'.\")\n",
    "        \n",
    "        # Update values for genes that have arrays in this input type\n",
    "        for idx, row in annotations.iterrows():\n",
    "            gene_id = row['ID']\n",
    "            training_present = gene_id in array_data[input_type][\"training\"]\n",
    "            test_present = gene_id in array_data[input_type][\"test\"]\n",
    "            \n",
    "            if training_present and test_present:\n",
    "                annotations.at[idx, column_name] = \"Both\"\n",
    "            elif training_present:\n",
    "                annotations.at[idx, column_name] = \"Training\"\n",
    "            elif test_present:\n",
    "                annotations.at[idx, column_name] = \"Test\"\n",
    "    \n",
    "    # Count the distribution for each column\n",
    "    print(\"\\nDistribution of genes in target categories:\")\n",
    "    for target_type in target_paths.keys():\n",
    "        column_name = f\"Included_{target_type}\" if target_type != \"default\" else \"Included\"\n",
    "        dist = annotations[column_name].value_counts().to_dict()\n",
    "        print(f\"  {target_type if target_type != 'default' else 'Default targets'}:\")\n",
    "        for category, count in sorted(dist.items()):\n",
    "            print(f\"    {category}: {count}\")\n",
    "    \n",
    "    # Count the distribution for each input type\n",
    "    print(\"\\nDistribution of genes in input arrays:\")\n",
    "    for input_type in input_types:\n",
    "        column_name = input_type.replace(\"-\", \"_\")\n",
    "        dist = annotations[column_name].value_counts().to_dict()\n",
    "        print(f\"  {input_type}:\")\n",
    "        for category, count in sorted(dist.items()):\n",
    "            print(f\"    {category}: {count}\")\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    \n",
    "    # Save the updated annotations file\n",
    "    annotations.to_csv(output_file, sep='\\t', index=False)\n",
    "    print(f\"\\nUpdated annotations saved to {output_file}\")\n",
    "    \n",
    "    # Prepare summary statistics\n",
    "    summary = {\n",
    "        \"species\": species,\n",
    "        \"total_genes\": len(annotations),\n",
    "        \"target_distributions\": {},\n",
    "        \"array_distributions\": {}\n",
    "    }\n",
    "    \n",
    "    # Add target distributions to summary\n",
    "    for target_type in target_paths.keys():\n",
    "        column_name = f\"Included_{target_type}\" if target_type != \"default\" else \"Included\"\n",
    "        summary[\"target_distributions\"][target_type] = annotations[column_name].value_counts().to_dict()\n",
    "    \n",
    "    # Add array distributions to summary\n",
    "    for input_type in input_types:\n",
    "        column_name = input_type.replace(\"-\", \"_\")\n",
    "        summary[\"array_distributions\"][input_type] = annotations[column_name].value_counts().to_dict()\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def main():\n",
    "    # Check if running in Jupyter\n",
    "    is_jupyter = 'ipykernel_launcher' in sys.argv[0] or 'ipykernel' in sys.argv[0]\n",
    "    \n",
    "    if is_jupyter:\n",
    "        # Default to processing both species when run in Jupyter\n",
    "        species_to_process = ['mouse', 'human']\n",
    "    else:\n",
    "        # When run as standalone script, use argparse\n",
    "        parser = argparse.ArgumentParser(description='Update gene annotations with target and input information')\n",
    "        parser.add_argument('--species', choices=['mouse', 'human', 'both'], default='both',\n",
    "                            help='Species to process (mouse, human, or both)')\n",
    "        args = parser.parse_args()\n",
    "        \n",
    "        if args.species == 'both':\n",
    "            species_to_process = ['mouse', 'human']\n",
    "        else:\n",
    "            species_to_process = [args.species]\n",
    "    \n",
    "    # Process requested species\n",
    "    results = {}\n",
    "    for species in species_to_process:\n",
    "        results[species] = process_annotation_file(species=species)\n",
    "    \n",
    "    # Print overall summary\n",
    "    print(\"\\n=== OVERALL SUMMARY ===\")\n",
    "    for species, result in results.items():\n",
    "        print(f\"\\n{species.upper()} ANNOTATIONS:\")\n",
    "        if \"error\" in result:\n",
    "            print(f\"  Error: {result['error']}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"  Total genes: {result['total_genes']}\")\n",
    "        \n",
    "        print(\"  Target distributions:\")\n",
    "        for target_type, dist in result['target_distributions'].items():\n",
    "            target_name = target_type if target_type != \"default\" else \"Default\"\n",
    "            print(f\"    {target_name}:\")\n",
    "            for category, count in sorted(dist.items()):\n",
    "                print(f\"      {category}: {count}\")\n",
    "        \n",
    "        print(\"  Array distributions:\")\n",
    "        for input_type, dist in result['array_distributions'].items():\n",
    "            print(f\"    {input_type}:\")\n",
    "            for category, count in sorted(dist.items()):\n",
    "                print(f\"      {category}: {count}\")\n",
    "\n",
    "# This function can be called directly from jupyter\n",
    "def run_for_species(species=\"both\"):\n",
    "    \"\"\"\n",
    "    Wrapper function to run the annotation processing for specific species.\n",
    "    Can be called directly from Jupyter notebook.\n",
    "    \n",
    "    Parameters:\n",
    "    species (str): 'mouse', 'human', or 'both'\n",
    "    \"\"\"\n",
    "    if species == 'both':\n",
    "        species_list = ['mouse', 'human']\n",
    "    else:\n",
    "        species_list = [species]\n",
    "        \n",
    "    results = {}\n",
    "    for sp in species_list:\n",
    "        results[sp] = process_annotation_file(species=sp)\n",
    "        \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
